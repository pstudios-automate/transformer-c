Script started on 2024-09-05 13:46:34+05:30 [TERM="xterm-256color" TTY="/dev/pts/3" COLUMNS="93" LINES="22"]
[?2004h]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K[A[K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K[A[K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K[A[K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ clear
[?2004l[H[2J[3J[?2004h]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ cleard "Structure Format"[A]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ source '/home/kartikey-bartwal/Technical Stuffs/Freelance Stuffs/AI-Scoring-for-PTE-Tests/venv/bin/activate'[Acd "Structure Format"[K[A]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [C[2Plear
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K[K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ [K]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ g[Kc[Kcd "Structure Format"
[?2004lbash: cd: Structure Format: No such file or directory
[?2004h]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ cd[K[Kcd [K[K[Kgcc main.c Data_Loading_Cleaning.c Tokenizer.c Data_Preprocessing.c transformer_block.c fee d_forward_layer.c activation_functions.c backpropagation.c -lm
[?2004l[?2004h]0;kartikey-bartwal@Guts: ~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[01;32mkartikey-bartwal@Guts[00m:[01;34m~/Technical Stuffs/C-Transformers-Unleashing-the-BERT-Beast/Structure Format[00m$ ./a.out
[?2004l
==============================
       RAW TEXT DATA         
==============================
Martin Luther King Jr. (born Michael King Jr.; January 15, 1929 – April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968. King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi. He was the son of early civil rights activist and minister Martin Luther King Sr.\n\nKing
==============================
    EXTRACTING SENTENCES      
==============================

==============================
    EXTRACTED SENTENCES       
==============================
  [1] Martin Luther King Jr
  [2]  (born Michael King Jr
  [3] ; January 15, 1929 – April 4, 1968) was an American Baptist minister and activist who became the most visible spokesman and leader in the American civil rights movement from 1955 until his assassination in 1968
  [4]  King advanced civil rights through nonviolence and civil disobedience, inspired by his Christian beliefs and the nonviolent activism of Mahatma Gandhi
  [5]  He was the son of early civil rights activist and minister Martin Luther King Sr

==============================
      CLEANED SENTENCES       
==============================
  [1] martin luther king jr
  [2]  born michael king jr
  [3]  january 15 1929 april 4 1968 was an american baptist minister and activist who became the most visible spokesman and leader in the american civil rights movement from 1955 until his assassination in 1968
  [4]  king advanced civil rights through nonviolence and civil disobedience inspired by his christian beliefs and the nonviolent activism of mahatma gandhi
  [5]  he was the son of early civil rights activist and minister martin luther king sr
 PREPARED THE WORD MAPPINGS 
 GENERATING WORD MAPPINGS FOR EVERY WORD  HERE ARE SOME OF THE WORD MAPPINGS: 





TOKEN   VS  TOKEN_ID   VS  EMBEDDING:
Table Size: 100000 
Birmingham : 90 : { 36.9244, -14.3247 }
Southern : 74 : { -29.3375, 3.9951 }
Conference : 77 : { -40.6324, 1.1309 }
luther : 2 : { 25.7421, -12.4195 }
King : 67 : { -7.2282, 13.2335 }
king : 3 : { 5.8092, 29.9422 }
son : 50 : { -4.8334, 39.6574 }
vote : 61 : { 38.3383, 9.6511 }
helped : 85 : { -8.7650, 43.8975 }
the : 22 : { 0.0883, -39.1514 }
was : 13 : { -14.4252, -8.9888 }
who : 20 : { -18.9421, 11.0736 }
Washington : 94 : { -0.7556, 24.9436 }
steps : 103 : { 10.3965, 0.2144 }
Memorial : 105 : { -37.7488, 10.1927 }
Dream" : 101 : { 21.6717, 21.7481 }
first : 72 : { -11.3579, -2.0857 }
president : 73 : { 21.9132, 19.1364 }
rights : 29 : { 28.9231, 35.9050 }
(SCLC) : 78 : { 1.0388, 36.1794 }
Christian : 75 : { -6.0713, 43.0831 }
christian : 42 : { 5.4895, -15.2025 }
blacks\' : 58 : { -31.0240, -2.6668 }
organize : 86 : { -34.2178, 16.4097 }
labor : 63 : { 8.1126, 42.8869 }
where : 95 : { -13.3718, 35.9412 }
delivered : 96 : { 7.5708, -6.8133 }
famous : 97 : { 9.4070, -11.9261 }
boycott : 70 : { -25.9948, 15.5208 }
March : 92 : { -6.1913, 2.9709 }
right : 59 : { 3.5940, 6.4384 }
from : 31 : { 10.5745, 43.0509 }
nonviolent : 44 : { -0.4798, 39.3311 }
april : 10 : { 19.4976, -8.9755 }
michael : 6 : { 0.6316, 12.3738 }
assassination : 35 : { -0.5367, -15.1571 }
Leadership : 76 : { 7.8948, 21.4108 }
inspired : 40 : { 10.4300, 9.7341 }
activism : 45 : { -12.7329, 9.8062 }
activist : 19 : { -0.0962, 46.7435 }
through : 37 : { 4.0133, 33.0533 }
leader : 26 : { 1.9823, -26.7788 }
Alabama : 91 : { -1.4934, -5.2189 }
early : 51 : { -1.1735, -21.0902 }
desegregation : 62 : { 0.3368, 17.5432 }
later : 71 : { -40.4645, -13.9092 }
Lincoln : 104 : { 10.8075, -31.2478 }
mahatma : 47 : { 1.9439, -35.5927 }
minister : 17 : { -10.8292, 5.6125 }
became : 21 : { 2.2376, -14.7840 }
\n\nKing : 53 : { -13.9500, -34.9923 }
martin : 1 : { 28.6258, -5.7065 }
visible : 24 : { -27.8669, 17.7742 }
"I : 98 : { 26.6381, -20.3022 }
beliefs : 43 : { -27.3821, -9.4182 }
nonviolence : 38 : { 12.7678, 21.1026 }
civil : 28 : { 10.5716, 14.5457 }
15 : 8 : { 44.7417, -6.0556 }
january : 7 : { -8.8835, 1.0103 }
unsuccessful : 81 : { -4.6183, 19.8506 }
an : 14 : { -36.6732, -5.3490 }
As : 79 : { 19.1010, 26.4620 }
by : 41 : { -2.9799, 32.9797 }
Montgomery : 68 : { 31.2129, 10.2179 }
baptist : 16 : { -0.3723, -32.4754 }
he : 49 : { 9.6866, 9.4617 }
in : 27 : { -29.4359, -4.7685 }
jr : 4 : { 12.4704, -17.5327 }
of : 46 : { 41.1638, -3.8308 }
on : 93 : { -17.7696, -12.7086 }
sr : 52 : { 44.4231, -6.8477 }
to : 60 : { 10.0707, -38.7445 }
1929 : 9 : { 5.5929, -19.7986 }
1955 : 32 : { 22.1788, 29.2745 }
1963 : 88 : { 0.3324, -31.9427 }
1968 : 12 : { 19.4652, 25.6681 }
Movement : 83 : { 48.4176, -7.3782 }
movement : 30 : { 4.1792, -6.7393 }
marches : 56 : { -14.9165, -16.4281 }
protests : 89 : { -28.9730, -5.5257 }
other : 64 : { 16.9694, -4.5774 }
speech : 102 : { 40.1280, 4.9126 }

 : 106 : { -10.8858, -17.2964 }
4 : 11 : { 48.3695, -0.1138 }
a : 100 : { -18.9012, 28.5460 }
[1] : 66 : { 0.5465, 26.7644 }
disobedience : 39 : { -20.7897, 6.3603 }
participated : 54 : { -7.8834, 5.6432 }
most : 23 : { 18.3363, 7.8615 }
Georgia : 84 : { 28.5912, 25.4690 }
spokesman : 25 : { 5.6944, 44.5374 }
basic : 65 : { 20.6518, 7.3871 }
Albany : 82 : { -14.0346, -32.4973 }
SCLC : 80 : { 3.8344, -3.5288 }
until : 33 : { -23.3314, -0.0528 }
and : 18 : { -10.3300, 1.6037 }
american : 15 : { 32.4445, 21.4037 }
gandhi : 48 : { 4.6433, -27.1402 }
born : 5 : { 21.3097, 1.5309 }
bus : 69 : { 4.2990, 29.1531 }
Have : 99 : { -33.2974, 1.6939 }
for : 57 : { -2.3102, -24.6468 }
his : 34 : { -6.6026, -22.0839 }
advanced : 36 : { -3.1347, 5.8955 }
led : 55 : { -11.9567, -0.4844 }
some : 87 : { 40.7489, 24.1044 }
 PREPARING TRAINING DATA... 
 TRAINING DATA PREPARED. TOTAL SAMPLES: 10
 SAMPLE OF TRAINING DATA ( FIRST 10 TOKENS OF FIRST 5 SAMPLES: 
 SAMPLE 1:  1  2  3  4  0  0  0  0  0  0  ...
 SAMPLE 2:  5  6  3  4  0  0  0  0  0  0  ...
 SAMPLE 3:  7  8  9  10  11  12  13  14  15  16  ...
 SAMPLE 4:  3  36  28  29  37  38  18  28  39  40  ...
 SAMPLE 5:  49  13  22  50  46  51  28  29  19  18  ...
 SAMPLE 6:  53  54  27  18  55  56  57  58  59  60  ...
 SAMPLE 7:  66  67  55  22  32  68  69  70  18  71  ...
 SAMPLE 8:  79  73  46  22  0  49  55  22  81  82  ...
 SAMPLE 9:  67  85  86  22  88  92  93  0  95  49  ...
 SAMPLE 10:  106  0  0  0  0  0  0  0  0  0  ...
 DATA PREPARATION COMPLETED
 ADDED PADDING
TOKEN MAPPING COMPLETED 

NUM SAMPLES: 10 
key_weight_1.txt 
filename: Model Trained Weights/self-attention-block-weights/key_weight_1.txt 
key_weight_2.txt 
filename: Model Trained Weights/self-attention-block-weights/key_weight_2.txt 
key_weight_3.txt 
filename: Model Trained Weights/self-attention-block-weights/key_weight_3.txt 
key_weight_4.txt 
filename: Model Trained Weights/self-attention-block-weights/key_weight_4.txt 
Initialized KEY MATRIX
query_weight_1.txt 
filename: Model Trained Weights/self-attention-block-weights/query_weight_1.txt 
query_weight_2.txt 
filename: Model Trained Weights/self-attention-block-weights/query_weight_2.txt 
query_weight_3.txt 
filename: Model Trained Weights/self-attention-block-weights/query_weight_3.txt 
query_weight_4.txt 
filename: Model Trained Weights/self-attention-block-weights/query_weight_4.txt 
Initialized QUERY MATRIX
value_weight_1.txt 
filename: Model Trained Weights/self-attention-block-weights/value_weight_1.txt 
value_weight_2.txt 
filename: Model Trained Weights/self-attention-block-weights/value_weight_2.txt 
value_weight_3.txt 
filename: Model Trained Weights/self-attention-block-weights/value_weight_3.txt 
value_weight_4.txt 
filename: Model Trained Weights/self-attention-block-weights/value_weight_4.txt 
Initialized VALUE MATRIX
Epoch: 0
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.642000 0.354000 
-0.873000 0.719000 

Q Matrix:
0.457000 -0.298000 
0.832000 -0.564000 

V Matrix:
-0.932000 0.117000 
-0.451000 0.683000 
Final K Matrix:
[-14.268985, 6.749554]
[-6.758161, 0.983804]
[-30.880925, 24.271237]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[9.166176, -5.876018]
[2.380390, -1.304392]
[28.400774, -19.174015]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-24.556619, 0.134671]
[-19.233098, -4.815198]
[-19.945999, 21.597666]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-19.233098 -4.815198  
-19.233098 -4.815198  
-19.233098 -4.815198  
-21.894859 -2.340264  
-21.894859 -2.340264  
-21.894859 -2.340264  
-21.894859 -2.340264  
-21.894859 -2.340264  
-21.894859 -2.340264  
-21.894859 -2.340264  


Context Matrix (embedding_matrix + self_attention_matrix:
9.392714 -9.521712 
6.988465 -16.357102 
-12.582459 25.667293 
-21.894859 -2.340264 
-21.894859 -2.340264 
-21.894859 -2.340264 
-21.894859 -2.340264 
-21.894859 -2.340264 
-21.894859 -2.340264 
-21.894859 -2.340264 




semi_final_layer_weights[0] = -0.896191
semi_final_layer_weights[1] = -0.370582
semi_final_layer_weights[2] = 0.428389
semi_final_layer_weights[3] = -0.823069
semi_final_layer_weights[4] = -0.397813
semi_final_layer_weights[5] = 0.398136
semi_final_layer_weights[6] = 0.029025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.576670
semi_final_layer_weights[9] = -0.447157
.
.
.
semi_final_layer_weights[511] = -0.067156
semi_final_layer_weights[512] = 0.981305


 Semi Final Layer Node Values: 
1.011798 
3.842430 
6.033788 
20.770247 
10.038860 
-0.100470 
-0.007324 
0.211849 
14.552338 
11.284062 
.
.
.
23.357175 
-0.139882 
-0.031536 
4.451501 
10.313872 
12.991520 
-0.156575 
-0.099060 
-0.133037 
-0.195163 
1.694690 
-0.247634 


final_layer_weights[0] = 0.197141
final_layer_weights[1] = 0.500194
final_layer_weights[2] = 0.055747
final_layer_weights[3] = -0.272823
final_layer_weights[4] = 0.609656
final_layer_weights[5] = 0.821736
final_layer_weights[6] = 0.222277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.481983
final_layer_weights[9] = 0.968050
.
.
.
final_layer_weights[1022] = 0.213603
final_layer_weights[1023] = -0.053709


Output Embedding: 42.692488, 121.836419 

 Expected Embedding: 12.470385, -17.532656 
 loss: 10168.557192 


Updated weights for final layer:
final_layer_weights[0] = 0.207141
final_layer_weights[1] = 0.510194
final_layer_weights[2] = 0.045747
final_layer_weights[3] = -0.262823
final_layer_weights[4] = 0.619656
final_layer_weights[5] = 0.811736
final_layer_weights[6] = 0.212277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.491983
final_layer_weights[9] = 0.978050
.
.
.
final_layer_weights[1022] = 0.223603
final_layer_weights[1023] = -0.063709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.886191
semi_final_layer_weights[1] = -0.360582
semi_final_layer_weights[2] = 0.418389
semi_final_layer_weights[3] = -0.813069
semi_final_layer_weights[4] = -0.387813
semi_final_layer_weights[5] = 0.388136
semi_final_layer_weights[6] = 0.019025
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.566670
semi_final_layer_weights[9] = -0.437157
.
.
.
semi_final_layer_weights[510] = -0.057156
semi_final_layer_weights[511] = 0.971305
Updating K Matrix:
k_matrix[0][0] = 0.358000
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.127000
k_matrix[1][1] = -0.281000

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.168000
q_matrix[1][1] = 0.436000

Updating V Matrix:
v_matrix[0][0] = 0.068000
v_matrix[0][1] = -0.883000
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.317000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.358000 -0.646000 
0.127000 -0.281000 

Q Matrix:
-0.543000 0.702000 
-0.168000 0.436000 

V Matrix:
0.068000 -0.883000 
0.549000 -0.317000 
Final K Matrix:
[7.950308, -14.477271]
[2.080659, -4.441336]
[6.252205, -12.861893]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-11.996375, 16.062909]
[-2.829498, 6.557516]
[-8.732356, 17.959115]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[2.838539, -19.618788]
[7.350556, -5.181690]
[17.187131, -15.535464]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.350556 -5.181690  
7.350556 -5.181690  
7.350556 -5.181690  
5.094547 -12.400239  
5.094547 -12.400239  
5.094547 -12.400239  
5.094547 -12.400239  
5.094547 -12.400239  
5.094547 -12.400239  
5.094547 -12.400239  


Context Matrix (embedding_matrix + self_attention_matrix:
28.660279 -2.650765 
8.461543 8.069692 
14.001195 25.300801 
5.094547 -12.400239 
5.094547 -12.400239 
5.094547 -12.400239 
5.094547 -12.400239 
5.094547 -12.400239 
5.094547 -12.400239 
5.094547 -12.400239 




semi_final_layer_weights[0] = -0.886191
semi_final_layer_weights[1] = -0.360582
semi_final_layer_weights[2] = 0.418389
semi_final_layer_weights[3] = -0.813069
semi_final_layer_weights[4] = -0.387813
semi_final_layer_weights[5] = 0.388136
semi_final_layer_weights[6] = 0.019025
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.566670
semi_final_layer_weights[9] = -0.437157
.
.
.
semi_final_layer_weights[511] = -0.057156
semi_final_layer_weights[512] = 0.971305


 Semi Final Layer Node Values: 
-0.221632 
-0.063214 
16.861912 
6.753101 
3.221055 
-0.032237 
-0.001580 
-0.000133 
4.706586 
3.630891 
.
.
.
7.604542 
-0.045209 
-0.009549 
1.382075 
3.311571 
4.192871 
-0.050703 
-0.031773 
-0.042956 
-0.063404 
0.474720 
-0.080674 


final_layer_weights[0] = 0.207141
final_layer_weights[1] = 0.510194
final_layer_weights[2] = 0.045747
final_layer_weights[3] = -0.262823
final_layer_weights[4] = 0.619656
final_layer_weights[5] = 0.811736
final_layer_weights[6] = 0.212277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.491983
final_layer_weights[9] = 0.978050
.
.
.
final_layer_weights[1022] = 0.223603
final_layer_weights[1023] = -0.063709


Output Embedding: 25.313778, 58.589141 

 Expected Embedding: 12.470385, -17.532656 
 loss: 2979.740306 


Updated weights for final layer:
final_layer_weights[0] = 0.217141
final_layer_weights[1] = 0.520194
final_layer_weights[2] = 0.035747
final_layer_weights[3] = -0.252823
final_layer_weights[4] = 0.629656
final_layer_weights[5] = 0.801736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.501983
final_layer_weights[9] = 0.988050
.
.
.
final_layer_weights[1022] = 0.233603
final_layer_weights[1023] = -0.073709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.876191
semi_final_layer_weights[1] = -0.350582
semi_final_layer_weights[2] = 0.408389
semi_final_layer_weights[3] = -0.803069
semi_final_layer_weights[4] = -0.377813
semi_final_layer_weights[5] = 0.378136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.556670
semi_final_layer_weights[9] = -0.427157
.
.
.
semi_final_layer_weights[510] = -0.047156
semi_final_layer_weights[511] = 0.961305
Updating K Matrix:
k_matrix[0][0] = -0.642000
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.873000
k_matrix[1][1] = 0.719000

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.832000
q_matrix[1][1] = -0.564000

Updating V Matrix:
v_matrix[0][0] = -0.932000
v_matrix[0][1] = 0.117000
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.683000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.642000 0.354000 
-0.873000 0.719000 

Q Matrix:
0.457000 -0.298000 
0.832000 -0.564000 

V Matrix:
-0.932000 0.117000 
-0.451000 0.683000 
Final K Matrix:
[3.948203, -1.699347]
[-24.511536, 12.285265]
[12.681595, -11.568927]
[-5.384019, 0.852764]
[-31.174333, 17.063658]
[-34.589688, 24.981851]
[17.881849, -12.231300]
[29.256657, -17.625774]
[-38.458256, 26.136706]
[29.401665, -23.979230]
Final Q Matrix:
[-2.387182, 1.513469]
[16.357923, -10.555479]
[-13.082370, 8.944220]
[1.957524, -1.085271]
[22.079488, -14.386188]
[29.858370, -20.003913]
[-14.830162, 9.884683]
[-22.149509, 14.578177]
[31.745289, -21.145942]
[-27.811807, 18.837277]
Final V Matrix:
[7.372758, 0.333667]
[-39.810811, 1.754262]
[2.688623, -12.400583]
[-15.085381, -3.683998]
[-45.688809, 5.403668]
[-29.914301, 19.331558]
[17.813198, -8.486735]
[37.341143, -8.624804]
[-38.891159, 17.879731]
[15.999546, -22.482610]
 

Self-Attention Matrix: 
-39.810811 1.754262  
7.372758 0.333667  
-39.810811 1.754262  
7.372758 0.333667  
7.372758 0.333667  
7.372758 0.333667  
-39.810811 1.754262  
-39.810811 1.754262  
7.372758 0.333667  
-39.810811 1.754262  


Context Matrix (embedding_matrix + self_attention_matrix:
-48.694290 3.764560 
52.593911 -4.844391 
-33.376409 -17.504022 
27.867831 -8.571051 
56.651525 -0.196271 
27.436402 25.200595 
-54.094905 -8.224502 
-76.834828 -4.531234 
39.060411 21.083705 
-41.160670 -30.931941 




semi_final_layer_weights[0] = -0.876191
semi_final_layer_weights[1] = -0.350582
semi_final_layer_weights[2] = 0.408389
semi_final_layer_weights[3] = -0.803069
semi_final_layer_weights[4] = -0.377813
semi_final_layer_weights[5] = 0.378136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.556670
semi_final_layer_weights[9] = -0.427157
.
.
.
semi_final_layer_weights[511] = -0.047156
semi_final_layer_weights[512] = 0.961305


 Semi Final Layer Node Values: 
38.490834 
-0.163895 
-0.211874 
-0.146936 
-0.209517 
20.282080 
-0.005715 
0.691463 
-0.340371 
31.222021 
.
.
.
12.836681 
-0.075739 
-0.014879 
2.216994 
5.510003 
7.014090 
-0.085116 
-0.052809 
-0.071895 
-0.106792 
0.668439 
-0.136266 


final_layer_weights[0] = 0.217141
final_layer_weights[1] = 0.520194
final_layer_weights[2] = 0.035747
final_layer_weights[3] = -0.252823
final_layer_weights[4] = 0.629656
final_layer_weights[5] = 0.801736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.501983
final_layer_weights[9] = 0.988050
.
.
.
final_layer_weights[1022] = 0.233603
final_layer_weights[1023] = -0.073709


Output Embedding: 50.372470, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 807.055494 


Updated weights for final layer:
final_layer_weights[0] = 0.227141
final_layer_weights[1] = 0.530194
final_layer_weights[2] = 0.025747
final_layer_weights[3] = -0.242823
final_layer_weights[4] = 0.639656
final_layer_weights[5] = 0.791736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.511983
final_layer_weights[9] = 0.998050
.
.
.
final_layer_weights[1022] = 0.243603
final_layer_weights[1023] = -0.083709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.866191
semi_final_layer_weights[1] = -0.340582
semi_final_layer_weights[2] = 0.398389
semi_final_layer_weights[3] = -0.793069
semi_final_layer_weights[4] = -0.367813
semi_final_layer_weights[5] = 0.368136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.546670
semi_final_layer_weights[9] = -0.417157
.
.
.
semi_final_layer_weights[510] = -0.037156
semi_final_layer_weights[511] = 0.951305
Updating K Matrix:
k_matrix[0][0] = 0.358000
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.127000
k_matrix[1][1] = -0.281000

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.168000
q_matrix[1][1] = 0.436000

Updating V Matrix:
v_matrix[0][0] = 0.068000
v_matrix[0][1] = -0.827255
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.317000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.358000 -0.646000 
0.127000 -0.281000 

Q Matrix:
-0.543000 0.702000 
-0.168000 0.436000 

V Matrix:
0.068000 -0.827255 
0.549000 -0.317000 
Final K Matrix:
[6.009340, -12.447478]
[-0.090388, -0.187965]
[6.001806, -11.612015]
[15.280485, -29.437876]
[5.907189, -12.351001]
[7.363424, -14.339344]
[-3.569675, 6.409558]
[5.387430, -10.426850]
[-6.988896, 12.315457]
[4.593440, -8.782335]
Final Q Matrix:
[-8.352666, 17.568831]
[0.303908, 1.089106]
[-8.731753, 14.589472]
[-22.290795, 36.689672]
[-8.155983, 17.685417]
[-10.668551, 18.234579]
[5.429455, -6.885007]
[-7.836264, 13.108643]
[10.741025, -12.637522]
[-6.732602, 10.787778]
Final V Matrix:
[17.382285, -14.614337]
[3.537887, 0.049477]
[9.058281, -14.223790]
[21.785298, -36.156253]
[18.252510, -14.418174]
[12.054394, -17.492906]
[-0.355907, 8.234248]
[8.166467, -12.769359]
[1.667819, 16.015425]
[5.871036, -10.838480]
 

Self-Attention Matrix: 
3.537887 0.049477  
17.336822 -14.566184  
3.537887 0.049477  
3.537887 0.049477  
3.537887 0.049477  
3.537887 0.049477  
17.382285 -14.614337  
3.537887 0.049477  
17.382285 -14.614337  
3.537887 0.049477  


Context Matrix (embedding_matrix + self_attention_matrix:
9.347056 30.991665 
14.681587 -7.793063 
14.950986 15.135433 
33.458450 36.025250 
8.460443 32.686592 
16.904207 20.350905 
7.193406 -14.000609 
13.758732 13.658674 
-4.164218 -8.907632 
12.990364 9.572732 




semi_final_layer_weights[0] = -0.866191
semi_final_layer_weights[1] = -0.340582
semi_final_layer_weights[2] = 0.398389
semi_final_layer_weights[3] = -0.793069
semi_final_layer_weights[4] = -0.367813
semi_final_layer_weights[5] = 0.368136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.546670
semi_final_layer_weights[9] = -0.417157
.
.
.
semi_final_layer_weights[511] = -0.037156
semi_final_layer_weights[512] = 0.951305


 Semi Final Layer Node Values: 
-0.358072 
-0.020055 
12.384487 
-0.558984 
-0.155022 
14.083084 
0.007612 
0.045610 
7.692658 
-0.098295 
.
.
.
-0.019503 
1.141775 
0.206805 
-0.003188 
-0.008247 
-0.010558 
1.285830 
0.789503 
1.082711 
1.618826 
-0.000809 
2.071615 


final_layer_weights[0] = 0.227141
final_layer_weights[1] = 0.530194
final_layer_weights[2] = 0.025747
final_layer_weights[3] = -0.242823
final_layer_weights[4] = 0.639656
final_layer_weights[5] = 0.791736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.511983
final_layer_weights[9] = 0.998050
.
.
.
final_layer_weights[1022] = 0.243603
final_layer_weights[1023] = -0.083709


Output Embedding: -0.047190, 30.301929 

 Expected Embedding: 4.643339, -27.140186 
 loss: 1660.798853 


Updated weights for final layer:
final_layer_weights[0] = 0.237141
final_layer_weights[1] = 0.540194
final_layer_weights[2] = 0.015747
final_layer_weights[3] = -0.232823
final_layer_weights[4] = 0.649656
final_layer_weights[5] = 0.781736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.521983
final_layer_weights[9] = 1.008050
.
.
.
final_layer_weights[1022] = 0.253603
final_layer_weights[1023] = -0.093709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.856191
semi_final_layer_weights[1] = -0.330582
semi_final_layer_weights[2] = 0.388389
semi_final_layer_weights[3] = -0.783069
semi_final_layer_weights[4] = -0.357813
semi_final_layer_weights[5] = 0.358136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.536670
semi_final_layer_weights[9] = -0.407157
.
.
.
semi_final_layer_weights[510] = -0.027156
semi_final_layer_weights[511] = 0.941305
Updating K Matrix:
k_matrix[0][0] = -0.642000
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.873000
k_matrix[1][1] = 0.719000

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.832000
q_matrix[1][1] = -0.564000

Updating V Matrix:
v_matrix[0][0] = -0.932000
v_matrix[0][1] = 0.172745
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.683000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.642000 0.354000 
-0.873000 0.719000 

Q Matrix:
0.457000 -0.298000 
0.832000 -0.564000 

V Matrix:
-0.932000 0.172745 
-0.451000 0.683000 
Final K Matrix:
[-15.351843, 10.950998]
[16.034264, -10.768754]
[33.110580, -27.432246]
[-32.220000, 27.206615]
[-23.303297, 11.840288]
[19.480302, -15.943424]
[-18.711677, 13.538834]
[-48.870979, 35.256999]
[-39.688821, 32.836643]
[6.043410, -3.001352]
Final Q Matrix:
[13.130888, -8.786990]
[-13.121734, 8.730555]
[-31.699534, 21.499591]
[31.300794, -21.263565]
[15.693906, -10.142478]
[-18.476373, 12.518067]
[16.174037, -10.837792]
[42.151394, -28.236821]
[37.956939, -25.740484]
[-4.008627, 2.584034]
Final V Matrix:
[-13.746147, 8.818629]
[16.655621, -7.949008]
[16.547054, -26.210773]
[-14.342301, 26.471684]
[-37.296711, 4.367229]
[10.408934, -15.051105]
[-16.097884, 11.109092]
[-42.400200, 28.819263]
[-19.991543, 31.332015]
[9.910408, -1.001952]
 

Self-Attention Matrix: 
16.655621 -7.949008  
-13.746147 8.818629  
-13.746147 8.818629  
16.655621 -7.949008  
16.655621 -7.949008  
-13.746147 8.818629  
16.655621 -7.949008  
16.655621 -7.949008  
16.655621 -7.949008  
-13.746147 8.818629  


Context Matrix (embedding_matrix + self_attention_matrix:
26.342251 2.512648 
-27.691935 0.707440 
-12.816365 -29.792482 
12.819685 31.779137 
58.728688 -12.195987 
-14.321202 -13.072684 
27.368369 5.606654 
45.227906 27.019574 
15.802642 38.140836 
-25.053676 10.211554 




semi_final_layer_weights[0] = -0.856191
semi_final_layer_weights[1] = -0.330582
semi_final_layer_weights[2] = 0.388389
semi_final_layer_weights[3] = -0.783069
semi_final_layer_weights[4] = -0.357813
semi_final_layer_weights[5] = 0.358136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.536670
semi_final_layer_weights[9] = -0.407157
.
.
.
semi_final_layer_weights[511] = -0.027156
semi_final_layer_weights[512] = 0.941305


 Semi Final Layer Node Values: 
-0.255615 
8.590006 
-0.169372 
-0.357070 
-0.162922 
-0.101689 
0.306625 
-0.006149 
-0.294865 
5.635917 
.
.
.
-0.025589 
1.486135 
0.245516 
-0.003941 
-0.010654 
-0.013720 
1.677281 
1.018701 
1.407762 
2.119136 
-0.000785 
2.719946 


final_layer_weights[0] = 0.237141
final_layer_weights[1] = 0.540194
final_layer_weights[2] = 0.015747
final_layer_weights[3] = -0.232823
final_layer_weights[4] = 0.649656
final_layer_weights[5] = 0.781736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.521983
final_layer_weights[9] = 1.008050
.
.
.
final_layer_weights[1022] = 0.253603
final_layer_weights[1023] = -0.093709


Output Embedding: -0.001198, -0.000000 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1010.206477 


Updated weights for final layer:
final_layer_weights[0] = 0.247141
final_layer_weights[1] = 0.550194
final_layer_weights[2] = 0.005747
final_layer_weights[3] = -0.222823
final_layer_weights[4] = 0.659656
final_layer_weights[5] = 0.771736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.531983
final_layer_weights[9] = 1.018050
.
.
.
final_layer_weights[1022] = 0.263603
final_layer_weights[1023] = -0.103709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.846191
semi_final_layer_weights[1] = -0.320582
semi_final_layer_weights[2] = 0.378389
semi_final_layer_weights[3] = -0.773069
semi_final_layer_weights[4] = -0.347813
semi_final_layer_weights[5] = 0.348136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.526670
semi_final_layer_weights[9] = -0.397157
.
.
.
semi_final_layer_weights[510] = -0.017156
semi_final_layer_weights[511] = 0.931305
Updating K Matrix:
k_matrix[0][0] = 0.358000
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.127000
k_matrix[1][1] = -0.281000

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.168000
q_matrix[1][1] = 0.436000

Updating V Matrix:
v_matrix[0][0] = 0.068000
v_matrix[0][1] = -0.827255
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.317000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.358000 -0.646000 
0.127000 -0.281000 

Q Matrix:
-0.543000 0.702000 
-0.168000 0.436000 

V Matrix:
0.068000 -0.827255 
0.549000 -0.317000 
Final K Matrix:
[-9.311129, 18.563548]
[-1.822499, 2.950656]
[-10.773796, 19.660141]
[-3.128380, 5.558275]
[-4.069353, 7.389702]
[-7.313955, 14.090838]
[-4.032401, 8.605158]
[-11.689784, 21.280624]
[1.750379, -3.458344]
[-1.691962, 5.072241]
Final Q Matrix:
[13.285567, -24.613553]
[2.924889, -2.354564]
[16.237126, -21.916800]
[4.786241, -5.821354]
[6.150050, -8.147949]
[10.669182, -17.563176]
[5.484789, -12.700323]
[17.641851, -23.596122]
[-2.512434, 4.513867]
[1.606889, -10.601098]
Final V Matrix:
[-19.610365, 22.315781]
[3.076420, 4.057922]
[-4.265699, 24.995242]
[0.284667, 7.189557]
[-1.245649, 9.424523]
[-10.432468, 17.306298]
[-14.222073, 9.921239]
[-4.111693, 27.097174]
[3.368772, -4.180850]
[-20.768137, 4.826446]
 

Self-Attention Matrix: 
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
3.076420 4.057922  
-19.610365 22.315781  
-19.542510 22.261173  


Context Matrix (embedding_matrix + self_attention_matrix:
-10.873605 -29.934361 
-4.327585 10.578673 
-25.518037 -0.170268 
-6.256084 5.732380 
-7.971003 3.157330 
-11.241577 -13.171303 
0.907331 -21.578827 
-28.298345 0.454627 
-16.773180 28.100559 
-10.449314 -16.694153 




semi_final_layer_weights[0] = -0.846191
semi_final_layer_weights[1] = -0.320582
semi_final_layer_weights[2] = 0.378389
semi_final_layer_weights[3] = -0.773069
semi_final_layer_weights[4] = -0.347813
semi_final_layer_weights[5] = 0.348136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.526670
semi_final_layer_weights[9] = -0.397157
.
.
.
semi_final_layer_weights[511] = -0.017156
semi_final_layer_weights[512] = 0.931305


 Semi Final Layer Node Values: 
35.377525 
-0.023246 
-0.100986 
-0.003682 
1.326445 
-0.088471 
0.021130 
-0.000431 
-0.064925 
11.177375 
.
.
.
-0.051833 
2.985478 
0.443796 
-0.007483 
-0.021235 
-0.027517 
3.377084 
2.027837 
2.824913 
4.282322 
-0.001016 
5.513213 


final_layer_weights[0] = 0.247141
final_layer_weights[1] = 0.550194
final_layer_weights[2] = 0.005747
final_layer_weights[3] = -0.222823
final_layer_weights[4] = 0.659656
final_layer_weights[5] = 0.771736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.531983
final_layer_weights[9] = 1.018050
.
.
.
final_layer_weights[1022] = 0.263603
final_layer_weights[1023] = -0.103709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1062.857743 


Updated weights for final layer:
final_layer_weights[0] = 0.257141
final_layer_weights[1] = 0.560194
final_layer_weights[2] = -0.004253
final_layer_weights[3] = -0.212823
final_layer_weights[4] = 0.669656
final_layer_weights[5] = 0.761736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.541983
final_layer_weights[9] = 1.028050
.
.
.
final_layer_weights[1022] = 0.273603
final_layer_weights[1023] = -0.113709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.836191
semi_final_layer_weights[1] = -0.310582
semi_final_layer_weights[2] = 0.368389
semi_final_layer_weights[3] = -0.763069
semi_final_layer_weights[4] = -0.337813
semi_final_layer_weights[5] = 0.338136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.516670
semi_final_layer_weights[9] = -0.387157
.
.
.
semi_final_layer_weights[510] = -0.007156
semi_final_layer_weights[511] = 0.921305
Updating K Matrix:
k_matrix[0][0] = -0.642000
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.873000
k_matrix[1][1] = 0.719000

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.832000
q_matrix[1][1] = -0.564000

Updating V Matrix:
v_matrix[0][0] = -0.654743
v_matrix[0][1] = 0.172745
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.683000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.642000 0.354000 
-0.873000 0.719000 

Q Matrix:
0.457000 -0.298000 
0.832000 -0.564000 

V Matrix:
-0.654743 0.172745 
-0.451000 0.683000 
Final K Matrix:
[-24.589198, 20.156079]
[-7.986318, 7.756842]
[7.087226, -3.894637]
[33.420344, -27.714632]
[-40.015962, 28.922388]
[-28.643679, 18.031835]
[-27.436906, 21.821044]
[4.181718, 1.159825]
[6.288309, -3.241622]
[38.932601, -24.822799]
Final Q Matrix:
[23.349749, -15.821990]
[8.656278, -5.947548]
[-5.033195, 3.280841]
[-32.018910, 21.717932]
[34.561464, -23.156400]
[22.372493, -14.790809]
[25.460802, -17.207123]
[0.094262, -0.374600]
[-4.276204, 2.768023]
[-30.686896, 20.313437]
Final V Matrix:
[-12.879583, 19.057499]
[-1.945425, 8.472083]
[7.252443, -1.881954]
[16.914461, -26.504535]
[-28.131934, 23.698649]
[-25.075207, 11.926868]
[-15.608671, 20.002384]
[10.672058, 5.410060]
[6.830524, -1.266288]
[33.502054, -16.802897]
 

Self-Attention Matrix: 
-1.945425 8.472083  
-1.945425 8.472083  
-12.879583 19.057499  
-12.879583 19.057499  
-1.945425 8.472083  
-1.945425 8.472083  
-1.945425 8.472083  
-11.161310 17.394030  
-12.879583 19.057499  
-12.879583 19.057499  


Context Matrix (embedding_matrix + self_attention_matrix:
-1.398897 36.236490 
-8.694162 22.583203 
-23.994832 19.113357 
-11.793777 -20.023178 
21.142700 37.330486 
29.865931 17.888794 
2.494674 36.635156 
-37.506929 31.978417 
-23.966384 20.007577 
-54.321657 4.937465 




semi_final_layer_weights[0] = -0.836191
semi_final_layer_weights[1] = -0.310582
semi_final_layer_weights[2] = 0.368389
semi_final_layer_weights[3] = -0.763069
semi_final_layer_weights[4] = -0.337813
semi_final_layer_weights[5] = 0.338136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.516670
semi_final_layer_weights[9] = -0.387157
.
.
.
semi_final_layer_weights[511] = -0.007156
semi_final_layer_weights[512] = 0.921305


 Semi Final Layer Node Values: 
-0.299671 
-0.046243 
-0.014299 
25.041601 
-0.200908 
16.485728 
0.362172 
0.038017 
1.528727 
18.732278 
.
.
.
-0.063640 
3.634339 
0.477656 
-0.008558 
-0.025638 
-0.033440 
4.120700 
2.444981 
3.434923 
5.244975 
-0.000526 
6.773699 


final_layer_weights[0] = 0.257141
final_layer_weights[1] = 0.560194
final_layer_weights[2] = -0.004253
final_layer_weights[3] = -0.212823
final_layer_weights[4] = 0.669656
final_layer_weights[5] = 0.761736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.541983
final_layer_weights[9] = 1.028050
.
.
.
final_layer_weights[1022] = 0.273603
final_layer_weights[1023] = -0.113709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013082 


Updated weights for final layer:
final_layer_weights[0] = 0.267141
final_layer_weights[1] = 0.570194
final_layer_weights[2] = -0.014253
final_layer_weights[3] = -0.202823
final_layer_weights[4] = 0.679656
final_layer_weights[5] = 0.751736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.551983
final_layer_weights[9] = 1.038050
.
.
.
final_layer_weights[1022] = 0.283603
final_layer_weights[1023] = -0.123709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.826191
semi_final_layer_weights[1] = -0.300582
semi_final_layer_weights[2] = 0.358389
semi_final_layer_weights[3] = -0.753069
semi_final_layer_weights[4] = -0.327813
semi_final_layer_weights[5] = 0.328136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.506670
semi_final_layer_weights[9] = -0.377157
.
.
.
semi_final_layer_weights[510] = 0.002844
semi_final_layer_weights[511] = 0.911305
Updating K Matrix:
k_matrix[0][0] = 0.358000
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.127000
k_matrix[1][1] = -0.281000

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.168000
q_matrix[1][1] = 0.436000

Updating V Matrix:
v_matrix[0][0] = 0.345257
v_matrix[0][1] = -0.827255
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.317000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.358000 -0.646000 
0.127000 -0.281000 

Q Matrix:
-0.543000 0.702000 
-0.168000 0.436000 

V Matrix:
0.345257 -0.827255 
0.549000 -0.317000 
Final K Matrix:
[10.325839, -20.056082]
[10.558323, -20.089542]
[14.619979, -26.210747]
[-4.574528, 10.280240]
[0.000000, 0.000000]
[4.781952, -9.077781]
[-4.417238, 8.047195]
[-5.185124, 11.434248]
[0.513745, -1.922062]
[-9.528258, 18.888788]
Final Q Matrix:
[-14.985468, 25.382356]
[-15.521524, 24.445697]
[-22.256037, 28.053009]
[5.975961, -16.276940]
[0.000000, 0.000000]
[-7.039777, 10.996126]
[6.663576, -8.937406]
[6.877284, -17.662566]
[-0.306436, 4.596588]
[13.646528, -24.799220]
Final V Matrix:
[21.671414, -24.506867]
[18.718877, -24.868812]
[12.696092, -33.705945]
[-21.080410, 11.490336]
[0.000000, 0.000000]
[8.305623, -11.253785]
[-4.888882, 10.241910]
[-22.098860, 12.924986]
[8.683358, -1.638897]
[-23.139770, 22.787296]
 

Self-Attention Matrix: 
21.671414 -24.506867  
21.671414 -24.506867  
21.671414 -24.506867  
18.718877 -24.868812  
20.195145 -24.687840  
21.671414 -24.506867  
18.718877 -24.868812  
18.718877 -24.868812  
21.671414 -24.506867  
18.718877 -24.868812  


Context Matrix (embedding_matrix + self_attention_matrix:
40.772415 2.955181 
44.063996 -4.492865 
63.676655 -27.797396 
19.804683 -63.949489 
20.195145 -24.687840 
31.956516 -15.846354 
6.903276 -26.343249 
18.456404 -64.956680 
16.296361 -5.309906 
3.706780 -57.576904 




semi_final_layer_weights[0] = -0.826191
semi_final_layer_weights[1] = -0.300582
semi_final_layer_weights[2] = 0.358389
semi_final_layer_weights[3] = -0.753069
semi_final_layer_weights[4] = -0.327813
semi_final_layer_weights[5] = 0.328136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.506670
semi_final_layer_weights[9] = -0.377157
.
.
.
semi_final_layer_weights[511] = 0.002844
semi_final_layer_weights[512] = 0.911305


 Semi Final Layer Node Values: 
-0.369535 
-0.115938 
12.500343 
33.997155 
1.800577 
4.958188 
0.019929 
-0.000762 
-0.050598 
20.694651 
.
.
.
4.699451 
-0.026602 
-0.003019 
0.584428 
1.860436 
2.443255 
-0.030235 
-0.017717 
-0.025112 
-0.038635 
-0.000156 
-0.050055 


final_layer_weights[0] = 0.267141
final_layer_weights[1] = 0.570194
final_layer_weights[2] = -0.014253
final_layer_weights[3] = -0.202823
final_layer_weights[4] = 0.679656
final_layer_weights[5] = 0.751736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.551983
final_layer_weights[9] = 1.038050
.
.
.
final_layer_weights[1022] = 0.283603
final_layer_weights[1023] = -0.123709


Output Embedding: 77.022348, 104.677642 

 Expected Embedding: -1.493413, -5.218942 
 loss: 9120.992051 


Updated weights for final layer:
final_layer_weights[0] = 0.277141
final_layer_weights[1] = 0.580194
final_layer_weights[2] = -0.024253
final_layer_weights[3] = -0.192823
final_layer_weights[4] = 0.689656
final_layer_weights[5] = 0.741736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.561983
final_layer_weights[9] = 1.048050
.
.
.
final_layer_weights[1022] = 0.273603
final_layer_weights[1023] = -0.133709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.816191
semi_final_layer_weights[1] = -0.290582
semi_final_layer_weights[2] = 0.348389
semi_final_layer_weights[3] = -0.743069
semi_final_layer_weights[4] = -0.317813
semi_final_layer_weights[5] = 0.318136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.496670
semi_final_layer_weights[9] = -0.367157
.
.
.
semi_final_layer_weights[510] = -0.007156
semi_final_layer_weights[511] = 0.901305
Updating K Matrix:
k_matrix[0][0] = -0.642000
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.873000
k_matrix[1][1] = 0.719000

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.832000
q_matrix[1][1] = -0.564000

Updating V Matrix:
v_matrix[0][0] = -0.654743
v_matrix[0][1] = 0.172745
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.683000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.642000 0.354000 
-0.873000 0.719000 

Q Matrix:
0.457000 -0.298000 
0.832000 -0.564000 

V Matrix:
-0.654743 0.172745 
-0.451000 0.683000 
Final K Matrix:
[-7.785397, 7.675143]
[-33.769317, 29.260203]
[6.630280, 0.371810]
[33.420344, -27.714632]
[27.452163, -22.826492]
[1.696431, -0.419833]
[23.276339, -16.089755]
[0.000000, 0.000000]
[-21.735517, 20.370257]
[-13.667244, 9.734390]
Final Q Matrix:
[8.539032, -5.873722]
[33.466373, -22.784054]
[-1.150605, 0.386362]
[-32.018910, 21.717932]
[-26.355155, 17.880398]
[-0.750716, 0.442941]
[-19.453427, 12.979282]
[0.000000, 0.000000]
[22.902511, -15.691882]
[11.676775, -7.812797]
Final V Matrix:
[-1.686734, 8.472876]
[-14.768634, 29.150105]
[14.208495, 5.811237]
[16.914461, -26.504535]
[13.780890, -21.886629]
[2.683322, 0.515784]
[17.720189, -12.401356]
[0.000000, 0.000000]
[-6.664115, 21.660776]
[-9.874363, 7.822792]
 

Self-Attention Matrix: 
-1.686734 8.472876  
-1.686734 8.472876  
-14.750196 29.120963  
-14.768634 29.150105  
-14.768634 29.150105  
-14.768633 29.150103  
-14.768634 29.150105  
-8.227684 18.811490  
-1.686734 8.472876  
-1.686734 8.472876  


Context Matrix (embedding_matrix + self_attention_matrix:
-8.914897 22.706413 
-9.972336 53.247988 
-48.126539 46.070943 
-13.682828 -9.930572 
-13.526974 -3.208782 
-20.361457 31.319820 
-32.397124 15.451538 
-8.227684 18.811490 
-15.815299 43.760455 
7.022365 17.723737 




semi_final_layer_weights[0] = -0.816191
semi_final_layer_weights[1] = -0.290582
semi_final_layer_weights[2] = 0.348389
semi_final_layer_weights[3] = -0.743069
semi_final_layer_weights[4] = -0.317813
semi_final_layer_weights[5] = 0.318136
semi_final_layer_weights[6] = 0.009025
semi_final_layer_weights[7] = -0.008395
semi_final_layer_weights[8] = -0.496670
semi_final_layer_weights[9] = -0.367157
.
.
.
semi_final_layer_weights[511] = -0.007156
semi_final_layer_weights[512] = 0.901305


 Semi Final Layer Node Values: 
-0.120727 
-0.128657 
-0.003678 
18.289455 
5.636654 
3.804386 
-0.001439 
-0.000972 
-0.143762 
-0.094529 
.
.
.
-0.097951 
5.494362 
0.520889 
-0.011167 
-0.038077 
-0.050369 
6.260642 
3.620484 
5.180174 
8.031980 
-0.000829 
10.440543 


final_layer_weights[0] = 0.277141
final_layer_weights[1] = 0.580194
final_layer_weights[2] = -0.024253
final_layer_weights[3] = -0.192823
final_layer_weights[4] = 0.689656
final_layer_weights[5] = 0.741736
final_layer_weights[6] = 0.202277
final_layer_weights[7] = 0.821416
final_layer_weights[8] = 0.561983
final_layer_weights[9] = 1.048050
.
.
.
final_layer_weights[1022] = 0.273603
final_layer_weights[1023] = -0.133709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.433293 


Updated weights for final layer:
final_layer_weights[0] = 0.287141
final_layer_weights[1] = 0.590194
final_layer_weights[2] = -0.034253
final_layer_weights[3] = -0.182823
final_layer_weights[4] = 0.699656
final_layer_weights[5] = 0.731736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.571983
final_layer_weights[9] = 1.058050
.
.
.
final_layer_weights[1022] = 0.283603
final_layer_weights[1023] = -0.143709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.806191
semi_final_layer_weights[1] = -0.280582
semi_final_layer_weights[2] = 0.338389
semi_final_layer_weights[3] = -0.733069
semi_final_layer_weights[4] = -0.307813
semi_final_layer_weights[5] = 0.308136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.486670
semi_final_layer_weights[9] = -0.357157
.
.
.
semi_final_layer_weights[510] = 0.002844
semi_final_layer_weights[511] = 0.891305
Updating K Matrix:
k_matrix[0][0] = 0.358000
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.127000
k_matrix[1][1] = -0.281000

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.168000
q_matrix[1][1] = 0.436000

Updating V Matrix:
v_matrix[0][0] = 0.345257
v_matrix[0][1] = -0.827255
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.317000

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.358000 -0.646000 
0.127000 -0.281000 

Q Matrix:
-0.543000 0.702000 
-0.168000 0.436000 

V Matrix:
0.345257 -0.827255 
0.549000 -0.317000 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.806191
semi_final_layer_weights[1] = -0.280582
semi_final_layer_weights[2] = 0.338389
semi_final_layer_weights[3] = -0.733069
semi_final_layer_weights[4] = -0.307813
semi_final_layer_weights[5] = 0.308136
semi_final_layer_weights[6] = -0.000975
semi_final_layer_weights[7] = 0.001605
semi_final_layer_weights[8] = -0.486670
semi_final_layer_weights[9] = -0.357157
.
.
.
semi_final_layer_weights[511] = 0.002844
semi_final_layer_weights[512] = 0.891305


 Semi Final Layer Node Values: 
-0.008062 
-0.002806 
0.338389 
-0.007331 
-0.003078 
0.308136 
-0.000010 
0.001605 
-0.004867 
-0.003572 
.
.
.
-0.008356 
0.464314 
0.034967 
-0.000864 
-0.003187 
-0.004248 
0.530465 
0.302547 
0.437191 
0.683380 
0.002844 
0.891305 


final_layer_weights[0] = 0.287141
final_layer_weights[1] = 0.590194
final_layer_weights[2] = -0.034253
final_layer_weights[3] = -0.182823
final_layer_weights[4] = 0.699656
final_layer_weights[5] = 0.731736
final_layer_weights[6] = 0.192277
final_layer_weights[7] = 0.831416
final_layer_weights[8] = 0.571983
final_layer_weights[9] = 1.058050
.
.
.
final_layer_weights[1022] = 0.283603
final_layer_weights[1023] = -0.143709


Output Embedding: -0.000035, -0.000296 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.828416 


Updated weights for final layer:
final_layer_weights[0] = 0.297141
final_layer_weights[1] = 0.600194
final_layer_weights[2] = -0.044253
final_layer_weights[3] = -0.172823
final_layer_weights[4] = 0.709656
final_layer_weights[5] = 0.721736
final_layer_weights[6] = 0.194313
final_layer_weights[7] = 0.828064
final_layer_weights[8] = 0.581983
final_layer_weights[9] = 1.068050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.153709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.796191
semi_final_layer_weights[1] = -0.270582
semi_final_layer_weights[2] = 0.328389
semi_final_layer_weights[3] = -0.723069
semi_final_layer_weights[4] = -0.297813
semi_final_layer_weights[5] = 0.298136
semi_final_layer_weights[6] = 0.001061
semi_final_layer_weights[7] = -0.001747
semi_final_layer_weights[8] = -0.476670
semi_final_layer_weights[9] = -0.347157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.881305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.138212
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.182832
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.172745
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 0  total loss: 28438.482907 ******************************************************************* 

Epoch: 1
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.138212 0.305808 

Q Matrix:
0.457000 -0.298000 
0.182832 -0.474492 

V Matrix:
-0.375737 0.172745 
-0.451000 0.344986 
Final K Matrix:
[-10.502283, 8.694249]
[-8.620841, 5.752828]
[-6.804176, 11.676111]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[12.221496, -6.297289]
[9.873028, -2.337486]
[8.612509, -16.445585]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-8.633151, 3.321286]
[-4.647024, 0.547849]
[-16.246497, 11.664900]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-4.647024 0.547849  
-4.647024 0.547849  
-8.633151 3.321286  
-6.640088 1.934568  
-6.640088 1.934568  
-6.640088 1.934568  
-6.640088 1.934568  
-6.640088 1.934568  
-6.640088 1.934568  
-6.640088 1.934568  


Context Matrix (embedding_matrix + self_attention_matrix:
23.978788 -4.158665 
21.574539 -10.994055 
-1.982512 33.803777 
-6.640088 1.934568 
-6.640088 1.934568 
-6.640088 1.934568 
-6.640088 1.934568 
-6.640088 1.934568 
-6.640088 1.934568 
-6.640088 1.934568 




semi_final_layer_weights[0] = -0.796191
semi_final_layer_weights[1] = -0.270582
semi_final_layer_weights[2] = 0.328389
semi_final_layer_weights[3] = -0.723069
semi_final_layer_weights[4] = -0.297813
semi_final_layer_weights[5] = 0.298136
semi_final_layer_weights[6] = 0.001061
semi_final_layer_weights[7] = -0.001747
semi_final_layer_weights[8] = -0.476670
semi_final_layer_weights[9] = -0.347157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.881305


 Semi Final Layer Node Values: 
-0.149844 
-0.025923 
10.778142 
2.679347 
1.103552 
-0.011047 
-0.000039 
0.006472 
1.766310 
1.286397 
.
.
.
3.059211 
-0.016835 
-0.000925 
0.283105 
1.143935 
1.537120 
-0.019286 
-0.010840 
-0.015830 
-0.024952 
0.011469 
-0.032657 


final_layer_weights[0] = 0.297141
final_layer_weights[1] = 0.600194
final_layer_weights[2] = -0.044253
final_layer_weights[3] = -0.172823
final_layer_weights[4] = 0.709656
final_layer_weights[5] = 0.721736
final_layer_weights[6] = 0.194313
final_layer_weights[7] = 0.828064
final_layer_weights[8] = 0.581983
final_layer_weights[9] = 1.068050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.153709


Output Embedding: 47.884707, 60.838538 

 Expected Embedding: 12.470385, -17.532656 
 loss: 3698.109126 


Updated weights for final layer:
final_layer_weights[0] = 0.307141
final_layer_weights[1] = 0.610194
final_layer_weights[2] = -0.054253
final_layer_weights[3] = -0.162823
final_layer_weights[4] = 0.719656
final_layer_weights[5] = 0.711736
final_layer_weights[6] = 0.184313
final_layer_weights[7] = 0.838064
final_layer_weights[8] = 0.591983
final_layer_weights[9] = 1.078050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.163709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.786191
semi_final_layer_weights[1] = -0.260582
semi_final_layer_weights[2] = 0.318389
semi_final_layer_weights[3] = -0.713069
semi_final_layer_weights[4] = -0.287813
semi_final_layer_weights[5] = 0.288136
semi_final_layer_weights[6] = -0.008939
semi_final_layer_weights[7] = 0.008253
semi_final_layer_weights[8] = -0.466670
semi_final_layer_weights[9] = -0.337157
.
.
.
semi_final_layer_weights[510] = 0.006905
semi_final_layer_weights[511] = 0.871305
Updating K Matrix:
k_matrix[0][0] = 0.610394
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.861788
k_matrix[1][1] = -0.694192

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.817168
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.624263
v_matrix[0][1] = -0.827255
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.655014

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.610394 -0.646000 
0.861788 -0.694192 

Q Matrix:
-0.543000 0.702000 
-0.817168 0.525508 

V Matrix:
0.624263 -0.827255 
0.549000 -0.655014 
Final K Matrix:
[15.188453, -15.523029]
[12.098021, -9.916703]
[30.328954, -25.457019]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-13.639371, 16.289447]
[-11.431875, 7.743622]
[-28.520621, 20.687545]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[14.692340, -19.286364]
[7.968556, -9.598909]
[20.886633, -25.468230]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.968556 -9.598909  
7.968556 -9.598909  
7.968556 -9.598909  
11.330448 -14.442637  
11.330448 -14.442637  
11.330448 -14.442637  
11.330448 -14.442637  
11.330448 -14.442637  
11.330448 -14.442637  
11.330448 -14.442637  


Context Matrix (embedding_matrix + self_attention_matrix:
29.278279 -7.067984 
9.079544 3.652473 
14.619196 20.883581 
11.330448 -14.442637 
11.330448 -14.442637 
11.330448 -14.442637 
11.330448 -14.442637 
11.330448 -14.442637 
11.330448 -14.442637 
11.330448 -14.442637 




semi_final_layer_weights[0] = -0.786191
semi_final_layer_weights[1] = -0.260582
semi_final_layer_weights[2] = 0.318389
semi_final_layer_weights[3] = -0.713069
semi_final_layer_weights[4] = -0.287813
semi_final_layer_weights[5] = 0.288136
semi_final_layer_weights[6] = -0.008939
semi_final_layer_weights[7] = 0.008253
semi_final_layer_weights[8] = -0.466670
semi_final_layer_weights[9] = -0.337157
.
.
.
semi_final_layer_weights[511] = 0.006905
semi_final_layer_weights[512] = 0.871305


 Semi Final Layer Node Values: 
-0.166753 
-0.035783 
11.622083 
2.932274 
1.183541 
-0.011849 
0.036759 
-0.000339 
1.919035 
1.386453 
.
.
.
3.353827 
-0.018271 
-0.000615 
0.273053 
1.228356 
1.664692 
-0.020991 
-0.011619 
-0.017156 
-0.027279 
-0.000284 
-0.035830 


final_layer_weights[0] = 0.307141
final_layer_weights[1] = 0.610194
final_layer_weights[2] = -0.054253
final_layer_weights[3] = -0.162823
final_layer_weights[4] = 0.719656
final_layer_weights[5] = 0.711736
final_layer_weights[6] = 0.184313
final_layer_weights[7] = 0.838064
final_layer_weights[8] = 0.591983
final_layer_weights[9] = 1.078050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.163709


Output Embedding: 56.680185, 70.389508 

 Expected Embedding: 12.470385, -17.532656 
 loss: 4842.406642 


Updated weights for final layer:
final_layer_weights[0] = 0.317141
final_layer_weights[1] = 0.620194
final_layer_weights[2] = -0.064253
final_layer_weights[3] = -0.152823
final_layer_weights[4] = 0.729656
final_layer_weights[5] = 0.701736
final_layer_weights[6] = 0.194313
final_layer_weights[7] = 0.828064
final_layer_weights[8] = 0.601983
final_layer_weights[9] = 1.088050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.173709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.776191
semi_final_layer_weights[1] = -0.250582
semi_final_layer_weights[2] = 0.308389
semi_final_layer_weights[3] = -0.703069
semi_final_layer_weights[4] = -0.277813
semi_final_layer_weights[5] = 0.278136
semi_final_layer_weights[6] = 0.001061
semi_final_layer_weights[7] = -0.001747
semi_final_layer_weights[8] = -0.456670
semi_final_layer_weights[9] = -0.327157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.861305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.138212
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.182832
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.172745
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.138212 0.305808 

Q Matrix:
0.457000 -0.298000 
0.182832 -0.474492 

V Matrix:
-0.375737 0.172745 
-0.451000 0.344986 
Final K Matrix:
[3.183207, -2.529987]
[-16.902750, 14.424798]
[0.154848, -3.611556]
[-6.754258, 4.532123]
[-19.126046, 17.282624]
[-11.253821, 14.707032]
[6.944351, -8.108153]
[15.293501, -15.028656]
[-15.213597, 17.562954]
[5.043541, -10.473548]
Final Q Matrix:
[-3.692204, 1.693407]
[19.719354, -11.018958]
[-0.580504, 7.220448]
[7.738184, -1.882316]
[22.423507, -14.433621]
[13.715549, -17.778122]
[-8.352266, 8.991502]
[-18.069164, 14.015574]
[18.275023, -19.288645]
[-6.592961, 15.911596]
Final V Matrix:
[2.431211, -0.841052]
[-14.655976, 6.025374]
[6.267840, -5.532328]
[-3.684739, 0.468419]
[-18.276876, 8.329843]
[-18.753647, 12.044640]
[9.867491, -5.910041]
[16.746068, -8.564125]
[-21.264505, 12.632360]
[15.248670, -11.509467]
 

Self-Attention Matrix: 
-14.655976 6.025374  
2.431211 -0.841052  
-14.655976 6.025374  
2.431211 -0.841052  
2.431211 -0.841052  
2.431211 -0.841052  
-14.655976 6.025374  
-14.655976 6.025374  
2.431211 -0.841052  
-14.655976 6.025374  


Context Matrix (embedding_matrix + self_attention_matrix:
-23.539456 8.035672 
47.652364 -6.019110 
-8.221574 -13.232910 
22.926284 -9.745770 
51.709978 -1.370990 
22.494856 24.025876 
-28.940070 -3.953390 
-51.679994 -0.260123 
34.118864 19.908986 
-16.005836 -26.660829 




semi_final_layer_weights[0] = -0.776191
semi_final_layer_weights[1] = -0.250582
semi_final_layer_weights[2] = 0.308389
semi_final_layer_weights[3] = -0.703069
semi_final_layer_weights[4] = -0.277813
semi_final_layer_weights[5] = 0.278136
semi_final_layer_weights[6] = 0.001061
semi_final_layer_weights[7] = -0.001747
semi_final_layer_weights[8] = -0.456670
semi_final_layer_weights[9] = -0.327157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.861305


 Semi Final Layer Node Values: 
11.257706 
-0.101820 
-0.069247 
-0.085637 
-0.137070 
13.217226 
-0.000360 
0.092470 
-0.251296 
14.285855 
.
.
.
2.030245 
-0.010946 
-0.000125 
0.142143 
0.727616 
0.995031 
-0.012613 
-0.006869 
-0.010262 
-0.016467 
0.007800 
-0.021707 


final_layer_weights[0] = 0.317141
final_layer_weights[1] = 0.620194
final_layer_weights[2] = -0.064253
final_layer_weights[3] = -0.152823
final_layer_weights[4] = 0.729656
final_layer_weights[5] = 0.701736
final_layer_weights[6] = 0.194313
final_layer_weights[7] = 0.828064
final_layer_weights[8] = 0.601983
final_layer_weights[9] = 1.088050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.173709


Output Embedding: 23.617331, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 338.045165 


Updated weights for final layer:
final_layer_weights[0] = 0.327141
final_layer_weights[1] = 0.630194
final_layer_weights[2] = -0.074253
final_layer_weights[3] = -0.142823
final_layer_weights[4] = 0.739656
final_layer_weights[5] = 0.691736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.611983
final_layer_weights[9] = 1.098050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.183709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.766191
semi_final_layer_weights[1] = -0.240582
semi_final_layer_weights[2] = 0.298389
semi_final_layer_weights[3] = -0.693069
semi_final_layer_weights[4] = -0.267813
semi_final_layer_weights[5] = 0.268136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.446670
semi_final_layer_weights[9] = -0.317157
.
.
.
semi_final_layer_weights[510] = 0.006905
semi_final_layer_weights[511] = 0.851305
Updating K Matrix:
k_matrix[0][0] = 0.610394
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.329007
k_matrix[1][1] = -0.694192

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.435222
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.624263
v_matrix[0][1] = -0.411211
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.655014

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.610394 -0.646000 
0.329007 -0.694192 

Q Matrix:
-0.543000 0.702000 
-0.435222 0.525508 

V Matrix:
0.624263 -0.411211 
0.549000 -0.655014 
Final K Matrix:
[13.726086, -25.232547]
[0.607665, -2.986566]
[11.929878, -17.845414]
[30.099628, -44.302783]
[13.742546, -25.836401]
[14.838041, -22.727734]
[-6.017312, 6.155970]
[10.716269, -16.050064]
[-11.274315, 9.957491]
[8.902957, -12.717269]
Final Q Matrix:
[-16.621103, 20.338407]
[-1.506019, 1.695355]
[-12.763054, 15.939787]
[-31.904318, 39.909795]
[-16.877342, 20.606703]
[-16.093542, 20.051721]
[5.265453, -6.830074]
[-11.472942, 14.326776]
[9.216067, -12.126725]
[-9.277426, 11.640186]
Final V Matrix:
[20.613708, -22.656360]
[2.060880, -3.344626]
[15.406960, -14.574706]
[38.428987, -35.868305]
[20.990744, -23.401976]
[19.489577, -18.794099]
[-6.023599, 3.787781]
[13.851940, -13.117140]
[-10.317694, 5.122194]
[11.129094, -10.124830]
 

Self-Attention Matrix: 
2.060880 -3.344626  
2.060880 -3.344626  
2.060880 -3.344626  
2.060880 -3.344626  
2.060880 -3.344626  
2.060880 -3.344626  
20.613708 -22.656360  
2.060880 -3.344626  
20.613708 -22.656360  
2.060880 -3.344626  


Context Matrix (embedding_matrix + self_attention_matrix:
7.870048 27.597562 
-0.594355 3.428495 
13.473978 11.741331 
31.981442 32.631147 
6.983436 29.292490 
15.427199 16.956802 
10.424829 -22.042631 
12.281724 10.264572 
-0.932795 -16.949654 
11.513356 6.178629 




semi_final_layer_weights[0] = -0.766191
semi_final_layer_weights[1] = -0.240582
semi_final_layer_weights[2] = 0.298389
semi_final_layer_weights[3] = -0.693069
semi_final_layer_weights[4] = -0.267813
semi_final_layer_weights[5] = 0.268136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.446670
semi_final_layer_weights[9] = -0.317157
.
.
.
semi_final_layer_weights[511] = 0.006905
semi_final_layer_weights[512] = 0.851305


 Semi Final Layer Node Values: 
-0.279412 
-0.009224 
7.822360 
-0.454741 
-0.099830 
8.951453 
0.031871 
0.097904 
8.434224 
-0.059283 
.
.
.
2.118793 
-0.011300 
0.013404 
0.123575 
0.742263 
1.024850 
-0.013062 
-0.006992 
-0.010578 
-0.017134 
-0.000184 
-0.022672 


final_layer_weights[0] = 0.327141
final_layer_weights[1] = 0.630194
final_layer_weights[2] = -0.074253
final_layer_weights[3] = -0.142823
final_layer_weights[4] = 0.739656
final_layer_weights[5] = 0.691736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.611983
final_layer_weights[9] = 1.098050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.183709


Output Embedding: 44.584766, 73.302288 

 Expected Embedding: 4.643339, -27.140186 
 loss: 5842.004121 


Updated weights for final layer:
final_layer_weights[0] = 0.337141
final_layer_weights[1] = 0.640194
final_layer_weights[2] = -0.084253
final_layer_weights[3] = -0.132823
final_layer_weights[4] = 0.749656
final_layer_weights[5] = 0.681736
final_layer_weights[6] = 0.200726
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.621983
final_layer_weights[9] = 1.108050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.193709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.756191
semi_final_layer_weights[1] = -0.230582
semi_final_layer_weights[2] = 0.288389
semi_final_layer_weights[3] = -0.683069
semi_final_layer_weights[4] = -0.257813
semi_final_layer_weights[5] = 0.258136
semi_final_layer_weights[6] = 0.007474
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.436670
semi_final_layer_weights[9] = -0.307157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.841305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.670993
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.564778
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.588789
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.670993 0.305808 

Q Matrix:
0.457000 -0.298000 
0.564778 -0.474492 

V Matrix:
-0.375737 0.588789 
-0.451000 0.344986 
Final K Matrix:
[-10.793663, 6.628324]
[10.875908, -7.417274]
[25.545529, -11.478438]
[-25.162797, 10.791257]
[-13.542216, 13.595106]
[14.912959, -6.898105]
[-13.269500, 7.937741]
[-34.595593, 20.808256]
[-30.593628, 13.792682]
[3.470836, -3.576898]
Final Q Matrix:
[10.335302, -7.850587]
[-10.954245, 8.004538]
[-21.381791, 18.043584]
[20.684555, -17.707574]
[16.828792, -10.522617]
[-12.626530, 10.558617]
[12.551664, -9.624451]
[32.807016, -25.106850]
[25.640713, -21.615070]
[-4.380848, 2.708712]
Final V Matrix:
[-8.357837, 9.312504]
[8.898101, -11.009370]
[17.064257, -12.772851]
[-16.476089, 11.447101]
[-13.893039, 23.306998]
[10.089052, -7.890785]
[-10.138784, 10.984060]
[-26.506508, 28.886713]
[-20.466023, 15.398130]
[3.620453, -6.177206]
 

Self-Attention Matrix: 
8.898101 -11.009370  
-8.357837 9.312504  
-8.357837 9.312504  
8.898101 -11.009370  
8.898101 -11.009370  
-8.357837 9.312504  
8.898101 -11.009370  
8.898101 -11.009370  
8.898101 -11.009370  
-8.357837 9.312504  


Context Matrix (embedding_matrix + self_attention_matrix:
18.584731 -0.547713 
-22.303625 1.201316 
-7.428055 -29.298606 
5.062165 28.718776 
50.971168 -15.256348 
-8.932892 -12.578809 
19.610849 2.546292 
37.470386 23.959212 
8.045122 35.080474 
-19.665366 10.705430 




semi_final_layer_weights[0] = -0.756191
semi_final_layer_weights[1] = -0.230582
semi_final_layer_weights[2] = 0.288389
semi_final_layer_weights[3] = -0.683069
semi_final_layer_weights[4] = -0.257813
semi_final_layer_weights[5] = 0.258136
semi_final_layer_weights[6] = 0.007474
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.436670
semi_final_layer_weights[9] = -0.307157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.841305


 Semi Final Layer Node Values: 
-0.128832 
4.635231 
-0.108800 
-0.237578 
-0.089499 
-0.058111 
0.173080 
-0.003647 
-0.192683 
2.444950 
.
.
.
1.239885 
-0.006539 
-0.000078 
0.057452 
0.424107 
0.591577 
-0.007583 
-0.003986 
-0.006111 
-0.009997 
0.004885 
-0.013278 


final_layer_weights[0] = 0.337141
final_layer_weights[1] = 0.640194
final_layer_weights[2] = -0.084253
final_layer_weights[3] = -0.132823
final_layer_weights[4] = 0.749656
final_layer_weights[5] = 0.681736
final_layer_weights[6] = 0.200726
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.621983
final_layer_weights[9] = 1.108050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.193709


Output Embedding: 31.448536, 19.877329 

 Expected Embedding: 44.423141, -6.847703 
 loss: 441.283859 


Updated weights for final layer:
final_layer_weights[0] = 0.347141
final_layer_weights[1] = 0.650194
final_layer_weights[2] = -0.094253
final_layer_weights[3] = -0.122823
final_layer_weights[4] = 0.759656
final_layer_weights[5] = 0.671736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.631983
final_layer_weights[9] = 1.118050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.203709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.746191
semi_final_layer_weights[1] = -0.220582
semi_final_layer_weights[2] = 0.278389
semi_final_layer_weights[3] = -0.673069
semi_final_layer_weights[4] = -0.247813
semi_final_layer_weights[5] = 0.248136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.426670
semi_final_layer_weights[9] = -0.297157
.
.
.
semi_final_layer_weights[510] = 0.006905
semi_final_layer_weights[511] = 0.831305
Updating K Matrix:
k_matrix[0][0] = 0.610394
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.329007
k_matrix[1][1] = -0.694192

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.435222
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.624263
v_matrix[0][1] = -0.411211
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.655014

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.610394 -0.646000 
0.329007 -0.694192 

Q Matrix:
-0.543000 0.702000 
-0.435222 0.525508 

V Matrix:
0.624263 -0.411211 
0.549000 -0.655014 
Final K Matrix:
[-19.698721, 32.608892]
[-2.373988, 0.256333]
[-18.844998, 21.407196]
[-5.145598, 4.866402]
[-7.039585, 7.761819]
[-14.408162, 21.209819]
[-9.758674, 19.198061]
[-20.336487, 22.769477]
[3.635035, -5.848569]
[-7.266147, 21.168276]
Final Q Matrix:
[22.369057, -27.656138]
[1.182400, -1.770904]
[17.366992, -22.295257]
[4.338789, -5.671477]
[6.390708, -8.228559]
[15.273212, -19.105331]
[12.335496, -14.995020]
[18.604731, -23.918646]
[-4.058255, 5.031652]
[12.016614, -14.087915]
Final V Matrix:
[-27.370242, 28.001826]
[-1.042151, -1.226572]
[-20.171726, 14.527887]
[-4.906656, 2.740838]
[-7.390917, 5.132725]
[-18.397034, 17.173104]
[-15.428656, 17.684381]
[-21.564300, 15.261866]
[4.946992, -4.955793]
[-15.709932, 21.777056]
 

Self-Attention Matrix: 
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-1.042151 -1.226572  
-27.370242 28.001826  
-1.042151 -1.226572  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.992176 -35.218855 
-8.446156 5.294179 
-29.636609 -5.454762 
-10.374655 0.447886 
-12.089574 -2.127164 
-15.360148 -18.455797 
-3.211240 -26.863322 
-32.416916 -4.829867 
-24.533057 33.786605 
8.051046 -40.181898 




semi_final_layer_weights[0] = -0.746191
semi_final_layer_weights[1] = -0.220582
semi_final_layer_weights[2] = 0.278389
semi_final_layer_weights[3] = -0.673069
semi_final_layer_weights[4] = -0.247813
semi_final_layer_weights[5] = 0.248136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.426670
semi_final_layer_weights[9] = -0.297157
.
.
.
semi_final_layer_weights[511] = 0.006905
semi_final_layer_weights[512] = 0.831305


 Semi Final Layer Node Values: 
38.213211 
0.474687 
-0.100474 
6.008332 
3.770905 
-0.086391 
0.078489 
-0.001590 
-0.043749 
9.845065 
.
.
.
-0.001407 
0.073355 
-0.000009 
-0.000048 
-0.000469 
-0.000662 
0.085357 
0.044005 
0.068434 
0.113100 
0.001253 
0.150824 


final_layer_weights[0] = 0.347141
final_layer_weights[1] = 0.650194
final_layer_weights[2] = -0.094253
final_layer_weights[3] = -0.122823
final_layer_weights[4] = 0.759656
final_layer_weights[5] = 0.671736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.631983
final_layer_weights[9] = 1.118050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.203709


Output Embedding: 27.001999, -0.278395 

 Expected Embedding: 28.923067, 35.905037 
 loss: 656.465616 


Updated weights for final layer:
final_layer_weights[0] = 0.357141
final_layer_weights[1] = 0.660194
final_layer_weights[2] = -0.104253
final_layer_weights[3] = -0.112823
final_layer_weights[4] = 0.769656
final_layer_weights[5] = 0.661736
final_layer_weights[6] = 0.200726
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.641983
final_layer_weights[9] = 1.128050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.213709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.736191
semi_final_layer_weights[1] = -0.210582
semi_final_layer_weights[2] = 0.268389
semi_final_layer_weights[3] = -0.663069
semi_final_layer_weights[4] = -0.237813
semi_final_layer_weights[5] = 0.238136
semi_final_layer_weights[6] = 0.007474
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.416670
semi_final_layer_weights[9] = -0.287157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.821305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.670993
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.564778
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.588789
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.670993 0.305808 

Q Matrix:
0.457000 -0.298000 
0.564778 -0.474492 

V Matrix:
-0.375737 0.588789 
-0.451000 0.344986 
Final K Matrix:
[-18.842647, 8.684044]
[-6.839113, 1.926238]
[4.293085, -3.917717]
[25.799817, -11.566803]
[-28.359046, 16.998322]
[-18.712432, 14.140924]
[-20.627107, 10.184284]
[0.478386, -4.866329]
[3.681986, -3.634187]
[25.620511, -18.988511]
Final Q Matrix:
[15.930486, -13.336851]
[4.885475, -4.684488]
[-5.048122, 3.285840]
[-21.575689, 18.219894]
[26.849861, -20.573339]
[19.856140, -13.947937]
[17.935006, -14.686299]
[-3.803009, 0.930821]
[-4.530086, 2.853063]
[-26.913711, 19.049580]
Final V Matrix:
[-12.727098, 9.900123]
[-3.828362, 0.894560]
[4.151224, -6.525263]
[17.217407, -12.842979]
[-21.690212, 23.549774]
[-16.199654, 21.978801]
[-14.369857, 12.330148]
[3.321477, -10.480593]
[3.737242, -6.200020]
[21.939474, -29.271840]
 

Self-Attention Matrix: 
-3.828362 0.894560  
-3.828362 0.894560  
-12.727098 9.900123  
-12.727098 9.900123  
-3.828362 0.894560  
-3.828362 0.894560  
-3.828362 0.894560  
-12.727098 9.900123  
-12.727098 9.900123  
-12.727098 9.900123  


Context Matrix (embedding_matrix + self_attention_matrix:
-3.281834 28.658966 
-10.577099 15.005679 
-23.842348 9.955981 
-11.641292 -29.180554 
19.259764 29.752962 
27.982994 10.311271 
0.611738 29.057632 
-39.072717 24.484510 
-23.813900 10.850200 
-54.169172 -4.219911 




semi_final_layer_weights[0] = -0.736191
semi_final_layer_weights[1] = -0.210582
semi_final_layer_weights[2] = 0.268389
semi_final_layer_weights[3] = -0.663069
semi_final_layer_weights[4] = -0.237813
semi_final_layer_weights[5] = 0.238136
semi_final_layer_weights[6] = 0.007474
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.416670
semi_final_layer_weights[9] = -0.287157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.821305


 Semi Final Layer Node Values: 
-0.194186 
-0.011432 
-0.034586 
27.730770 
-0.118937 
9.357379 
0.229228 
0.079383 
4.984915 
17.053991 
.
.
.
1.439592 
-0.007415 
-0.000093 
0.030840 
0.467673 
0.667198 
-0.008659 
-0.004373 
-0.006905 
-0.011534 
0.005820 
-0.015444 


final_layer_weights[0] = 0.357141
final_layer_weights[1] = 0.660194
final_layer_weights[2] = -0.104253
final_layer_weights[3] = -0.112823
final_layer_weights[4] = 0.769656
final_layer_weights[5] = 0.661736
final_layer_weights[6] = 0.200726
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.641983
final_layer_weights[9] = 1.128050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.213709


Output Embedding: 59.664111, 69.763594 

 Expected Embedding: 1.038754, 36.179375 
 loss: 2282.416101 


Updated weights for final layer:
final_layer_weights[0] = 0.367141
final_layer_weights[1] = 0.670194
final_layer_weights[2] = -0.114253
final_layer_weights[3] = -0.102823
final_layer_weights[4] = 0.779656
final_layer_weights[5] = 0.651736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.651983
final_layer_weights[9] = 1.138050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.223709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.726191
semi_final_layer_weights[1] = -0.200582
semi_final_layer_weights[2] = 0.258389
semi_final_layer_weights[3] = -0.653069
semi_final_layer_weights[4] = -0.227813
semi_final_layer_weights[5] = 0.228136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.406670
semi_final_layer_weights[9] = -0.277157
.
.
.
semi_final_layer_weights[510] = 0.006905
semi_final_layer_weights[511] = 0.811305
Updating K Matrix:
k_matrix[0][0] = 0.610394
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.329007
k_matrix[1][1] = -0.694192

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.435222
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.624263
v_matrix[0][1] = -0.411211
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.655014

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.610394 -0.646000 
0.329007 -0.694192 

Q Matrix:
-0.543000 0.702000 
-0.435222 0.525508 

V Matrix:
0.624263 -0.411211 
0.549000 -0.655014 
Final K Matrix:
[20.694353, -31.403185]
[20.253054, -28.359171]
[24.557151, -24.851127]
[-12.195054, 26.428069]
[0.000000, 0.000000]
[9.127339, -12.656237]
[-7.697275, 8.656420]
[-13.349409, 27.998240]
[3.035037, -9.854095]
[-19.924495, 32.403515]
Final Q Matrix:
[-22.323934, 27.840432]
[-20.869708, 26.237113]
[-21.376735, 27.758480]
[16.419182, -19.774977]
[0.000000, 0.000000]
[-9.354058, 11.771312]
[7.057579, -9.069380]
[17.589649, -21.250755]
[-5.436288, 6.314871]
[22.386853, -27.726859]
Final V Matrix:
[27.000704, -25.842571]
[24.966538, -22.317532]
[24.415799, -15.117687]
[-20.777464, 25.151892]
[0.000000, 0.000000]
[11.175226, -9.902107]
[-8.185503, 5.824485]
[-22.172091, 26.366043]
[7.183687, -10.363994]
[-27.328232, 27.597399]
 

Self-Attention Matrix: 
27.000704 -25.842571  
27.000704 -25.842571  
27.000704 -25.842571  
24.966538 -22.317532  
25.983621 -24.080052  
27.000704 -25.842571  
24.966538 -22.317532  
24.966538 -22.317532  
27.000704 -25.842571  
24.966538 -22.317532  


Context Matrix (embedding_matrix + self_attention_matrix:
46.101706 1.619477 
49.393286 -5.828569 
69.005946 -29.133100 
26.052343 -61.398209 
25.983621 -24.080052 
37.285807 -17.182058 
13.150937 -23.791969 
24.704065 -62.405400 
21.625652 -6.645611 
9.954441 -55.025624 




semi_final_layer_weights[0] = -0.726191
semi_final_layer_weights[1] = -0.200582
semi_final_layer_weights[2] = 0.258389
semi_final_layer_weights[3] = -0.653069
semi_final_layer_weights[4] = -0.227813
semi_final_layer_weights[5] = 0.228136
semi_final_layer_weights[6] = -0.002526
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.406670
semi_final_layer_weights[9] = -0.277157
.
.
.
semi_final_layer_weights[511] = 0.006905
semi_final_layer_weights[512] = 0.811305


 Semi Final Layer Node Values: 
-0.353809 
-0.085377 
10.044316 
23.736358 
-0.002058 
4.358253 
0.029403 
-0.001609 
-0.056853 
12.768951 
.
.
.
-0.006827 
0.347254 
-0.000045 
-0.000058 
-0.002157 
-0.003116 
0.407026 
0.201087 
0.322747 
0.545196 
0.006239 
0.733070 


final_layer_weights[0] = 0.367141
final_layer_weights[1] = 0.670194
final_layer_weights[2] = -0.114253
final_layer_weights[3] = -0.102823
final_layer_weights[4] = 0.779656
final_layer_weights[5] = 0.651736
final_layer_weights[6] = 0.190726
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.651983
final_layer_weights[9] = 1.138050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.223709


Output Embedding: -0.237261, 19.082772 

 Expected Embedding: -1.493413, -5.218942 
 loss: 296.075607 


Updated weights for final layer:
final_layer_weights[0] = 0.377141
final_layer_weights[1] = 0.680194
final_layer_weights[2] = -0.124253
final_layer_weights[3] = -0.092823
final_layer_weights[4] = 0.789656
final_layer_weights[5] = 0.641736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.661983
final_layer_weights[9] = 1.148050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.233709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.716191
semi_final_layer_weights[1] = -0.190582
semi_final_layer_weights[2] = 0.248389
semi_final_layer_weights[3] = -0.643069
semi_final_layer_weights[4] = -0.217813
semi_final_layer_weights[5] = 0.218136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.396670
semi_final_layer_weights[9] = -0.267157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.801305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.645103
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.564778
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.588789
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.645103 0.305808 

Q Matrix:
0.457000 -0.298000 
0.564778 -0.474492 

V Matrix:
-0.375737 0.588789 
-0.451000 0.344986 
Final K Matrix:
[-6.365962, 1.793958]
[-25.656435, 10.759478]
[2.069134, -6.631788]
[24.788020, -11.566803]
[20.391052, -9.456054]
[0.779306, -1.316343]
[15.705145, -10.429615]
[0.000000, 0.000000]
[-17.259548, 5.789707]
[-9.360872, 5.912007]
Final Q Matrix:
[4.735517, -4.599705]
[21.501473, -18.776319]
[-5.680015, 1.903522]
[-21.575689, 18.219894]
[-17.708145, 14.984015]
[-1.330513, 0.637149]
[-15.792868, 11.753149]
[0.000000, 0.000000]
[13.472890, -12.533358]
[9.204740, -6.984770]
Final V Matrix:
[-3.703434, 0.654512]
[-17.080365, 10.568322]
[4.896301, -13.804107]
[17.217407, -12.842979]
[14.127320, -10.432290]
[1.122891, -2.544470]
[12.801737, -15.105271]
[0.000000, 0.000000]
[-10.606067, 3.854984]
[-7.444473, 8.319238]
 

Self-Attention Matrix: 
-3.703434 0.654512  
-3.703434 0.654512  
-17.080365 10.568322  
-17.080365 10.568322  
-17.080365 10.568322  
-17.080307 10.568279  
-17.080365 10.568322  
-10.391899 5.611417  
-3.703434 0.654512  
-3.703434 0.654512  


Context Matrix (embedding_matrix + self_attention_matrix:
-10.931596 14.888048 
-11.989036 45.429624 
-50.456708 27.518302 
-15.994559 -28.512355 
-15.838705 -21.790565 
-22.673131 12.737995 
-34.708855 -3.130245 
-10.391899 5.611417 
-17.831999 35.942090 
5.005666 9.905373 




semi_final_layer_weights[0] = -0.716191
semi_final_layer_weights[1] = -0.190582
semi_final_layer_weights[2] = 0.248389
semi_final_layer_weights[3] = -0.643069
semi_final_layer_weights[4] = -0.217813
semi_final_layer_weights[5] = 0.218136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.005842
semi_final_layer_weights[8] = -0.396670
semi_final_layer_weights[9] = -0.267157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.801305


 Semi Final Layer Node Values: 
-0.035498 
-0.065638 
-0.054493 
29.264086 
8.413957 
-0.019491 
-0.001924 
0.022086 
-0.075804 
-0.042507 
.
.
.
2.818660 
-0.014151 
-0.000188 
-0.000136 
0.864638 
1.265777 
-0.016652 
-0.008035 
-0.013125 
-0.022433 
0.011701 
-0.030293 


final_layer_weights[0] = 0.377141
final_layer_weights[1] = 0.680194
final_layer_weights[2] = -0.124253
final_layer_weights[3] = -0.092823
final_layer_weights[4] = 0.789656
final_layer_weights[5] = 0.641736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.823969
final_layer_weights[8] = 0.661983
final_layer_weights[9] = 1.148050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.233709


Output Embedding: 101.105913, 64.864444 

 Expected Embedding: -37.748848, 10.192697 
 loss: 11134.822302 


Updated weights for final layer:
final_layer_weights[0] = 0.387141
final_layer_weights[1] = 0.690194
final_layer_weights[2] = -0.134253
final_layer_weights[3] = -0.082823
final_layer_weights[4] = 0.799656
final_layer_weights[5] = 0.631736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.671983
final_layer_weights[9] = 1.158050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.243709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.706191
semi_final_layer_weights[1] = -0.180582
semi_final_layer_weights[2] = 0.238389
semi_final_layer_weights[3] = -0.633069
semi_final_layer_weights[4] = -0.207813
semi_final_layer_weights[5] = 0.208136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.386670
semi_final_layer_weights[9] = -0.257157
.
.
.
semi_final_layer_weights[510] = 0.006905
semi_final_layer_weights[511] = 0.791305
Updating K Matrix:
k_matrix[0][0] = 0.610394
k_matrix[0][1] = -0.646000
k_matrix[1][0] = 0.354897
k_matrix[1][1] = -0.694192

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.702000
q_matrix[1][0] = -0.435222
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.624263
v_matrix[0][1] = -0.411211
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.655014

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.610394 -0.646000 
0.354897 -0.694192 

Q Matrix:
-0.543000 0.702000 
-0.435222 0.525508 

V Matrix:
0.624263 -0.411211 
0.549000 -0.655014 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.706191
semi_final_layer_weights[1] = -0.180582
semi_final_layer_weights[2] = 0.238389
semi_final_layer_weights[3] = -0.633069
semi_final_layer_weights[4] = -0.207813
semi_final_layer_weights[5] = 0.208136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.004158
semi_final_layer_weights[8] = -0.386670
semi_final_layer_weights[9] = -0.257157
.
.
.
semi_final_layer_weights[511] = 0.006905
semi_final_layer_weights[512] = 0.791305


 Semi Final Layer Node Values: 
-0.007062 
-0.001806 
0.238389 
-0.006331 
-0.002078 
0.208136 
-0.000050 
0.004158 
-0.003867 
-0.002572 
.
.
.
-0.007356 
0.364314 
-0.000050 
-0.000064 
-0.002187 
-0.003248 
0.430465 
0.202547 
0.337191 
0.583380 
0.006905 
0.791305 


final_layer_weights[0] = 0.387141
final_layer_weights[1] = 0.690194
final_layer_weights[2] = -0.134253
final_layer_weights[3] = -0.082823
final_layer_weights[4] = 0.799656
final_layer_weights[5] = 0.631736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.833969
final_layer_weights[8] = 0.671983
final_layer_weights[9] = 1.158050
.
.
.
final_layer_weights[1022] = 0.287664
final_layer_weights[1023] = -0.243709


Output Embedding: -0.000000, -0.000001 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.833911 


Updated weights for final layer:
final_layer_weights[0] = 0.397141
final_layer_weights[1] = 0.700194
final_layer_weights[2] = -0.144253
final_layer_weights[3] = -0.072823
final_layer_weights[4] = 0.809656
final_layer_weights[5] = 0.621736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.681983
final_layer_weights[9] = 1.168050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.253709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.696191
semi_final_layer_weights[1] = -0.170582
semi_final_layer_weights[2] = 0.228389
semi_final_layer_weights[3] = -0.623069
semi_final_layer_weights[4] = -0.197813
semi_final_layer_weights[5] = 0.198136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.376670
semi_final_layer_weights[9] = -0.247157
.
.
.
semi_final_layer_weights[510] = -0.003095
semi_final_layer_weights[511] = 0.781305
Updating K Matrix:
k_matrix[0][0] = -0.389606
k_matrix[0][1] = 0.354000
k_matrix[1][0] = -0.386248
k_matrix[1][1] = 0.305808

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.298000
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.375737
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.344986

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 1  total loss: 29740.462449 ******************************************************************* 

Epoch: 2
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.389606 0.354000 
-0.386248 0.305808 

Q Matrix:
0.457000 -0.298000 
0.473669 -0.474492 

V Matrix:
-0.375737 0.447537 
-0.451000 0.344986 
Final K Matrix:
[-9.334897, 8.694249]
[-5.758029, 5.752828]
[-14.364942, 11.676111]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -6.297289]
[6.516209, -2.337486]
[17.477961, -16.445585]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-8.633151, 11.187438]
[-4.647024, 7.753332]
[-16.246497, 13.492444]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-4.647024 7.753332  
-4.647024 7.753332  
-4.647024 7.753332  
-6.640088 9.470385  
-6.640088 9.470385  
-6.640088 9.470385  
-6.640088 9.470385  
-6.640088 9.470385  
-6.640088 9.470385  
-6.640088 9.470385  


Context Matrix (embedding_matrix + self_attention_matrix:
23.978788 3.046818 
21.574539 -3.788572 
2.003615 38.235823 
-6.640088 9.470385 
-6.640088 9.470385 
-6.640088 9.470385 
-6.640088 9.470385 
-6.640088 9.470385 
-6.640088 9.470385 
-6.640088 9.470385 




semi_final_layer_weights[0] = -0.696191
semi_final_layer_weights[1] = -0.170582
semi_final_layer_weights[2] = 0.228389
semi_final_layer_weights[3] = -0.623069
semi_final_layer_weights[4] = -0.197813
semi_final_layer_weights[5] = 0.198136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.376670
semi_final_layer_weights[9] = -0.247157
.
.
.
semi_final_layer_weights[511] = -0.003095
semi_final_layer_weights[512] = 0.781305


 Semi Final Layer Node Values: 
-0.195112 
-0.028634 
9.418634 
-0.023865 
-0.007577 
0.758920 
0.018970 
-0.000173 
-0.014428 
-0.009467 
.
.
.
-0.027792 
1.357128 
0.019025 
0.013785 
-0.007994 
-0.012059 
1.610506 
0.737512 
1.253239 
2.196216 
-0.000119 
2.992630 


final_layer_weights[0] = 0.397141
final_layer_weights[1] = 0.700194
final_layer_weights[2] = -0.144253
final_layer_weights[3] = -0.072823
final_layer_weights[4] = 0.809656
final_layer_weights[5] = 0.621736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.681983
final_layer_weights[9] = 1.168050
.
.
.
final_layer_weights[1022] = 0.277664
final_layer_weights[1023] = -0.253709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.452254 


Updated weights for final layer:
final_layer_weights[0] = 0.407141
final_layer_weights[1] = 0.710194
final_layer_weights[2] = -0.154253
final_layer_weights[3] = -0.062823
final_layer_weights[4] = 0.819656
final_layer_weights[5] = 0.611736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.691983
final_layer_weights[9] = 1.178050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.263709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.686191
semi_final_layer_weights[1] = -0.160582
semi_final_layer_weights[2] = 0.218389
semi_final_layer_weights[3] = -0.613069
semi_final_layer_weights[4] = -0.187813
semi_final_layer_weights[5] = 0.188136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.366670
semi_final_layer_weights[9] = -0.237157
.
.
.
semi_final_layer_weights[510] = 0.004069
semi_final_layer_weights[511] = 0.771305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.465341
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.401991

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.391728
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.453492

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.465341 
0.507732 -0.401991 

Q Matrix:
-0.543000 0.391728 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.453492 
Final K Matrix:
[12.198711, -10.933697]
[7.297141, -5.843928]
[18.883040, -15.348511]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 9.677631]
[-7.577876, 7.398913]
[-19.655169, 18.624036]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -12.920580]
[7.823742, -6.623174]
[20.019740, -17.497795]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -6.623174  
7.823742 -6.623174  
7.823742 -6.623174  
9.869209 -9.771877  
9.869209 -9.771877  
9.869209 -9.771877  
9.869209 -9.771877  
9.869209 -9.771877  
9.869209 -9.771877  
9.869209 -9.771877  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.092250 
8.934729 6.628208 
14.474382 23.859316 
9.869209 -9.771877 
9.869209 -9.771877 
9.869209 -9.771877 
9.869209 -9.771877 
9.869209 -9.771877 
9.869209 -9.771877 
9.869209 -9.771877 




semi_final_layer_weights[0] = -0.686191
semi_final_layer_weights[1] = -0.160582
semi_final_layer_weights[2] = 0.218389
semi_final_layer_weights[3] = -0.613069
semi_final_layer_weights[4] = -0.187813
semi_final_layer_weights[5] = 0.188136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.366670
semi_final_layer_weights[9] = -0.237157
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.771305


 Semi Final Layer Node Values: 
-0.164969 
-0.026597 
8.590047 
0.553398 
0.169533 
-0.001698 
0.004556 
-0.000049 
0.330981 
0.214074 
.
.
.
0.645933 
-0.003108 
0.004543 
0.004270 
0.179370 
0.275150 
-0.003705 
-0.001648 
-0.002863 
-0.005085 
-0.000037 
-0.006962 


final_layer_weights[0] = 0.407141
final_layer_weights[1] = 0.710194
final_layer_weights[2] = -0.154253
final_layer_weights[3] = -0.062823
final_layer_weights[4] = 0.819656
final_layer_weights[5] = 0.611736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.691983
final_layer_weights[9] = 1.178050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.263709


Output Embedding: 16.888606, 21.853730 

 Expected Embedding: 12.470385, -17.532656 
 loss: 785.404031 


Updated weights for final layer:
final_layer_weights[0] = 0.417141
final_layer_weights[1] = 0.720194
final_layer_weights[2] = -0.164253
final_layer_weights[3] = -0.052823
final_layer_weights[4] = 0.829656
final_layer_weights[5] = 0.601736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.701983
final_layer_weights[9] = 1.188050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.273709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.676191
semi_final_layer_weights[1] = -0.150582
semi_final_layer_weights[2] = 0.208389
semi_final_layer_weights[3] = -0.603069
semi_final_layer_weights[4] = -0.177813
semi_final_layer_weights[5] = 0.178136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.356670
semi_final_layer_weights[9] = -0.227157
.
.
.
semi_final_layer_weights[510] = -0.005931
semi_final_layer_weights[511] = 0.761305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.534659
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.598009

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.608272
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.546508

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.534659 
-0.492268 0.598009 

Q Matrix:
0.457000 -0.608272 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.546508 
Final K Matrix:
[3.344240, -3.547456]
[-19.512352, 21.081374]
[6.341179, -8.076410]
[-5.615109, 5.632777]
[-23.779996, 26.030430]
[-22.029325, 25.597848]
[11.880782, -13.604507]
[21.156480, -23.554006]
[-25.673537, 29.350792]
[16.748897, -20.268348]
Final Q Matrix:
[-3.107533, 4.449704]
[18.213381, -25.049828]
[-6.181535, 5.224031]
[5.148358, -8.241369]
[22.269381, -29.723457]
[20.947785, -24.003315]
[-11.254464, 13.423461]
[-19.897222, 25.503101]
[24.309912, -29.120446]
[-16.099335, 16.330420]
Final V Matrix:
[3.589148, -2.877044]
[-20.550429, 17.408304]
[5.429133, -7.645171]
[-6.356215, 4.305811]
[-24.700227, 21.764473]
[-21.368887, 22.569206]
[11.729383, -11.846140]
[21.572046, -20.004704]
[-25.394902, 25.521470]
[15.424621, -18.467385]
 

Self-Attention Matrix: 
-20.550429 17.408304  
3.589148 -2.877044  
-20.550429 17.408304  
3.589148 -2.877044  
3.589148 -2.877044  
3.589148 -2.877044  
-20.550429 17.408304  
-20.550429 17.408304  
3.589148 -2.877044  
-20.550429 17.408304  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.418603 
48.810301 -8.055102 
-14.116026 -1.849979 
24.084221 -11.781762 
52.867915 -3.406982 
23.652792 21.989884 
-34.834522 7.429541 
-57.574446 11.122808 
35.276801 17.872994 
-21.900288 -15.277899 




semi_final_layer_weights[0] = -0.676191
semi_final_layer_weights[1] = -0.150582
semi_final_layer_weights[2] = 0.208389
semi_final_layer_weights[3] = -0.603069
semi_final_layer_weights[4] = -0.177813
semi_final_layer_weights[5] = 0.178136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.356670
semi_final_layer_weights[9] = -0.227157
.
.
.
semi_final_layer_weights[511] = -0.005931
semi_final_layer_weights[512] = 0.761305


 Semi Final Layer Node Values: 
6.096068 
-0.059864 
-0.035355 
-0.068162 
-0.086170 
8.308740 
-0.001308 
0.205679 
-0.193136 
8.672442 
.
.
.
0.151707 
-0.000719 
-0.000011 
-0.000011 
0.040575 
0.063389 
-0.000861 
-0.000371 
-0.000660 
-0.001190 
0.001275 
-0.001637 


final_layer_weights[0] = 0.417141
final_layer_weights[1] = 0.720194
final_layer_weights[2] = -0.164253
final_layer_weights[3] = -0.052823
final_layer_weights[4] = 0.829656
final_layer_weights[5] = 0.601736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.701983
final_layer_weights[9] = 1.188050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.273709


Output Embedding: -0.000003, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 518.871472 


Updated weights for final layer:
final_layer_weights[0] = 0.427141
final_layer_weights[1] = 0.730194
final_layer_weights[2] = -0.174253
final_layer_weights[3] = -0.042823
final_layer_weights[4] = 0.839656
final_layer_weights[5] = 0.591736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.711983
final_layer_weights[9] = 1.198050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.283709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.666191
semi_final_layer_weights[1] = -0.140582
semi_final_layer_weights[2] = 0.198389
semi_final_layer_weights[3] = -0.593069
semi_final_layer_weights[4] = -0.167813
semi_final_layer_weights[5] = 0.168136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.346670
semi_final_layer_weights[9] = -0.217157
.
.
.
semi_final_layer_weights[510] = 0.004069
semi_final_layer_weights[511] = 0.751305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.465341
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.401991

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.391728
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.453492

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.465341 
0.507732 -0.401991 

Q Matrix:
-0.543000 0.391728 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.453492 
Final K Matrix:
[18.685488, -15.141735]
[2.079066, -1.487146]
[13.504794, -11.375406]
[33.589743, -28.385212]
[19.091982, -15.410504]
[17.153191, -14.380894]
[-4.906579, 4.494589]
[12.144389, -10.226957]
[-8.137466, 7.732425]
[9.676308, -8.226891]
Final Q Matrix:
[-19.440204, 18.535983]
[-2.123109, 2.519201]
[-14.137515, 12.398619]
[-35.182021, 30.626274]
[-19.850865, 19.079370]
[-17.943177, 15.904523]
[5.209537, -3.668747]
[-12.712858, 11.155532]
[8.696137, -5.441442]
[-10.145077, 8.707345]
Final V Matrix:
[19.856499, -17.241383]
[2.406982, -1.604638]
[13.919294, -13.146671]
[34.528924, -32.844718]
[20.349102, -17.520198]
[17.747314, -16.590927]
[-4.695507, 5.350654]
[12.519681, -11.818297]
[-7.509167, 9.315693]
[9.896990, -9.540860]
 

Self-Attention Matrix: 
2.406982 -1.604638  
2.406982 -1.604638  
2.406982 -1.604638  
2.406982 -1.604638  
2.406982 -1.604638  
2.406982 -1.604638  
19.856499 -17.241383  
2.406982 -1.604638  
19.856499 -17.241383  
2.406982 -1.604638  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 29.337550 
-0.248253 5.168483 
13.820081 13.481319 
32.327545 34.371135 
7.329538 31.032478 
15.773302 18.696790 
9.667620 -16.627655 
12.627826 12.004560 
-1.690004 -11.534678 
11.859459 7.918617 




semi_final_layer_weights[0] = -0.666191
semi_final_layer_weights[1] = -0.140582
semi_final_layer_weights[2] = 0.198389
semi_final_layer_weights[3] = -0.593069
semi_final_layer_weights[4] = -0.167813
semi_final_layer_weights[5] = 0.168136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.346670
semi_final_layer_weights[9] = -0.217157
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.751305


 Semi Final Layer Node Values: 
-0.256841 
-0.008323 
5.614686 
-0.401500 
-0.066055 
5.963799 
0.040178 
0.140331 
4.931270 
-0.045121 
.
.
.
-0.004930 
0.229851 
-0.000036 
-0.000034 
-0.001267 
-0.002019 
0.276734 
0.115202 
0.210628 
0.385110 
0.002884 
0.532472 


final_layer_weights[0] = 0.427141
final_layer_weights[1] = 0.730194
final_layer_weights[2] = -0.174253
final_layer_weights[3] = -0.042823
final_layer_weights[4] = 0.839656
final_layer_weights[5] = 0.591736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.711983
final_layer_weights[9] = 1.198050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.283709


Output Embedding: -0.000000, 5.873930 

 Expected Embedding: 4.643339, -27.140186 
 loss: 555.746239 


Updated weights for final layer:
final_layer_weights[0] = 0.437141
final_layer_weights[1] = 0.740194
final_layer_weights[2] = -0.184253
final_layer_weights[3] = -0.032823
final_layer_weights[4] = 0.849656
final_layer_weights[5] = 0.581736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.721983
final_layer_weights[9] = 1.208050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.293709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.656191
semi_final_layer_weights[1] = -0.130582
semi_final_layer_weights[2] = 0.188389
semi_final_layer_weights[3] = -0.583069
semi_final_layer_weights[4] = -0.157813
semi_final_layer_weights[5] = 0.158136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.336670
semi_final_layer_weights[9] = -0.207157
.
.
.
semi_final_layer_weights[510] = -0.005931
semi_final_layer_weights[511] = 0.741305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.534659
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.598009

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.608272
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.546508

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.534659 
-0.492268 0.598009 

Q Matrix:
0.457000 -0.608272 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.546508 
Final K Matrix:
[-9.875602, 11.435206]
[10.796392, -12.306803]
[18.553405, -22.592663]
[-17.685505, 21.706859]
[-18.434884, 19.955015]
[11.056930, -13.398654]
[-11.899277, 13.834071]
[-31.153021, 36.187946]
[-22.272413, 27.106074]
[4.830737, -5.212691]
Final Q Matrix:
[9.382155, -10.856080]
[-10.215246, 12.331530]
[-17.863986, 17.755099]
[17.064978, -16.517390]
[17.215728, -23.576724]
[-10.632042, 10.737041]
[11.316626, -12.948319]
[29.621076, -33.972038]
[21.441530, -21.350414]
[-4.507755, 6.217125]
Final V Matrix:
[-9.620462, 10.052508]
[10.715896, -10.674091]
[16.943063, -20.685170]
[-15.976085, 19.995026]
[-19.377147, 16.508260]
[10.164009, -12.221137]
[-11.535161, 12.202633]
[-30.230826, 31.897775]
[-20.354840, 24.806729]
[5.094358, -4.299297]
 

Self-Attention Matrix: 
10.715896 -10.674091  
-9.620462 10.052508  
-9.620462 10.052508  
10.715896 -10.674091  
10.715896 -10.674091  
-9.620462 10.052508  
10.715896 -10.674091  
10.715896 -10.674091  
10.715896 -10.674091  
-9.620462 10.052508  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 -0.212434 
-23.566251 1.941319 
-8.690681 -28.558603 
6.879960 29.054055 
52.788963 -14.921069 
-10.195518 -11.838806 
21.428643 2.881571 
39.288181 24.294491 
9.862917 35.415753 
-20.927992 11.445433 




semi_final_layer_weights[0] = -0.656191
semi_final_layer_weights[1] = -0.130582
semi_final_layer_weights[2] = 0.188389
semi_final_layer_weights[3] = -0.583069
semi_final_layer_weights[4] = -0.157813
semi_final_layer_weights[5] = 0.158136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.336670
semi_final_layer_weights[9] = -0.207157
.
.
.
semi_final_layer_weights[511] = -0.005931
semi_final_layer_weights[512] = 0.741305


 Semi Final Layer Node Values: 
-0.125924 
2.693245 
-0.072057 
-0.215351 
-0.058182 
-0.036426 
0.125350 
-0.002923 
-0.155806 
1.757221 
.
.
.
0.523150 
-0.002398 
-0.000038 
-0.000040 
0.128739 
0.209707 
-0.002903 
-0.001164 
-0.002191 
-0.004070 
0.004526 
-0.005657 


final_layer_weights[0] = 0.437141
final_layer_weights[1] = 0.740194
final_layer_weights[2] = -0.184253
final_layer_weights[3] = -0.032823
final_layer_weights[4] = 0.849656
final_layer_weights[5] = 0.581736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.721983
final_layer_weights[9] = 1.208050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.293709


Output Embedding: 20.653293, 8.576862 

 Expected Embedding: 44.423141, -6.847703 
 loss: 401.461465 


Updated weights for final layer:
final_layer_weights[0] = 0.447141
final_layer_weights[1] = 0.750194
final_layer_weights[2] = -0.194253
final_layer_weights[3] = -0.022823
final_layer_weights[4] = 0.859656
final_layer_weights[5] = 0.571736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.731983
final_layer_weights[9] = 1.218050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.303709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.646191
semi_final_layer_weights[1] = -0.120582
semi_final_layer_weights[2] = 0.178389
semi_final_layer_weights[3] = -0.573069
semi_final_layer_weights[4] = -0.147813
semi_final_layer_weights[5] = 0.148136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.326670
semi_final_layer_weights[9] = -0.197157
.
.
.
semi_final_layer_weights[510] = 0.004069
semi_final_layer_weights[511] = 0.731305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.465341
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.401991

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.391728
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.453492

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.465341 
0.507732 -0.401991 

Q Matrix:
-0.543000 0.391728 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.453492 
Final K Matrix:
[-24.403422, 20.156121]
[-0.481132, 0.824102]
[-16.791312, 15.005869]
[-3.929424, 3.669679]
[-6.115148, 5.502849]
[-16.080732, 13.588750]
[-14.127494, 11.315116]
[-17.897954, 16.048457]
[4.390170, -3.645689]
[-15.121837, 11.428265]
Final Q Matrix:
[25.466048, -23.327832]
[0.588303, 0.526354]
[17.752217, -13.423190]
[4.186231, -2.775859]
[6.472760, -4.800850]
[16.842943, -14.662854]
[14.671225, -14.322012]
[18.933022, -14.183926]
[-4.585298, 4.151352]
[15.565780, -16.909282]
Final V Matrix:
[-25.551895, 23.122096]
[-0.077059, 1.133328]
[-16.444517, 17.714821]
[-3.690189, 4.396507]
[-5.950916, 6.511700]
[-16.530723, 15.723474]
[-15.145922, 12.824401]
[-17.474686, 18.967452]
[4.577173, -4.190789]
[-16.895205, 12.642276]
 

Self-Attention Matrix: 
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-0.077059 1.133328  
-25.551895 23.122096  
-0.077059 1.133328  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -32.858955 
-7.481065 7.654080 
-28.671517 -3.094862 
-9.409564 2.807786 
-11.124483 0.232737 
-14.395056 -16.095897 
-2.246148 -24.503421 
-31.451825 -2.469967 
-22.714709 28.906874 
9.016138 -37.821998 




semi_final_layer_weights[0] = -0.646191
semi_final_layer_weights[1] = -0.120582
semi_final_layer_weights[2] = 0.178389
semi_final_layer_weights[3] = -0.573069
semi_final_layer_weights[4] = -0.147813
semi_final_layer_weights[5] = 0.148136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.326670
semi_final_layer_weights[9] = -0.197157
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.731305


 Semi Final Layer Node Values: 
30.943528 
-0.001414 
-0.058452 
3.210205 
1.462129 
-0.046649 
0.140064 
-0.001912 
-0.023495 
5.876434 
.
.
.
-0.002116 
0.095322 
-0.000016 
-0.000015 
-0.000497 
-0.000830 
0.116043 
0.044651 
0.086826 
0.163941 
0.001274 
0.229070 


final_layer_weights[0] = 0.447141
final_layer_weights[1] = 0.750194
final_layer_weights[2] = -0.194253
final_layer_weights[3] = -0.022823
final_layer_weights[4] = 0.859656
final_layer_weights[5] = 0.571736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.731983
final_layer_weights[9] = 1.218050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.303709


Output Embedding: 15.542229, -0.029070 

 Expected Embedding: 28.923067, 35.905037 
 loss: 735.153450 


Updated weights for final layer:
final_layer_weights[0] = 0.457141
final_layer_weights[1] = 0.760194
final_layer_weights[2] = -0.204253
final_layer_weights[3] = -0.012823
final_layer_weights[4] = 0.869656
final_layer_weights[5] = 0.561736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.741983
final_layer_weights[9] = 1.228050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.313709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.636191
semi_final_layer_weights[1] = -0.110582
semi_final_layer_weights[2] = 0.168389
semi_final_layer_weights[3] = -0.563069
semi_final_layer_weights[4] = -0.137813
semi_final_layer_weights[5] = 0.138136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.316670
semi_final_layer_weights[9] = -0.187157
.
.
.
semi_final_layer_weights[510] = -0.005931
semi_final_layer_weights[511] = 0.721305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.534659
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.598009

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.608272
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.546508

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.534659 
-0.492268 0.598009 

Q Matrix:
0.457000 -0.608272 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.546508 
Final K Matrix:
[-13.934147, 16.895562]
[-3.654047, 4.830299]
[5.395127, -5.909465]
[18.708440, -22.790049]
[-25.469705, 29.601850]
[-20.154855, 22.639503]
[-16.029894, 19.215701]
[5.673406, -5.364333]
[4.941053, -5.359504]
[27.168539, -30.601282]
Final Q Matrix:
[13.400909, -13.506424]
[3.599831, -2.590542]
[-5.053211, 6.734594]
[-18.015102, 17.882999]
[24.220611, -27.736945]
[18.998196, -23.818119]
[15.369107, -16.063939]
[-5.131772, 9.105137]
[-4.616646, 6.292990]
[-25.627254, 31.907907]
Final V Matrix:
[-12.798337, 15.418062]
[-2.948683, 4.691528]
[5.600066, -4.943962]
[17.075876, -20.871965]
[-24.699686, 26.104146]
[-20.346176, 19.383077]
[-14.948612, 17.378455]
[6.755556, -3.820164]
[5.182375, -4.442533]
[27.341334, -26.263587]
 

Self-Attention Matrix: 
-2.948683 4.691528  
-2.948683 4.691528  
-12.798337 15.418062  
-12.798337 15.418062  
-2.948683 4.691528  
-2.948683 4.691528  
-2.948683 4.691528  
-12.798337 15.418062  
-12.798337 15.418062  
-12.798337 15.418062  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 32.455934 
-9.697419 18.802647 
-23.913586 15.473920 
-11.712531 -23.662615 
20.139443 33.549930 
28.862673 14.108239 
1.491417 32.854601 
-39.143956 30.002449 
-23.885138 16.368140 
-54.240410 1.298028 




semi_final_layer_weights[0] = -0.636191
semi_final_layer_weights[1] = -0.110582
semi_final_layer_weights[2] = 0.168389
semi_final_layer_weights[3] = -0.563069
semi_final_layer_weights[4] = -0.137813
semi_final_layer_weights[5] = 0.138136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.316670
semi_final_layer_weights[9] = -0.187157
.
.
.
semi_final_layer_weights[511] = -0.005931
semi_final_layer_weights[512] = 0.721305


 Semi Final Layer Node Values: 
-0.197561 
-0.011175 
-0.012528 
20.481717 
-0.075369 
6.073966 
0.175053 
0.036842 
2.063738 
9.721380 
.
.
.
-0.021174 
0.936297 
0.015801 
0.016762 
-0.004731 
-0.008107 
1.146742 
0.421670 
0.850011 
1.633208 
-0.000189 
2.294677 


final_layer_weights[0] = 0.457141
final_layer_weights[1] = 0.760194
final_layer_weights[2] = -0.204253
final_layer_weights[3] = -0.012823
final_layer_weights[4] = 0.869656
final_layer_weights[5] = 0.561736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.741983
final_layer_weights[9] = 1.228050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.313709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013082 


Updated weights for final layer:
final_layer_weights[0] = 0.467141
final_layer_weights[1] = 0.770194
final_layer_weights[2] = -0.214253
final_layer_weights[3] = -0.002823
final_layer_weights[4] = 0.879656
final_layer_weights[5] = 0.551736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.751983
final_layer_weights[9] = 1.238050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.323709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.626191
semi_final_layer_weights[1] = -0.100582
semi_final_layer_weights[2] = 0.158389
semi_final_layer_weights[3] = -0.553069
semi_final_layer_weights[4] = -0.127813
semi_final_layer_weights[5] = 0.128136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.306670
semi_final_layer_weights[9] = -0.177157
.
.
.
semi_final_layer_weights[510] = 0.004069
semi_final_layer_weights[511] = 0.711305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.465341
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.401991

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.391728
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.453492

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.465341 
0.507732 -0.401991 

Q Matrix:
-0.543000 0.391728 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.453492 
Final K Matrix:
[23.725861, -19.927983]
[21.630015, -18.465641]
[19.842088, -18.223996]
[-19.286431, 15.204823]
[0.000000, 0.000000]
[9.664691, -8.267531]
[-6.799926, 6.090994]
[-20.488329, 16.237114]
[6.994107, -5.215779]
[-24.295332, 20.134112]
Final Q Matrix:
[-24.825964, 21.913920]
[-22.693157, 19.289315]
[-21.076939, 14.725418]
[19.979770, -20.111872]
[0.000000, 0.000000]
[-10.143105, 8.580130]
[7.191913, -5.403327]
[21.242000, -21.169317]
[-7.185297, 7.982601]
[25.366843, -23.069022]
Final V Matrix:
[24.510942, -23.006409]
[22.047727, -21.447255]
[18.940532, -21.714099]
[-20.918996, 17.122906]
[0.000000, 0.000000]
[9.834592, -9.609609]
[-6.645372, 7.196323]
[-22.137879, 18.324533]
[7.884310, -5.736151]
[-25.371447, 23.126480]
 

Self-Attention Matrix: 
22.047727 -21.447255  
22.047727 -21.447255  
22.047727 -21.447255  
24.510942 -23.006409  
23.279334 -22.226832  
22.047727 -21.447255  
24.510942 -23.006409  
24.510942 -23.006409  
24.506863 -23.003827  
24.510942 -23.006409  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 6.014793 
44.440308 -1.433253 
64.052968 -24.737784 
25.596747 -62.087086 
23.279334 -22.226832 
32.332829 -12.786742 
12.695341 -24.480846 
24.248469 -63.094276 
19.131810 -3.806867 
9.498845 -55.714500 




semi_final_layer_weights[0] = -0.626191
semi_final_layer_weights[1] = -0.100582
semi_final_layer_weights[2] = 0.158389
semi_final_layer_weights[3] = -0.553069
semi_final_layer_weights[4] = -0.127813
semi_final_layer_weights[5] = 0.128136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.306670
semi_final_layer_weights[9] = -0.177157
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.711305


 Semi Final Layer Node Values: 
-0.301596 
-0.042252 
6.068704 
20.734744 
-0.000067 
2.376421 
0.064534 
-0.002181 
-0.043930 
8.364584 
.
.
.
-0.000344 
0.014927 
-0.000003 
-0.000002 
-0.000073 
-0.000129 
0.018400 
0.006434 
0.013503 
0.026429 
0.000214 
0.037345 


final_layer_weights[0] = 0.467141
final_layer_weights[1] = 0.770194
final_layer_weights[2] = -0.214253
final_layer_weights[3] = -0.002823
final_layer_weights[4] = 0.879656
final_layer_weights[5] = 0.551736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.751983
final_layer_weights[9] = 1.238050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.323709


Output Embedding: 11.223316, 23.552971 

 Expected Embedding: -1.493413, -5.218942 
 loss: 494.769105 


Updated weights for final layer:
final_layer_weights[0] = 0.477141
final_layer_weights[1] = 0.780194
final_layer_weights[2] = -0.224253
final_layer_weights[3] = 0.007177
final_layer_weights[4] = 0.889656
final_layer_weights[5] = 0.541736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.761983
final_layer_weights[9] = 1.248050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.333709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.616191
semi_final_layer_weights[1] = -0.090582
semi_final_layer_weights[2] = 0.148389
semi_final_layer_weights[3] = -0.543069
semi_final_layer_weights[4] = -0.117813
semi_final_layer_weights[5] = 0.118136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.296670
semi_final_layer_weights[9] = -0.167157
.
.
.
semi_final_layer_weights[510] = -0.005931
semi_final_layer_weights[511] = 0.701305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.534659
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.598009

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.608272
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.546508

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.534659 
-0.492268 0.598009 

Q Matrix:
0.457000 -0.608272 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.546508 
Final K Matrix:
[-3.480419, 4.647176]
[-17.999174, 22.345934]
[7.938871, -7.708728]
[18.708440, -22.790049]
[15.323486, -18.687031]
[1.660403, -1.692745]
[15.343500, -17.617093]
[0.000000, 0.000000]
[-10.478252, 13.548314]
[-8.802674, 10.188494]
Final Q Matrix:
[3.438718, -2.357007]
[17.422074, -16.205526]
[-7.224304, 12.259276]
[-18.015102, 17.882999]
[-14.759971, 14.598762]
[-1.528193, 2.372447]
[-14.544810, 17.222781]
[0.000000, 0.000000]
[10.257887, -8.149656]
[8.361907, -9.686963]
Final V Matrix:
[-2.761263, 4.543870]
[-16.000360, 20.761842]
[9.246815, -5.673859]
[17.075876, -20.871965]
[13.965473, -17.128702]
[1.851900, -1.317230]
[15.099562, -15.375784]
[0.000000, 0.000000]
[-8.764448, 12.961885]
[-8.579680, 8.953317]
 

Self-Attention Matrix: 
-2.761263 4.543870  
-2.761263 4.543870  
-16.000360 20.761842  
-16.000360 20.761842  
-16.000360 20.761842  
-16.000360 20.761842  
-16.000360 20.761842  
-9.380811 12.652856  
-2.761263 4.543870  
-2.761263 4.543870  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.777406 
-11.046865 49.318982 
-49.376702 37.711822 
-14.914554 -18.318835 
-14.758700 -11.597045 
-21.593184 22.931559 
-33.628850 7.063276 
-9.380811 12.652856 
-16.889828 39.831448 
5.947837 13.794731 




semi_final_layer_weights[0] = -0.616191
semi_final_layer_weights[1] = -0.090582
semi_final_layer_weights[2] = 0.148389
semi_final_layer_weights[3] = -0.543069
semi_final_layer_weights[4] = -0.117813
semi_final_layer_weights[5] = 0.118136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.296670
semi_final_layer_weights[9] = -0.167157
.
.
.
semi_final_layer_weights[511] = -0.005931
semi_final_layer_weights[512] = 0.701305


 Semi Final Layer Node Values: 
-0.060313 
-0.035573 
-0.015826 
18.591092 
3.222862 
0.276246 
-0.001266 
-0.000193 
-0.071028 
-0.034673 
.
.
.
-0.027580 
1.171882 
0.021219 
0.022510 
-0.005499 
-0.010032 
1.454482 
0.480806 
1.056011 
2.107741 
-0.000253 
2.996006 


final_layer_weights[0] = 0.477141
final_layer_weights[1] = 0.780194
final_layer_weights[2] = -0.224253
final_layer_weights[3] = 0.007177
final_layer_weights[4] = 0.889656
final_layer_weights[5] = 0.541736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.761983
final_layer_weights[9] = 1.248050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.333709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.433293 


Updated weights for final layer:
final_layer_weights[0] = 0.487141
final_layer_weights[1] = 0.790194
final_layer_weights[2] = -0.234253
final_layer_weights[3] = 0.017177
final_layer_weights[4] = 0.899656
final_layer_weights[5] = 0.531736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.771983
final_layer_weights[9] = 1.258050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.343709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.606191
semi_final_layer_weights[1] = -0.080582
semi_final_layer_weights[2] = 0.138389
semi_final_layer_weights[3] = -0.533069
semi_final_layer_weights[4] = -0.107813
semi_final_layer_weights[5] = 0.108136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.286670
semi_final_layer_weights[9] = -0.157157
.
.
.
semi_final_layer_weights[510] = 0.004069
semi_final_layer_weights[511] = 0.691305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.465341
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.401991

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.391728
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.453492

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.465341 
0.507732 -0.401991 

Q Matrix:
-0.543000 0.391728 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.453492 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.606191
semi_final_layer_weights[1] = -0.080582
semi_final_layer_weights[2] = 0.138389
semi_final_layer_weights[3] = -0.533069
semi_final_layer_weights[4] = -0.107813
semi_final_layer_weights[5] = 0.108136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.286670
semi_final_layer_weights[9] = -0.157157
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.691305


 Semi Final Layer Node Values: 
-0.006062 
-0.000806 
0.138389 
-0.005331 
-0.001078 
0.108136 
-0.000050 
0.005475 
-0.002867 
-0.001572 
.
.
.
-0.006356 
0.264314 
-0.000050 
-0.000047 
-0.001187 
-0.002248 
0.330465 
0.102547 
0.237191 
0.483380 
0.004069 
0.691305 


final_layer_weights[0] = 0.487141
final_layer_weights[1] = 0.790194
final_layer_weights[2] = -0.234253
final_layer_weights[3] = 0.017177
final_layer_weights[4] = 0.899656
final_layer_weights[5] = 0.531736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.771983
final_layer_weights[9] = 1.258050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.343709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.833923 


Updated weights for final layer:
final_layer_weights[0] = 0.497141
final_layer_weights[1] = 0.800194
final_layer_weights[2] = -0.244253
final_layer_weights[3] = 0.027177
final_layer_weights[4] = 0.909656
final_layer_weights[5] = 0.521736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.781983
final_layer_weights[9] = 1.268050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.353709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.596191
semi_final_layer_weights[1] = -0.070582
semi_final_layer_weights[2] = 0.128389
semi_final_layer_weights[3] = -0.523069
semi_final_layer_weights[4] = -0.097813
semi_final_layer_weights[5] = 0.098136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.276670
semi_final_layer_weights[9] = -0.147157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.681305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 2  total loss: 5351.138314 ******************************************************************* 

Epoch: 3
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.596191
semi_final_layer_weights[1] = -0.070582
semi_final_layer_weights[2] = 0.128389
semi_final_layer_weights[3] = -0.523069
semi_final_layer_weights[4] = -0.097813
semi_final_layer_weights[5] = 0.098136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.276670
semi_final_layer_weights[9] = -0.147157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.681305


 Semi Final Layer Node Values: 
-0.136486 
-0.008225 
4.635714 
0.497593 
0.093049 
-0.000934 
-0.000047 
0.004305 
0.263195 
0.139990 
.
.
.
0.595113 
-0.002419 
-0.000047 
-0.000049 
0.103416 
0.204356 
-0.003049 
-0.000880 
-0.002161 
-0.004503 
0.004212 
-0.006481 


final_layer_weights[0] = 0.497141
final_layer_weights[1] = 0.800194
final_layer_weights[2] = -0.244253
final_layer_weights[3] = 0.027177
final_layer_weights[4] = 0.909656
final_layer_weights[5] = 0.521736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.781983
final_layer_weights[9] = 1.268050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.353709


Output Embedding: 20.206557, 22.924199 

 Expected Embedding: 12.470385, -17.532656 
 loss: 848.302735 


Updated weights for final layer:
final_layer_weights[0] = 0.507141
final_layer_weights[1] = 0.810194
final_layer_weights[2] = -0.254253
final_layer_weights[3] = 0.037177
final_layer_weights[4] = 0.919656
final_layer_weights[5] = 0.511736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.791983
final_layer_weights[9] = 1.278050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.363709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.586191
semi_final_layer_weights[1] = -0.060582
semi_final_layer_weights[2] = 0.118389
semi_final_layer_weights[3] = -0.513069
semi_final_layer_weights[4] = -0.087813
semi_final_layer_weights[5] = 0.088136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.266670
semi_final_layer_weights[9] = -0.137157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.671305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.586191
semi_final_layer_weights[1] = -0.060582
semi_final_layer_weights[2] = 0.118389
semi_final_layer_weights[3] = -0.513069
semi_final_layer_weights[4] = -0.087813
semi_final_layer_weights[5] = 0.088136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.266670
semi_final_layer_weights[9] = -0.137157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.671305


 Semi Final Layer Node Values: 
-0.136814 
-0.009609 
4.573601 
0.677530 
0.115961 
-0.001164 
0.006665 
-0.000072 
0.352149 
0.181122 
.
.
.
0.812903 
-0.003226 
0.006646 
0.006406 
0.130352 
0.270472 
-0.004100 
-0.001090 
-0.002868 
-0.006119 
-0.000074 
-0.008865 


final_layer_weights[0] = 0.507141
final_layer_weights[1] = 0.810194
final_layer_weights[2] = -0.254253
final_layer_weights[3] = 0.037177
final_layer_weights[4] = 0.919656
final_layer_weights[5] = 0.511736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.791983
final_layer_weights[9] = 1.278050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.363709


Output Embedding: 28.594340, 31.432129 

 Expected Embedding: 12.470385, -17.532656 
 loss: 1328.766015 


Updated weights for final layer:
final_layer_weights[0] = 0.517141
final_layer_weights[1] = 0.820194
final_layer_weights[2] = -0.264253
final_layer_weights[3] = 0.047177
final_layer_weights[4] = 0.929656
final_layer_weights[5] = 0.501736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.801983
final_layer_weights[9] = 1.288050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.373709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.576191
semi_final_layer_weights[1] = -0.050582
semi_final_layer_weights[2] = 0.108389
semi_final_layer_weights[3] = -0.503069
semi_final_layer_weights[4] = -0.077813
semi_final_layer_weights[5] = 0.078136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.256670
semi_final_layer_weights[9] = -0.127157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.661305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.576191
semi_final_layer_weights[1] = -0.050582
semi_final_layer_weights[2] = 0.108389
semi_final_layer_weights[3] = -0.503069
semi_final_layer_weights[4] = -0.077813
semi_final_layer_weights[5] = 0.078136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.256670
semi_final_layer_weights[9] = -0.127157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.661305


 Semi Final Layer Node Values: 
5.036544 
-0.020055 
-0.018092 
-0.056324 
-0.037626 
3.636154 
-0.001294 
0.204438 
-0.138713 
4.819757 
.
.
.
0.079414 
-0.000307 
-0.000007 
-0.000007 
0.011633 
0.025548 
-0.000394 
-0.000095 
-0.000272 
-0.000595 
0.000581 
-0.000867 


final_layer_weights[0] = 0.517141
final_layer_weights[1] = 0.820194
final_layer_weights[2] = -0.264253
final_layer_weights[3] = 0.047177
final_layer_weights[4] = 0.929656
final_layer_weights[5] = 0.501736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.801983
final_layer_weights[9] = 1.288050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.373709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 518.871412 


Updated weights for final layer:
final_layer_weights[0] = 0.527141
final_layer_weights[1] = 0.830194
final_layer_weights[2] = -0.274253
final_layer_weights[3] = 0.057177
final_layer_weights[4] = 0.939656
final_layer_weights[5] = 0.491736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.811983
final_layer_weights[9] = 1.298050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.383709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.566191
semi_final_layer_weights[1] = -0.040582
semi_final_layer_weights[2] = 0.098389
semi_final_layer_weights[3] = -0.493069
semi_final_layer_weights[4] = -0.067813
semi_final_layer_weights[5] = 0.068136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.246670
semi_final_layer_weights[9] = -0.117157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.651305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.566191
semi_final_layer_weights[1] = -0.040582
semi_final_layer_weights[2] = 0.098389
semi_final_layer_weights[3] = -0.493069
semi_final_layer_weights[4] = -0.067813
semi_final_layer_weights[5] = 0.068136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.246670
semi_final_layer_weights[9] = -0.117157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.651305


 Semi Final Layer Node Values: 
-0.216257 
-0.002257 
2.749257 
-0.332033 
-0.026449 
2.392352 
0.048448 
0.138368 
3.912981 
-0.023923 
.
.
.
0.172645 
-0.000650 
0.001459 
0.001406 
0.022816 
0.053574 
-0.000842 
-0.000181 
-0.000572 
-0.001285 
-0.000016 
-0.001888 


final_layer_weights[0] = 0.527141
final_layer_weights[1] = 0.830194
final_layer_weights[2] = -0.274253
final_layer_weights[3] = 0.057177
final_layer_weights[4] = 0.939656
final_layer_weights[5] = 0.491736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.811983
final_layer_weights[9] = 1.298050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.383709


Output Embedding: -0.035265, 16.839065 

 Expected Embedding: 4.643339, -27.140186 
 loss: 978.031953 


Updated weights for final layer:
final_layer_weights[0] = 0.537141
final_layer_weights[1] = 0.840194
final_layer_weights[2] = -0.284253
final_layer_weights[3] = 0.067177
final_layer_weights[4] = 0.949656
final_layer_weights[5] = 0.481736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.821983
final_layer_weights[9] = 1.308050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.393709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.556191
semi_final_layer_weights[1] = -0.030582
semi_final_layer_weights[2] = 0.088389
semi_final_layer_weights[3] = -0.483069
semi_final_layer_weights[4] = -0.057813
semi_final_layer_weights[5] = 0.058136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.236670
semi_final_layer_weights[9] = -0.107157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.641305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.556191
semi_final_layer_weights[1] = -0.030582
semi_final_layer_weights[2] = 0.088389
semi_final_layer_weights[3] = -0.483069
semi_final_layer_weights[4] = -0.057813
semi_final_layer_weights[5] = 0.058136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.236670
semi_final_layer_weights[9] = -0.107157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.641305


 Semi Final Layer Node Values: 
-0.120246 
0.647694 
-0.034298 
-0.180492 
-0.021563 
-0.013713 
0.127478 
-0.002942 
-0.110544 
0.968330 
.
.
.
0.483286 
-0.001769 
-0.000041 
-0.000042 
0.056708 
0.144280 
-0.002315 
-0.000434 
-0.001545 
-0.003577 
0.003654 
-0.005293 


final_layer_weights[0] = 0.537141
final_layer_weights[1] = 0.840194
final_layer_weights[2] = -0.284253
final_layer_weights[3] = 0.067177
final_layer_weights[4] = 0.949656
final_layer_weights[5] = 0.481736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.821983
final_layer_weights[9] = 1.308050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.393709


Output Embedding: 20.086152, 11.358357 

 Expected Embedding: 44.423141, -6.847703 
 loss: 461.874828 


Updated weights for final layer:
final_layer_weights[0] = 0.547141
final_layer_weights[1] = 0.850194
final_layer_weights[2] = -0.294253
final_layer_weights[3] = 0.077177
final_layer_weights[4] = 0.959656
final_layer_weights[5] = 0.471736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.831983
final_layer_weights[9] = 1.318050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.403709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.546191
semi_final_layer_weights[1] = -0.020582
semi_final_layer_weights[2] = 0.078389
semi_final_layer_weights[3] = -0.473069
semi_final_layer_weights[4] = -0.047813
semi_final_layer_weights[5] = 0.048136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.226670
semi_final_layer_weights[9] = -0.097157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.631305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.546191
semi_final_layer_weights[1] = -0.020582
semi_final_layer_weights[2] = 0.078389
semi_final_layer_weights[3] = -0.473069
semi_final_layer_weights[4] = -0.047813
semi_final_layer_weights[5] = 0.048136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.226670
semi_final_layer_weights[9] = -0.097157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.631305


 Semi Final Layer Node Values: 
26.343527 
-0.000170 
-0.025956 
2.813381 
0.585090 
-0.015325 
0.141807 
-0.001931 
-0.020383 
2.929397 
.
.
.
-0.005990 
0.212611 
-0.000052 
-0.000050 
-0.000611 
-0.001715 
0.281449 
0.044275 
0.184387 
0.440574 
0.005798 
0.656943 


final_layer_weights[0] = 0.547141
final_layer_weights[1] = 0.850194
final_layer_weights[2] = -0.294253
final_layer_weights[3] = 0.077177
final_layer_weights[4] = 0.959656
final_layer_weights[5] = 0.471736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.831983
final_layer_weights[9] = 1.318050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.403709


Output Embedding: -0.008064, -0.000000 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1063.091023 


Updated weights for final layer:
final_layer_weights[0] = 0.557141
final_layer_weights[1] = 0.860194
final_layer_weights[2] = -0.304253
final_layer_weights[3] = 0.087177
final_layer_weights[4] = 0.969656
final_layer_weights[5] = 0.461736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.841983
final_layer_weights[9] = 1.328050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.413709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.536191
semi_final_layer_weights[1] = -0.010582
semi_final_layer_weights[2] = 0.068389
semi_final_layer_weights[3] = -0.463069
semi_final_layer_weights[4] = -0.037813
semi_final_layer_weights[5] = 0.038136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.216670
semi_final_layer_weights[9] = -0.087157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.621305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.536191
semi_final_layer_weights[1] = -0.010582
semi_final_layer_weights[2] = 0.068389
semi_final_layer_weights[3] = -0.463069
semi_final_layer_weights[4] = -0.037813
semi_final_layer_weights[5] = 0.038136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.216670
semi_final_layer_weights[9] = -0.087157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.621305


 Semi Final Layer Node Values: 
-0.162501 
-0.000990 
-0.006093 
17.525035 
-0.020397 
1.648377 
0.171353 
0.043495 
1.730600 
4.829600 
.
.
.
-0.011722 
0.402721 
0.010294 
0.010671 
-0.001010 
-0.003209 
0.539821 
0.067455 
0.346508 
0.856742 
-0.000092 
1.287672 


final_layer_weights[0] = 0.557141
final_layer_weights[1] = 0.860194
final_layer_weights[2] = -0.304253
final_layer_weights[3] = 0.087177
final_layer_weights[4] = 0.969656
final_layer_weights[5] = 0.461736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.841983
final_layer_weights[9] = 1.328050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.413709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013082 


Updated weights for final layer:
final_layer_weights[0] = 0.567141
final_layer_weights[1] = 0.870194
final_layer_weights[2] = -0.314253
final_layer_weights[3] = 0.097177
final_layer_weights[4] = 0.979656
final_layer_weights[5] = 0.451736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.851983
final_layer_weights[9] = 1.338050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.423709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.526191
semi_final_layer_weights[1] = -0.000582
semi_final_layer_weights[2] = 0.058389
semi_final_layer_weights[3] = -0.453069
semi_final_layer_weights[4] = -0.027813
semi_final_layer_weights[5] = 0.028136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.206670
semi_final_layer_weights[9] = -0.077157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.611305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.526191
semi_final_layer_weights[1] = -0.000582
semi_final_layer_weights[2] = 0.058389
semi_final_layer_weights[3] = -0.453069
semi_final_layer_weights[4] = -0.027813
semi_final_layer_weights[5] = 0.028136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.206670
semi_final_layer_weights[9] = -0.077157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.611305


 Semi Final Layer Node Values: 
-0.247855 
-0.000238 
2.175302 
17.644586 
0.033502 
0.491993 
0.071874 
-0.002261 
-0.025550 
3.755224 
.
.
.
0.669222 
-0.002220 
0.006062 
0.005843 
0.046629 
0.174441 
-0.003017 
-0.000272 
-0.001893 
-0.004859 
-0.000067 
-0.007363 


final_layer_weights[0] = 0.567141
final_layer_weights[1] = 0.870194
final_layer_weights[2] = -0.314253
final_layer_weights[3] = 0.097177
final_layer_weights[4] = 0.979656
final_layer_weights[5] = 0.451736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.851983
final_layer_weights[9] = 1.338050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.423709


Output Embedding: 35.550949, 42.421282 

 Expected Embedding: -1.493413, -5.218942 
 loss: 1820.937845 


Updated weights for final layer:
final_layer_weights[0] = 0.577141
final_layer_weights[1] = 0.880194
final_layer_weights[2] = -0.324253
final_layer_weights[3] = 0.107177
final_layer_weights[4] = 0.989656
final_layer_weights[5] = 0.441736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.861983
final_layer_weights[9] = 1.348050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.433709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.516191
semi_final_layer_weights[1] = 0.009418
semi_final_layer_weights[2] = 0.048389
semi_final_layer_weights[3] = -0.443069
semi_final_layer_weights[4] = -0.017813
semi_final_layer_weights[5] = 0.018136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.196670
semi_final_layer_weights[9] = -0.067157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.601305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-3.480419, 2.566519]
[-17.999174, 15.393009]
[7.938871, -9.487745]
[18.708440, -16.548005]
[15.323486, -13.528270]
[1.660403, -1.883222]
[15.343500, -14.921091]
[0.000000, 0.000000]
[-10.478252, 8.283023]
[-8.802674, 8.457992]
Final Q Matrix:
[3.438718, -3.672096]
[17.422074, -17.713005]
[-7.224304, 6.186796]
[-18.015102, 18.080550]
[-14.759971, 14.824669]
[-1.528193, 1.354891]
[-14.544810, 14.015460]
[0.000000, 0.000000]
[10.257887, -10.720202]
[8.361907, -8.102432]
Final V Matrix:
[-2.761263, 3.790134]
[-16.000360, 18.390780]
[9.246815, -6.571444]
[17.075876, -18.802451]
[13.965473, -15.415140]
[1.851900, -1.432127]
[15.099562, -14.650378]
[0.000000, 0.000000]
[-8.764448, 11.093234]
[-8.579680, 8.463439]
 

Self-Attention Matrix: 
-2.761263 3.790134  
-2.761263 3.790134  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-9.380811 11.090457  
-2.761263 3.790134  
-2.761263 3.790134  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.023671 
-11.046865 48.565246 
-49.376702 35.340760 
-14.914554 -20.689897 
-14.758700 -13.968107 
-21.593184 20.560497 
-33.628850 4.692213 
-9.380811 11.090457 
-16.889828 39.077712 
5.947837 13.040995 




semi_final_layer_weights[0] = -0.516191
semi_final_layer_weights[1] = 0.009418
semi_final_layer_weights[2] = 0.048389
semi_final_layer_weights[3] = -0.443069
semi_final_layer_weights[4] = -0.017813
semi_final_layer_weights[5] = 0.018136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.196670
semi_final_layer_weights[9] = -0.067157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.601305


 Semi Final Layer Node Values: 
-0.046634 
0.362766 
-0.006308 
16.218297 
0.529524 
-0.000006 
-0.001384 
-0.000123 
-0.045604 
-0.013424 
.
.
.
-0.014783 
0.472329 
0.013459 
0.013952 
-0.000778 
-0.003653 
0.651575 
0.033998 
0.398835 
1.065920 
-0.000120 
1.629323 


final_layer_weights[0] = 0.577141
final_layer_weights[1] = 0.880194
final_layer_weights[2] = -0.324253
final_layer_weights[3] = 0.107177
final_layer_weights[4] = 0.989656
final_layer_weights[5] = 0.441736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.861983
final_layer_weights[9] = 1.348050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.433709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.433293 


Updated weights for final layer:
final_layer_weights[0] = 0.587141
final_layer_weights[1] = 0.870194
final_layer_weights[2] = -0.334253
final_layer_weights[3] = 0.117177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.871983
final_layer_weights[9] = 1.358050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.443709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.506191
semi_final_layer_weights[1] = -0.000582
semi_final_layer_weights[2] = 0.038389
semi_final_layer_weights[3] = -0.433069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.008136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.186670
semi_final_layer_weights[9] = -0.057157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.591305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.506191
semi_final_layer_weights[1] = -0.000582
semi_final_layer_weights[2] = 0.038389
semi_final_layer_weights[3] = -0.433069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.008136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.186670
semi_final_layer_weights[9] = -0.057157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.591305


 Semi Final Layer Node Values: 
-0.005062 
-0.000006 
0.038389 
-0.004331 
-0.000078 
0.008136 
-0.000050 
0.005475 
-0.001867 
-0.000572 
.
.
.
-0.005356 
0.164314 
-0.000050 
-0.000049 
-0.000187 
-0.001248 
0.230465 
0.002547 
0.137191 
0.383380 
0.005572 
0.591305 


final_layer_weights[0] = 0.587141
final_layer_weights[1] = 0.870194
final_layer_weights[2] = -0.334253
final_layer_weights[3] = 0.117177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.871983
final_layer_weights[9] = 1.358050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.443709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.833923 


Updated weights for final layer:
final_layer_weights[0] = 0.597141
final_layer_weights[1] = 0.871409
final_layer_weights[2] = -0.344253
final_layer_weights[3] = 0.127177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.881983
final_layer_weights[9] = 1.368050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.453709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.496191
semi_final_layer_weights[1] = 0.000633
semi_final_layer_weights[2] = 0.028389
semi_final_layer_weights[3] = -0.423069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.001864
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.176670
semi_final_layer_weights[9] = -0.047157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.581305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 3  total loss: 8648.156108 ******************************************************************* 

Epoch: 4
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.496191
semi_final_layer_weights[1] = 0.000633
semi_final_layer_weights[2] = 0.028389
semi_final_layer_weights[3] = -0.423069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.001864
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.176670
semi_final_layer_weights[9] = -0.047157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.581305


 Semi Final Layer Node Values: 
-0.113593 
0.007381 
1.025035 
0.402463 
-0.000021 
0.001773 
-0.000047 
0.004305 
0.168065 
0.044860 
.
.
.
0.499983 
-0.001468 
-0.000047 
-0.000049 
0.008287 
0.109227 
-0.002097 
0.002637 
-0.001210 
-0.003552 
0.004212 
-0.005530 


final_layer_weights[0] = 0.597141
final_layer_weights[1] = 0.871409
final_layer_weights[2] = -0.344253
final_layer_weights[3] = 0.127177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.881983
final_layer_weights[9] = 1.368050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.453709


Output Embedding: 20.399650, 20.925393 

 Expected Embedding: 12.470385, -17.532656 
 loss: 770.947369 


Updated weights for final layer:
final_layer_weights[0] = 0.607141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.354253
final_layer_weights[3] = 0.137177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.891983
final_layer_weights[9] = 1.378050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.463709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.486191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = 0.018389
semi_final_layer_weights[3] = -0.413069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.008136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.166670
semi_final_layer_weights[9] = -0.037157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.571305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.486191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = 0.018389
semi_final_layer_weights[3] = -0.413069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.008136
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.166670
semi_final_layer_weights[9] = -0.037157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.571305


 Semi Final Layer Node Values: 
-0.113474 
-0.000674 
0.710403 
0.545476 
0.010317 
-0.000107 
0.006665 
-0.000072 
0.220095 
0.049067 
.
.
.
0.680848 
-0.001906 
0.006646 
0.006406 
-0.000017 
0.138418 
-0.002779 
-0.000095 
-0.001548 
-0.004799 
-0.000074 
-0.007544 


final_layer_weights[0] = 0.607141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.354253
final_layer_weights[3] = 0.137177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.891983
final_layer_weights[9] = 1.378050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.463709


Output Embedding: 28.482893, 28.681095 

 Expected Embedding: 12.470385, -17.532656 
 loss: 1196.055613 


Updated weights for final layer:
final_layer_weights[0] = 0.617141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.147177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.901983
final_layer_weights[9] = 1.388050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.473709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.476191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.403069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.001864
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.156670
semi_final_layer_weights[9] = -0.027157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.561305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.476191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.403069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.001864
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.156670
semi_final_layer_weights[9] = -0.027157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.561305


 Semi Final Layer Node Values: 
4.162434 
0.227985 
-0.001400 
-0.045128 
0.105751 
-0.000867 
-0.001294 
0.204438 
-0.084670 
1.029358 
.
.
.
0.066300 
-0.000176 
-0.000007 
-0.000007 
0.001142 
0.012434 
-0.000263 
0.000364 
-0.000141 
-0.000463 
0.000581 
-0.000736 


final_layer_weights[0] = 0.617141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.147177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.901983
final_layer_weights[9] = 1.388050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.473709


Output Embedding: -0.000001, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 518.871427 


Updated weights for final layer:
final_layer_weights[0] = 0.627141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.157177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.911983
final_layer_weights[9] = 1.398050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.483709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.466191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.393069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.146670
semi_final_layer_weights[9] = -0.017157
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.551305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.466191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.393069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.146670
semi_final_layer_weights[9] = -0.017157
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.551305


 Semi Final Layer Node Values: 
-0.178062 
-0.000236 
-0.000450 
-0.264693 
-0.003047 
0.274142 
0.048448 
0.138368 
2.326659 
-0.003503 
.
.
.
0.143657 
-0.000360 
0.001459 
0.001406 
-0.000004 
0.024587 
-0.000552 
-0.000021 
-0.000282 
-0.000995 
-0.000016 
-0.001598 


final_layer_weights[0] = 0.627141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.157177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.911983
final_layer_weights[9] = 1.398050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.483709


Output Embedding: -0.040333, 10.690626 

 Expected Embedding: 4.643339, -27.140186 
 loss: 726.553566 


Updated weights for final layer:
final_layer_weights[0] = 0.637141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.167177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.921983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.493709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.456191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.383069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.136670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.541305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.456191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.383069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.136670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.541305


 Semi Final Layer Node Values: 
-0.098627 
-0.001218 
-0.003255 
-0.143128 
0.081569 
0.051711 
0.127478 
-0.002942 
-0.063836 
0.064675 
.
.
.
0.400755 
-0.000943 
-0.000041 
-0.000042 
0.006665 
0.061749 
-0.001489 
0.002288 
-0.000720 
-0.002751 
0.003654 
-0.004467 


final_layer_weights[0] = 0.637141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.167177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.921983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.493709


Output Embedding: 17.098242, 10.264866 

 Expected Embedding: 44.423141, -6.847703 
 loss: 519.745069 


Updated weights for final layer:
final_layer_weights[0] = 0.647141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.177177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.931983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.503709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.446191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.373069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.126670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.531305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.446191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.373069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.126670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.531305


 Semi Final Layer Node Values: 
21.520392 
-0.000035 
0.053343 
2.218672 
0.095608 
-0.002486 
0.141807 
-0.001931 
-0.011390 
-0.000857 
.
.
.
-0.004949 
0.108550 
-0.000052 
-0.000050 
0.002002 
-0.000675 
0.177388 
0.007522 
0.080326 
0.336513 
0.005798 
0.552882 


final_layer_weights[0] = 0.647141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.177177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.931983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.503709


Output Embedding: -0.000230, -0.000000 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1062.864392 


Updated weights for final layer:
final_layer_weights[0] = 0.657141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.187177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.941983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.513709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.436191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.363069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.116670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.521305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.436191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.363069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.116670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.521305


 Semi Final Layer Node Values: 
-0.132194 
0.053810 
-0.000747 
13.740494 
0.117971 
-0.000948 
0.171353 
0.043495 
0.931874 
0.396588 
.
.
.
-0.009649 
0.195468 
0.010294 
0.010671 
-0.000167 
-0.001136 
0.332568 
-0.000057 
0.139255 
0.649489 
-0.000092 
1.080419 


final_layer_weights[0] = 0.657141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.187177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.941983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.513709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013082 


Updated weights for final layer:
final_layer_weights[0] = 0.667141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.197177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.951983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.523709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.426191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.353069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.106670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.511305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.426191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.353069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.106670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.511305


 Semi Final Layer Node Values: 
-0.200752 
-0.001740 
-0.000600 
13.750127 
0.009411 
0.136528 
0.071874 
-0.002261 
-0.013187 
-0.001384 
.
.
.
0.548768 
-0.001016 
0.006062 
0.005843 
-0.000023 
0.053986 
-0.001812 
-0.000087 
-0.000689 
-0.003654 
-0.000067 
-0.006159 


final_layer_weights[0] = 0.667141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.197177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.951983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.523709


Output Embedding: 32.220157, 33.201930 

 Expected Embedding: -1.493413, -5.218942 
 loss: 1306.384101 


Updated weights for final layer:
final_layer_weights[0] = 0.677141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.207177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.961983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.533709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.416191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.343069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.096670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.501305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-3.480419, 2.566519]
[-17.999174, 15.393009]
[7.938871, -9.487745]
[18.708440, -16.548005]
[15.323486, -13.528270]
[1.660403, -1.883222]
[15.343500, -14.921091]
[0.000000, 0.000000]
[-10.478252, 8.283023]
[-8.802674, 8.457992]
Final Q Matrix:
[3.438718, -3.672096]
[17.422074, -17.713005]
[-7.224304, 6.186796]
[-18.015102, 18.080550]
[-14.759971, 14.824669]
[-1.528193, 1.354891]
[-14.544810, 14.015460]
[0.000000, 0.000000]
[10.257887, -10.720202]
[8.361907, -8.102432]
Final V Matrix:
[-2.761263, 3.790134]
[-16.000360, 18.390780]
[9.246815, -6.571444]
[17.075876, -18.802451]
[13.965473, -15.415140]
[1.851900, -1.432127]
[15.099562, -14.650378]
[0.000000, 0.000000]
[-8.764448, 11.093234]
[-8.579680, 8.463439]
 

Self-Attention Matrix: 
-2.761263 3.790134  
-2.761263 3.790134  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-9.380811 11.090457  
-2.761263 3.790134  
-2.761263 3.790134  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.023671 
-11.046865 48.565246 
-49.376702 35.340760 
-14.914554 -20.689897 
-14.758700 -13.968107 
-21.593184 20.560497 
-33.628850 4.692213 
-9.380811 11.090457 
-16.889828 39.077712 
5.947837 13.040995 




semi_final_layer_weights[0] = -0.416191
semi_final_layer_weights[1] = 0.005750
semi_final_layer_weights[2] = 0.008389
semi_final_layer_weights[3] = -0.343069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.096670
semi_final_layer_weights[9] = -0.007157
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.501305


 Semi Final Layer Node Values: 
-0.037600 
0.221486 
-0.001094 
12.557852 
-0.000650 
0.000072 
-0.001384 
-0.000123 
-0.022416 
-0.001431 
.
.
.
-0.012074 
0.201365 
0.013459 
0.013952 
-0.000219 
-0.000943 
0.380610 
-0.000075 
0.127871 
0.794956 
-0.000120 
1.358359 


final_layer_weights[0] = 0.677141
final_layer_weights[1] = 0.876526
final_layer_weights[2] = -0.364253
final_layer_weights[3] = 0.207177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.961983
final_layer_weights[9] = 1.408050
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.533709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.433293 


Updated weights for final layer:
final_layer_weights[0] = 0.687141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.217177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.971983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.543709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.406191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.333069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.086670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.491305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.406191
semi_final_layer_weights[1] = -0.004250
semi_final_layer_weights[2] = -0.001611
semi_final_layer_weights[3] = -0.333069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.086670
semi_final_layer_weights[9] = 0.002843
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.491305


 Semi Final Layer Node Values: 
-0.004062 
-0.000042 
-0.000016 
-0.003331 
-0.000078 
0.007808 
-0.000050 
0.005475 
-0.000867 
0.002843 
.
.
.
-0.004356 
0.064314 
-0.000050 
-0.000049 
0.001924 
-0.000248 
0.130465 
0.007228 
0.037191 
0.283380 
0.005572 
0.491305 


final_layer_weights[0] = 0.687141
final_layer_weights[1] = 0.866526
final_layer_weights[2] = -0.374253
final_layer_weights[3] = 0.217177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.971983
final_layer_weights[9] = 1.418050
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.543709


Output Embedding: -0.000000, -0.000001 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.833909 


Updated weights for final layer:
final_layer_weights[0] = 0.697141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.370889
final_layer_weights[3] = 0.227177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.981983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.553709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.396191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.001753
semi_final_layer_weights[3] = -0.323069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.076670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.481305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 4  total loss: 7729.701820 ******************************************************************* 

Epoch: 5
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.396191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.001753
semi_final_layer_weights[3] = -0.323069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.076670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.481305


 Semi Final Layer Node Values: 
-0.090700 
0.053900 
0.063307 
0.307334 
-0.000021 
0.002085 
-0.000047 
0.004305 
0.072936 
0.002943 
.
.
.
0.404854 
-0.000517 
-0.000047 
-0.000049 
0.001992 
0.014097 
-0.001146 
0.002637 
-0.000259 
-0.002601 
0.004212 
-0.004579 


final_layer_weights[0] = 0.697141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.370889
final_layer_weights[3] = 0.227177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.981983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.553709


Output Embedding: 18.461111, 17.929344 

 Expected Embedding: 12.470385, -17.532656 
 loss: 646.721109 


Updated weights for final layer:
final_layer_weights[0] = 0.707141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.380889
final_layer_weights[3] = 0.237177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.991983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.563709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.386191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.008247
semi_final_layer_weights[3] = -0.313069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.066670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.471305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.386191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.008247
semi_final_layer_weights[3] = -0.313069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.066670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.471305


 Semi Final Layer Node Values: 
-0.090135 
-0.000852 
-0.003186 
0.413421 
0.010317 
-0.000103 
0.006665 
-0.000072 
0.088041 
-0.000091 
.
.
.
0.548794 
-0.000585 
0.006646 
0.006406 
-0.000104 
0.006364 
-0.001459 
-0.000095 
-0.000227 
-0.003478 
-0.000074 
-0.006224 


final_layer_weights[0] = 0.707141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.380889
final_layer_weights[3] = 0.237177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.991983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.563709


Output Embedding: 25.305296, 24.399017 

 Expected Embedding: 12.470385, -17.532656 
 loss: 961.500057 


Updated weights for final layer:
final_layer_weights[0] = 0.717141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.370889
final_layer_weights[3] = 0.247177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.001983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.573709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.376191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.001753
semi_final_layer_weights[3] = -0.303069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.056670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.461305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.376191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.001753
semi_final_layer_weights[3] = -0.303069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.056670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.461305


 Semi Final Layer Node Values: 
3.288324 
0.183387 
-0.000293 
-0.033932 
0.105751 
-0.001020 
-0.001294 
0.204438 
-0.030626 
0.117281 
.
.
.
0.053186 
-0.000045 
-0.000007 
-0.000007 
0.000275 
-0.000007 
-0.000132 
0.000364 
-0.000009 
-0.000332 
0.000581 
-0.000605 


final_layer_weights[0] = 0.717141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.370889
final_layer_weights[3] = 0.247177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.001983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.573709


Output Embedding: -0.030787, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 519.471152 


Updated weights for final layer:
final_layer_weights[0] = 0.727141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.257177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.011983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.583709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.366191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.293069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.046670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.451305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.366191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.293069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.046670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.451305


 Semi Final Layer Node Values: 
-0.139867 
-0.000299 
-0.002055 
-0.197353 
-0.003047 
0.274142 
0.048448 
0.138368 
0.740337 
0.141013 
.
.
.
0.114669 
-0.000070 
0.001459 
0.001406 
-0.000023 
0.001397 
-0.000262 
-0.000021 
0.000814 
-0.000705 
-0.000016 
-0.001308 


final_layer_weights[0] = 0.727141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.257177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.011983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.583709


Output Embedding: -0.146600, 6.252772 

 Expected Embedding: 4.643339, -27.140186 
 loss: 569.016595 


Updated weights for final layer:
final_layer_weights[0] = 0.737141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.267177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.021983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.593709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.356191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.283069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.036670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.441305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.356191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.283069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.036670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.441305


 Semi Final Layer Node Values: 
-0.077007 
-0.000980 
-0.001026 
-0.105765 
0.081569 
0.051711 
0.127478 
-0.002942 
-0.017128 
0.027960 
.
.
.
0.318224 
-0.000118 
-0.000041 
-0.000042 
0.001728 
-0.000043 
-0.000664 
0.002288 
-0.000059 
-0.001926 
0.003654 
-0.003642 


final_layer_weights[0] = 0.737141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.267177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.021983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.593709


Output Embedding: 14.317094, 8.769325 

 Expected Embedding: 44.423141, -6.847703 
 loss: 575.132813 


Updated weights for final layer:
final_layer_weights[0] = 0.747141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.277177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.031983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.603709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.346191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.273069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.026670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.431305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.346191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.273069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.026670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.431305


 Semi Final Layer Node Values: 
16.697258 
-0.000044 
0.243525 
1.623964 
0.095608 
-0.002486 
0.141807 
-0.001931 
-0.002398 
-0.002082 
.
.
.
-0.003908 
0.004489 
-0.000052 
-0.000050 
0.008227 
-0.000050 
0.073327 
0.007522 
-0.000029 
0.232452 
0.005798 
0.448821 


final_layer_weights[0] = 0.747141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.277177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.031983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.603709


Output Embedding: -0.001788, -0.000004 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1062.909616 


Updated weights for final layer:
final_layer_weights[0] = 0.757141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.287177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.041983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.613709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.336191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.263069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.016670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.421305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.336191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.263069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.016670
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.421305


 Semi Final Layer Node Values: 
-0.101888 
0.043283 
-0.000236 
9.955953 
0.117971 
-0.000948 
0.171353 
0.043495 
0.133148 
0.171455 
.
.
.
-0.007577 
-0.000118 
0.010294 
0.010671 
-0.000043 
0.010738 
0.125315 
-0.000057 
0.014904 
0.442236 
-0.000092 
0.873167 


final_layer_weights[0] = 0.757141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.287177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.041983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.613709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013082 


Updated weights for final layer:
final_layer_weights[0] = 0.767141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.297177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.623709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.326191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.253069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.411305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.326191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.253069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.411305


 Semi Final Layer Node Values: 
-0.153648 
-0.002201 
-0.002740 
9.855668 
0.009411 
0.136528 
0.071874 
-0.002261 
-0.000825 
-0.003361 
.
.
.
0.428314 
-0.000052 
0.006062 
0.005843 
-0.000095 
0.005805 
-0.000608 
-0.000087 
0.003384 
-0.002450 
-0.000067 
-0.004954 


final_layer_weights[0] = 0.767141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.297177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.623709


Output Embedding: 31.526429, 26.013251 

 Expected Embedding: -1.493413, -5.218942 
 loss: 1032.879921 


Updated weights for final layer:
final_layer_weights[0] = 0.777141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.307177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.633709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.316191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.243069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.401305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-3.480419, 2.566519]
[-17.999174, 15.393009]
[7.938871, -9.487745]
[18.708440, -16.548005]
[15.323486, -13.528270]
[1.660403, -1.883222]
[15.343500, -14.921091]
[0.000000, 0.000000]
[-10.478252, 8.283023]
[-8.802674, 8.457992]
Final Q Matrix:
[3.438718, -3.672096]
[17.422074, -17.713005]
[-7.224304, 6.186796]
[-18.015102, 18.080550]
[-14.759971, 14.824669]
[-1.528193, 1.354891]
[-14.544810, 14.015460]
[0.000000, 0.000000]
[10.257887, -10.720202]
[8.361907, -8.102432]
Final V Matrix:
[-2.761263, 3.790134]
[-16.000360, 18.390780]
[9.246815, -6.571444]
[17.075876, -18.802451]
[13.965473, -15.415140]
[1.851900, -1.432127]
[15.099562, -14.650378]
[0.000000, 0.000000]
[-8.764448, 11.093234]
[-8.579680, 8.463439]
 

Self-Attention Matrix: 
-2.761263 3.790134  
-2.761263 3.790134  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-9.380811 11.090457  
-2.761263 3.790134  
-2.761263 3.790134  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.023671 
-11.046865 48.565246 
-49.376702 35.340760 
-14.914554 -20.689897 
-14.758700 -13.968107 
-21.593184 20.560497 
-33.628850 4.692213 
-9.380811 11.090457 
-16.889828 39.077712 
5.947837 13.040995 




semi_final_layer_weights[0] = -0.316191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.243069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.401305


 Semi Final Layer Node Values: 
-0.028565 
0.178159 
-0.000345 
8.897407 
-0.000650 
0.000072 
-0.001384 
-0.000123 
0.077216 
-0.000618 
.
.
.
-0.009364 
-0.000154 
0.013459 
0.013952 
-0.000057 
0.014039 
0.109646 
-0.000075 
0.019485 
0.523991 
-0.000120 
1.087394 


final_layer_weights[0] = 0.777141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.307177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.633709


Output Embedding: -0.000000, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.433292 


Updated weights for final layer:
final_layer_weights[0] = 0.787141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.317177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.643709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.306191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.233069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.391305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.306191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.233069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.391305


 Semi Final Layer Node Values: 
-0.003062 
-0.000054 
-0.000074 
-0.002331 
-0.000078 
0.007808 
-0.000050 
0.005475 
-0.000067 
0.006906 
.
.
.
-0.003356 
0.004314 
-0.000050 
-0.000049 
0.007906 
-0.000048 
0.030465 
0.007228 
-0.000028 
0.183380 
0.005572 
0.391305 


final_layer_weights[0] = 0.787141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.317177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.643709


Output Embedding: -0.000009, -0.000021 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.833453 


Updated weights for final layer:
final_layer_weights[0] = 0.797141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.327177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.653709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.296191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.223069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.381305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 5  total loss: 6995.911091 ******************************************************************* 

Epoch: 6
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.296191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.223069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.381305


 Semi Final Layer Node Values: 
-0.067807 
0.053900 
0.095515 
0.212204 
-0.000021 
0.002085 
-0.000047 
0.004305 
-0.000032 
0.002943 
.
.
.
0.309724 
0.004466 
-0.000047 
-0.000049 
0.001992 
-0.000049 
-0.000195 
0.002637 
-0.000029 
-0.001649 
0.004212 
-0.003627 


final_layer_weights[0] = 0.797141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.327177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.653709


Output Embedding: 14.771147, 14.003189 

 Expected Embedding: 12.470385, -17.532656 
 loss: 499.901502 


Updated weights for final layer:
final_layer_weights[0] = 0.807141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.337177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.663709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.286191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.213069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.371305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.286191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.213069
semi_final_layer_weights[4] = -0.007813
semi_final_layer_weights[5] = 0.007808
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.371305


 Semi Final Layer Node Values: 
-0.066796 
-0.000852 
-0.002841 
0.281367 
0.010317 
-0.000103 
0.006665 
-0.000072 
0.008808 
-0.000091 
.
.
.
0.416740 
-0.000070 
0.006646 
0.006406 
-0.000104 
0.006364 
-0.000138 
-0.000095 
0.009168 
-0.002158 
-0.000074 
-0.004903 


final_layer_weights[0] = 0.807141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.337177
final_layer_weights[4] = 0.999656
final_layer_weights[5] = 0.431408
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.663709


Output Embedding: 20.184301, 18.950043 

 Expected Embedding: 12.470385, -17.532656 
 loss: 695.245899 


Updated weights for final layer:
final_layer_weights[0] = 0.817141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.347177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.673709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.276191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.203069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.361305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.276191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.203069
semi_final_layer_weights[4] = 0.002187
semi_final_layer_weights[5] = -0.002192
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.361305


 Semi Final Layer Node Values: 
2.414214 
0.183387 
-0.000442 
-0.022736 
0.105751 
-0.001020 
-0.001294 
0.204438 
0.179964 
0.117281 
.
.
.
0.040073 
0.000616 
-0.000007 
-0.000007 
0.000275 
-0.000007 
-0.000001 
0.000364 
-0.000004 
-0.000201 
0.000581 
-0.000474 


final_layer_weights[0] = 0.817141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.347177
final_layer_weights[4] = 1.009656
final_layer_weights[5] = 0.421408
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.673709


Output Embedding: 8.059640, -0.000000 

 Expected Embedding: 19.465172, 25.668072 
 loss: 394.468037 


Updated weights for final layer:
final_layer_weights[0] = 0.827141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.357177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.683709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.266191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.193069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.351305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.266191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.193069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.351305


 Semi Final Layer Node Values: 
-0.101672 
-0.000299 
-0.002055 
-0.130013 
-0.002512 
0.226660 
0.048448 
0.138368 
0.105808 
0.141013 
.
.
.
0.085682 
-0.000015 
0.001459 
0.001406 
-0.000018 
0.001397 
0.000397 
-0.000021 
0.002013 
-0.000416 
-0.000016 
-0.001018 


final_layer_weights[0] = 0.827141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.357177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.683709


Output Embedding: 0.182332, 3.385625 

 Expected Embedding: 4.643339, -27.140186 
 loss: 475.862863 


Updated weights for final layer:
final_layer_weights[0] = 0.837141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.367177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.693709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.256191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.183069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.341305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.256191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.183069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.341305


 Semi Final Layer Node Values: 
-0.055388 
-0.000980 
-0.001026 
-0.068401 
0.132778 
0.083610 
0.127478 
-0.002942 
0.155538 
0.027960 
.
.
.
0.235693 
0.003875 
-0.000041 
-0.000042 
0.003165 
-0.000043 
-0.000042 
0.002288 
-0.000025 
-0.001101 
0.003654 
-0.002817 


final_layer_weights[0] = 0.837141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.367177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.693709


Output Embedding: 11.091804, 7.019862 

 Expected Embedding: 44.423141, -6.847703 
 loss: 651.643718 


Updated weights for final layer:
final_layer_weights[0] = 0.847141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.377177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.703709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.246191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.173069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.331305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.246191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.173069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.331305


 Semi Final Layer Node Values: 
11.874123 
-0.000044 
0.243525 
1.029256 
0.078807 
-0.002055 
0.141807 
-0.001931 
-0.000600 
-0.002082 
.
.
.
-0.002868 
0.005520 
-0.000052 
-0.000050 
0.006416 
-0.000050 
-0.000051 
0.007522 
-0.000072 
0.128391 
0.005798 
0.344760 


final_layer_weights[0] = 0.847141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.377177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.703709


Output Embedding: -0.027126, -0.000565 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1063.662961 


Updated weights for final layer:
final_layer_weights[0] = 0.857141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.387177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.713709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.236191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.163069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.321305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.236191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.163069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.321305


 Semi Final Layer Node Values: 
-0.071581 
0.043283 
-0.000236 
6.171413 
0.192033 
-0.001532 
0.171353 
0.043495 
-0.000266 
0.171455 
.
.
.
-0.005504 
-0.000097 
0.010294 
0.010671 
-0.000079 
0.010738 
0.010666 
-0.000057 
0.006336 
0.234983 
-0.000092 
0.665914 


final_layer_weights[0] = 0.857141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.387177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.713709


Output Embedding: -0.000000, -0.000002 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.013142 


Updated weights for final layer:
final_layer_weights[0] = 0.867141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.397177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.723709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.226191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.153069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.311305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.226191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.153069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.311305


 Semi Final Layer Node Values: 
-0.106544 
-0.002201 
-0.002740 
5.961209 
0.007757 
0.112881 
0.071874 
-0.002261 
-0.000825 
-0.003361 
.
.
.
0.307859 
-0.000064 
0.006062 
0.005843 
-0.000074 
0.005805 
0.005846 
-0.000087 
0.008363 
-0.001245 
-0.000067 
-0.003750 


final_layer_weights[0] = 0.867141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.397177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.723709


Output Embedding: 27.975833, 18.680146 

 Expected Embedding: -1.493413, -5.218942 
 loss: 719.801450 


Updated weights for final layer:
final_layer_weights[0] = 0.877141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.407177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.733709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.216191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.143069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.301305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-3.480419, 2.566519]
[-17.999174, 15.393009]
[7.938871, -9.487745]
[18.708440, -16.548005]
[15.323486, -13.528270]
[1.660403, -1.883222]
[15.343500, -14.921091]
[0.000000, 0.000000]
[-10.478252, 8.283023]
[-8.802674, 8.457992]
Final Q Matrix:
[3.438718, -3.672096]
[17.422074, -17.713005]
[-7.224304, 6.186796]
[-18.015102, 18.080550]
[-14.759971, 14.824669]
[-1.528193, 1.354891]
[-14.544810, 14.015460]
[0.000000, 0.000000]
[10.257887, -10.720202]
[8.361907, -8.102432]
Final V Matrix:
[-2.761263, 3.790134]
[-16.000360, 18.390780]
[9.246815, -6.571444]
[17.075876, -18.802451]
[13.965473, -15.415140]
[1.851900, -1.432127]
[15.099562, -14.650378]
[0.000000, 0.000000]
[-8.764448, 11.093234]
[-8.579680, 8.463439]
 

Self-Attention Matrix: 
-2.761263 3.790134  
-2.761263 3.790134  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-9.380811 11.090457  
-2.761263 3.790134  
-2.761263 3.790134  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.023671 
-11.046865 48.565246 
-49.376702 35.340760 
-14.914554 -20.689897 
-14.758700 -13.968107 
-21.593184 20.560497 
-33.628850 4.692213 
-9.380811 11.090457 
-16.889828 39.077712 
5.947837 13.040995 




semi_final_layer_weights[0] = -0.216191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.143069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.301305


 Semi Final Layer Node Values: 
-0.019531 
0.178159 
-0.000345 
5.236962 
-0.001058 
0.000116 
-0.001384 
-0.000123 
0.077216 
-0.000618 
.
.
.
-0.006654 
-0.000127 
0.013459 
0.013952 
-0.000104 
0.014039 
0.013945 
-0.000075 
0.008284 
0.253027 
-0.000120 
0.816430 


final_layer_weights[0] = 0.877141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.407177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.733709


Output Embedding: -0.000052, -0.000000 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.431339 


Updated weights for final layer:
final_layer_weights[0] = 0.887141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.417177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.743709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.206191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.133069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.291305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.206191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007355
semi_final_layer_weights[3] = -0.133069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.291305


 Semi Final Layer Node Values: 
-0.002062 
-0.000054 
-0.000074 
-0.001331 
-0.000064 
0.006455 
-0.000050 
0.005475 
-0.000067 
0.006906 
.
.
.
-0.002356 
0.005305 
-0.000050 
-0.000049 
0.006165 
-0.000048 
-0.000049 
0.007228 
-0.000069 
0.083380 
0.005572 
0.291305 


final_layer_weights[0] = 0.887141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379997
final_layer_weights[3] = 0.417177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.743709


Output Embedding: -0.000601, -0.000899 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.811825 


Updated weights for final layer:
final_layer_weights[0] = 0.897141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.427177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.753709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.196191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.123069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.281305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 6  total loss: 6128.842736 ******************************************************************* 

Epoch: 7
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.196191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002645
semi_final_layer_weights[3] = -0.123069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.281305


 Semi Final Layer Node Values: 
-0.044914 
0.053900 
0.095515 
0.117075 
-0.000034 
0.003372 
-0.000047 
0.004305 
-0.000032 
0.002943 
.
.
.
0.214595 
0.004466 
-0.000047 
-0.000049 
0.003648 
-0.000049 
-0.000049 
0.002637 
-0.000029 
-0.000698 
0.004212 
-0.002676 


final_layer_weights[0] = 0.897141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369997
final_layer_weights[3] = 0.427177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.753709


Output Embedding: 10.341280, 9.810357 

 Expected Embedding: 12.470385, -17.532656 
 loss: 376.086725 


Updated weights for final layer:
final_layer_weights[0] = 0.907141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.437177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.763709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.186191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.113069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.271305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.186191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.113069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.271305


 Semi Final Layer Node Values: 
-0.043456 
-0.000852 
-0.002821 
0.149313 
0.008504 
-0.000085 
0.006665 
-0.000072 
0.008808 
-0.000091 
.
.
.
0.284685 
-0.000070 
0.006646 
0.006406 
-0.000081 
0.006364 
0.006409 
-0.000095 
0.009168 
-0.000837 
-0.000074 
-0.003583 


final_layer_weights[0] = 0.907141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.437177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.763709


Output Embedding: 13.913770, 13.085621 

 Expected Embedding: 12.470385, -17.532656 
 loss: 469.781113 


Updated weights for final layer:
final_layer_weights[0] = 0.917141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.447177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.773709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.176191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.103069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.261305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.176191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.103069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.261305


 Semi Final Layer Node Values: 
1.540103 
0.183387 
-0.000450 
-0.011540 
0.172141 
-0.001650 
-0.001294 
0.204438 
0.179964 
0.117281 
.
.
.
0.026959 
0.000616 
-0.000007 
-0.000007 
0.000503 
-0.000007 
-0.000007 
0.000364 
-0.000004 
-0.000070 
0.000581 
-0.000343 


final_layer_weights[0] = 0.917141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.447177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.773709


Output Embedding: 8.726188, -0.000021 

 Expected Embedding: 19.465172, 25.668072 
 loss: 387.088372 


Updated weights for final layer:
final_layer_weights[0] = 0.927141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.457177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.783709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.166191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.093069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.251305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.166191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.093069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.251305


 Semi Final Layer Node Values: 
-0.063477 
-0.000299 
-0.002041 
-0.062673 
-0.002512 
0.226660 
0.048448 
0.138368 
0.105808 
0.141013 
.
.
.
0.056694 
-0.000015 
0.001459 
0.001406 
-0.000018 
0.001397 
0.001407 
-0.000021 
0.002013 
-0.000126 
-0.000016 
-0.000728 


final_layer_weights[0] = 0.927141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.457177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.783709


Output Embedding: 0.334203, 1.998274 

 Expected Embedding: 4.643339, -27.140186 
 loss: 433.809264 


Updated weights for final layer:
final_layer_weights[0] = 0.937141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.467177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.793709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.156191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.083069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.241305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.156191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.083069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.241305


 Semi Final Layer Node Values: 
-0.033768 
-0.000980 
-0.001046 
-0.031038 
0.132778 
0.083610 
0.127478 
-0.002942 
0.155538 
0.027960 
.
.
.
0.153162 
0.003875 
-0.000041 
-0.000042 
0.003165 
-0.000043 
-0.000042 
0.002288 
-0.000025 
-0.000275 
0.003654 
-0.001992 


final_layer_weights[0] = 0.937141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.467177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.793709


Output Embedding: 7.382024, 5.318612 

 Expected Embedding: 44.423141, -6.847703 
 loss: 760.031792 


Updated weights for final layer:
final_layer_weights[0] = 0.947141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.477177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.803709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.146191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.073069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.231305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.146191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.073069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.231305


 Semi Final Layer Node Values: 
7.050989 
-0.000044 
0.241830 
0.434547 
0.078807 
-0.002055 
0.141807 
-0.001931 
-0.000600 
-0.002082 
.
.
.
-0.001827 
0.005520 
-0.000052 
-0.000050 
0.006416 
-0.000050 
-0.000051 
0.007522 
-0.000072 
0.024329 
0.005798 
0.240699 


final_layer_weights[0] = 0.947141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.477177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.803709


Output Embedding: -0.252262, -0.082699 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1073.158485 


Updated weights for final layer:
final_layer_weights[0] = 0.957141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.487177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.813709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.136191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.063069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.221305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.136191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.063069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.221305


 Semi Final Layer Node Values: 
-0.041275 
0.043283 
-0.000240 
2.386872 
0.192033 
-0.001532 
0.171353 
0.043495 
-0.000266 
0.171455 
.
.
.
-0.003432 
-0.000097 
0.010294 
0.010671 
-0.000079 
0.010738 
0.010666 
-0.000057 
0.006336 
0.027730 
-0.000092 
0.458661 


final_layer_weights[0] = 0.957141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.487177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.813709


Output Embedding: -0.000170, -0.000387 

 Expected Embedding: 1.038754, 36.179375 
 loss: 655.027274 


Updated weights for final layer:
final_layer_weights[0] = 0.967141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.497177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.823709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.126191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.053069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.211305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.126191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.053069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.211305


 Semi Final Layer Node Values: 
-0.059441 
-0.002201 
-0.002721 
2.066750 
0.007757 
0.112881 
0.071874 
-0.002261 
-0.000825 
-0.003361 
.
.
.
0.187405 
-0.000064 
0.006062 
0.005843 
-0.000074 
0.005805 
0.005846 
-0.000087 
0.008363 
-0.000041 
-0.000067 
-0.002545 


final_layer_weights[0] = 0.967141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.497177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.823709


Output Embedding: 19.640088, 11.005259 

 Expected Embedding: -1.493413, -5.218942 
 loss: 354.924784 


Updated weights for final layer:
final_layer_weights[0] = 0.977141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.507177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.833709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.116191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.043069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.201305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-3.480419, 2.566519]
[-17.999174, 15.393009]
[7.938871, -9.487745]
[18.708440, -16.548005]
[15.323486, -13.528270]
[1.660403, -1.883222]
[15.343500, -14.921091]
[0.000000, 0.000000]
[-10.478252, 8.283023]
[-8.802674, 8.457992]
Final Q Matrix:
[3.438718, -3.672096]
[17.422074, -17.713005]
[-7.224304, 6.186796]
[-18.015102, 18.080550]
[-14.759971, 14.824669]
[-1.528193, 1.354891]
[-14.544810, 14.015460]
[0.000000, 0.000000]
[10.257887, -10.720202]
[8.361907, -8.102432]
Final V Matrix:
[-2.761263, 3.790134]
[-16.000360, 18.390780]
[9.246815, -6.571444]
[17.075876, -18.802451]
[13.965473, -15.415140]
[1.851900, -1.432127]
[15.099562, -14.650378]
[0.000000, 0.000000]
[-8.764448, 11.093234]
[-8.579680, 8.463439]
 

Self-Attention Matrix: 
-2.761263 3.790134  
-2.761263 3.790134  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-16.000360 18.390780  
-9.380811 11.090457  
-2.761263 3.790134  
-2.761263 3.790134  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.989425 18.023671 
-11.046865 48.565246 
-49.376702 35.340760 
-14.914554 -20.689897 
-14.758700 -13.968107 
-21.593184 20.560497 
-33.628850 4.692213 
-9.380811 11.090457 
-16.889828 39.077712 
5.947837 13.040995 




semi_final_layer_weights[0] = -0.116191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.043069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.201305


 Semi Final Layer Node Values: 
-0.010497 
0.178159 
-0.000352 
1.576517 
-0.001058 
0.000116 
-0.001384 
-0.000123 
0.077216 
-0.000618 
.
.
.
-0.003945 
-0.000127 
0.013459 
0.013952 
-0.000104 
0.014039 
0.013945 
-0.000075 
0.008284 
-0.000179 
-0.000120 
0.545465 


final_layer_weights[0] = 0.977141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.507177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.833709


Output Embedding: -0.023311, -0.000138 

 Expected Embedding: -37.748848, 10.192697 
 loss: 763.555025 


Updated weights for final layer:
final_layer_weights[0] = 0.987141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.517177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.843709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.106191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.033069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.191305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.106191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.007303
semi_final_layer_weights[3] = -0.033069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006670
semi_final_layer_weights[9] = 0.006906
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.191305


 Semi Final Layer Node Values: 
-0.001062 
-0.000054 
-0.000073 
-0.000331 
-0.000064 
0.006455 
-0.000050 
0.005475 
-0.000067 
0.006906 
.
.
.
-0.001356 
0.005305 
-0.000050 
-0.000049 
0.006165 
-0.000048 
-0.000049 
0.007228 
-0.000069 
0.003380 
0.005572 
0.191305 


final_layer_weights[0] = 0.987141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.379945
final_layer_weights[3] = 0.517177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.051983
final_layer_weights[9] = 1.422113
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.843709


Output Embedding: -0.028713, -0.036039 

 Expected Embedding: -10.885829, -17.296432 
 loss: 207.899080 


Updated weights for final layer:
final_layer_weights[0] = 0.997141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.527177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.853709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.096191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.023069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.181305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 7  total loss: 5481.361914 ******************************************************************* 

Epoch: 8
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-11.648366, 12.438396]
[-7.110600, 8.230264]
[-18.250090, 16.704387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.852665, -9.970915]
[6.516209, -5.702568]
[17.477961, -17.299079]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.364447, 10.488205]
[-8.064932, 6.038586]
[-17.113390, 18.021138]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.064932 6.038586  
-8.064932 6.038586  
-8.064932 6.038586  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  
-10.214690 8.263395  


Context Matrix (embedding_matrix + self_attention_matrix:
20.560880 1.332071 
18.156631 -5.503319 
-1.414293 36.521076 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 
-10.214690 8.263395 




semi_final_layer_weights[0] = -0.096191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.002697
semi_final_layer_weights[3] = -0.023069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003330
semi_final_layer_weights[9] = -0.003094
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.181305


 Semi Final Layer Node Values: 
-0.022021 
0.053900 
0.097363 
0.021945 
-0.000034 
0.003372 
-0.000047 
0.004305 
-0.000032 
0.002943 
.
.
.
0.119465 
0.004466 
-0.000047 
-0.000049 
0.003648 
-0.000049 
-0.000049 
0.002637 
-0.000029 
0.003469 
0.004212 
-0.001725 


final_layer_weights[0] = 0.997141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.369945
final_layer_weights[3] = 0.527177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.061983
final_layer_weights[9] = 1.412113
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.853709


Output Embedding: 5.592824, 5.414046 

 Expected Embedding: 12.470385, -17.532656 
 loss: 286.925983 


Updated weights for final layer:
final_layer_weights[0] = 1.007141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.537177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.863709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.086191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = -0.013069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.171305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[12.198711, -11.941076]
[7.297141, -8.002193]
[18.883040, -20.428743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-12.903283, 13.554714]
[-7.577876, 7.601046]
[-19.655169, 19.834051]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.914677, -13.054605]
[7.823742, -7.324900]
[20.019740, -19.111993]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.823742 -7.324900  
7.823742 -7.324900  
7.823742 -7.324900  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  
9.869209 -10.189753  


Context Matrix (embedding_matrix + self_attention_matrix:
29.133465 -4.793975 
8.934729 5.926482 
14.474382 23.157590 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 
9.869209 -10.189753 




semi_final_layer_weights[0] = -0.086191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = -0.013069
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.171305


 Semi Final Layer Node Values: 
-0.020117 
-0.000852 
-0.001947 
0.017258 
0.008504 
-0.000085 
0.006665 
-0.000072 
0.008220 
-0.000076 
.
.
.
0.152631 
-0.000070 
0.006646 
0.006406 
-0.000081 
0.006364 
0.006409 
-0.000068 
0.007546 
-0.000084 
-0.000074 
-0.002262 


final_layer_weights[0] = 1.007141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.537177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.863709


Output Embedding: 7.254376, 6.919369 

 Expected Embedding: 12.470385, -17.532656 
 loss: 312.554123 


Updated weights for final layer:
final_layer_weights[0] = 1.017141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.873709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.076191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.161305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[3.344240, -3.619516]
[-19.512352, 20.636786]
[6.341179, -5.166860]
[-5.615109, 6.483866]
[-23.779996, 24.725325]
[-22.029325, 21.040563]
[11.880782, -11.599901]
[21.156480, -21.500694]
[-25.673537, 25.126377]
[16.748897, -14.983944]
Final Q Matrix:
[-3.107533, 2.833447]
[18.213381, -16.822308]
[-6.181535, 6.394704]
[5.148358, -4.512503]
[22.269381, -20.757696]
[20.947785, -20.352942]
[-11.254464, 10.824618]
[-19.897222, 18.766964]
[24.309912, -23.355206]
[-16.099335, 16.084827]
Final V Matrix:
[3.589148, -2.983500]
[-20.550429, 17.682508]
[5.429133, -6.625351]
[-6.356215, 4.777359]
[-24.700227, 21.792535]
[-21.368887, 21.252380]
[11.729383, -11.317715]
[21.572046, -19.671856]
[-25.394902, 24.422653]
[15.424621, -16.736490]
 

Self-Attention Matrix: 
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  
3.589148 -2.983500  
3.589148 -2.983500  
3.589148 -2.983500  
-20.550429 17.682508  
-20.550429 17.682508  
3.589148 -2.983500  
-20.550429 17.682508  


Context Matrix (embedding_matrix + self_attention_matrix:
-29.433908 19.692806 
48.810301 -8.161557 
-14.116026 -1.575776 
24.084221 -11.888217 
52.867915 -3.513437 
23.652792 21.883429 
-34.834522 7.703744 
-57.574446 11.397012 
35.276801 17.766539 
-21.900288 -15.003695 




semi_final_layer_weights[0] = -0.076191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.161305


 Semi Final Layer Node Values: 
0.665993 
0.183387 
-0.000828 
-0.000344 
0.172141 
-0.001650 
-0.001294 
0.204438 
0.204033 
0.159812 
.
.
.
0.013846 
0.000616 
-0.000007 
-0.000007 
0.000503 
-0.000007 
-0.000007 
0.000632 
-0.000006 
0.000478 
0.000581 
-0.000212 


final_layer_weights[0] = 1.017141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.873709


Output Embedding: 5.446934, -0.063098 

 Expected Embedding: 19.465172, 25.668072 
 loss: 429.302039 


Updated weights for final layer:
final_layer_weights[0] = 1.027141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.883709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.066191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.151305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[18.685488, -20.272013]
[2.079066, -2.499367]
[13.504794, -14.118754]
[33.589743, -35.003596]
[19.091982, -20.787815]
[17.153191, -18.016456]
[-4.906579, 4.683513]
[12.144389, -12.699643]
[-8.137466, 7.424296]
[9.676308, -10.022084]
Final Q Matrix:
[-19.440204, 19.592901]
[-2.123109, 2.036108]
[-14.137515, 14.475114]
[-35.182021, 36.070010]
[-19.850865, 19.974978]
[-17.943177, 18.336386]
[5.209537, -5.522508]
[-12.712858, 13.015108]
[8.696137, -9.361605]
[-10.145077, 10.427125]
Final V Matrix:
[19.856499, -18.879924]
[2.406982, -1.963308]
[13.919294, -13.945547]
[34.528924, -34.749812]
[20.349102, -19.248494]
[17.747314, -17.665988]
[-4.695507, 5.318154]
[12.519681, -12.538971]
[-7.509167, 9.013495]
[9.896990, -10.045163]
 

Self-Attention Matrix: 
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  
19.856499 -18.879924  
2.406982 -1.963308  


Context Matrix (embedding_matrix + self_attention_matrix:
8.216151 28.978880 
-0.248253 4.809813 
13.820081 13.122649 
32.327545 34.012465 
7.329538 30.673808 
15.773302 18.338120 
9.667620 -18.266195 
12.627826 11.645890 
-1.690004 -13.173218 
11.859459 7.559947 




semi_final_layer_weights[0] = -0.066191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.151305


 Semi Final Layer Node Values: 
-0.025282 
-0.000299 
-0.001408 
0.466734 
-0.002512 
0.226660 
0.048448 
0.138368 
0.098743 
0.118101 
.
.
.
0.027707 
-0.000015 
0.001459 
0.001406 
-0.000018 
0.001397 
0.001407 
-0.000015 
0.001657 
-0.000018 
-0.000016 
-0.000439 


final_layer_weights[0] = 1.027141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.883709


Output Embedding: 1.010739, 1.657408 

 Expected Embedding: 4.643339, -27.140186 
 loss: 421.248623 


Updated weights for final layer:
final_layer_weights[0] = 1.037141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.893709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.056191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.141305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-9.875602, 9.482788]
[10.796392, -10.611497]
[18.553405, -16.421587]
[-17.685505, 15.438474]
[-18.434884, 19.449791]
[11.056930, -9.868750]
[-11.899277, 11.356100]
[-31.153021, 29.769257]
[-22.272413, 19.732451]
[4.830737, -5.117276]
Final Q Matrix:
[9.382155, -9.093698]
[-10.215246, 9.794238]
[-17.863986, 17.924263]
[17.064978, -17.215298]
[17.215728, -15.921965]
[-10.632042, 10.632416]
[11.316626, -10.999246]
[29.621076, -28.773606]
[21.441530, -21.505605]
[-4.507755, 4.159837]
Final V Matrix:
[-9.620462, 9.498512]
[10.715896, -10.244563]
[16.943063, -18.640522]
[-15.976085, 17.891225]
[-19.377147, 16.733159]
[10.164009, -11.061885]
[-11.535161, 11.484794]
[-30.230826, 30.046017]
[-20.354840, 22.366046]
[5.094358, -4.373059]
 

Self-Attention Matrix: 
10.715896 -10.244563  
-9.620462 9.498512  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  
10.715896 -10.244563  
10.715896 -10.244563  
10.715896 -10.244563  
-9.620462 9.498512  


Context Matrix (embedding_matrix + self_attention_matrix:
20.402526 0.217093 
-23.566251 1.387323 
-8.690681 -29.112599 
6.879960 29.483582 
52.788963 -14.491542 
-10.195518 -12.392802 
21.428643 3.311099 
39.288181 24.724019 
9.862917 35.845280 
-20.927992 10.891437 




semi_final_layer_weights[0] = -0.056191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.141305


 Semi Final Layer Node Values: 
-0.012148 
-0.000980 
-0.001924 
-0.001147 
0.132778 
0.083610 
0.127478 
-0.002942 
0.176340 
0.038100 
.
.
.
0.070632 
0.003875 
-0.000041 
-0.000042 
0.003165 
-0.000043 
-0.000042 
0.003977 
-0.000035 
0.003010 
0.003654 
-0.001166 


final_layer_weights[0] = 1.037141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.893709


Output Embedding: 3.622467, 3.180903 

 Expected Embedding: 44.423141, -6.847703 
 loss: 882.633996 


Updated weights for final layer:
final_layer_weights[0] = 1.047141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.903709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.046191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.131305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[-24.403422, 26.005611]
[-0.481132, -0.013648]
[-16.791312, 16.491172]
[-3.929424, 3.664190]
[-6.115148, 5.959049]
[-16.080732, 16.758053]
[-14.127494, 15.491153]
[-17.897954, 17.511895]
[4.390170, -4.654217]
[-15.121837, 17.424299]
Final Q Matrix:
[25.466048, -25.865894]
[0.588303, -0.820729]
[17.752217, -18.625655]
[4.186231, -4.473812]
[6.472760, -6.810814]
[16.842943, -17.267865]
[14.671225, -14.716655]
[18.933022, -19.892239]
[-4.585298, 4.667548]
[15.565780, -15.254869]
Final V Matrix:
[-25.551895, 24.922154]
[-0.077059, 0.788022]
[-16.444517, 17.938724]
[-3.690189, 4.307836]
[-5.950916, 6.559391]
[-16.530723, 16.635846]
[-15.145922, 14.181992]
[-17.474686, 19.158264]
[4.577173, -4.497122]
[-16.895205, 14.705152]
 

Self-Attention Matrix: 
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-0.077059 0.788022  
-25.551895 24.922154  
-0.077059 0.788022  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.027085 -33.204260 
-7.481065 7.308774 
-28.671517 -3.440168 
-9.409564 2.462481 
-11.124483 -0.112569 
-14.395056 -16.441203 
-2.246148 -24.848727 
-31.451825 -2.815272 
-22.714709 30.706932 
9.016138 -38.167304 




semi_final_layer_weights[0] = -0.046191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.131305


 Semi Final Layer Node Values: 
2.227854 
-0.000044 
0.166900 
-0.000412 
0.078807 
-0.002055 
0.141807 
-0.001931 
-0.000560 
-0.001744 
.
.
.
-0.000787 
0.005520 
-0.000052 
-0.000050 
0.006416 
-0.000050 
-0.000051 
0.005392 
-0.000059 
0.006611 
0.005798 
0.136637 


final_layer_weights[0] = 1.047141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.903709


Output Embedding: -0.214550, 0.114669 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1064.975585 


Updated weights for final layer:
final_layer_weights[0] = 1.057141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.913709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.036191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[510] = -0.004428
semi_final_layer_weights[511] = 0.121305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.506449
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.437503

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.426333
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.493553

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.487854 0.506449 
-0.492268 0.437503 

Q Matrix:
0.457000 -0.426333 
0.473669 -0.474492 

V Matrix:
-0.506085 0.447537 
-0.451000 0.493553 
Final K Matrix:
[-13.934147, 12.423797]
[-3.654047, 2.755766]
[5.395127, -5.604867]
[18.708440, -16.548005]
[-25.469705, 24.318590]
[-20.154855, 20.230663]
[-16.029894, 14.570110]
[5.673406, -6.961997]
[4.941053, -5.199236]
[27.168539, -27.165847]
Final Q Matrix:
[13.400909, -13.406988]
[3.599831, -3.818405]
[-5.053211, 4.712290]
[-18.015102, 18.080550]
[24.220611, -23.536299]
[18.998196, -18.030372]
[15.369107, -15.256109]
[-5.131772, 4.311824]
[-4.616646, 4.275861]
[-25.627254, 24.367951]
Final V Matrix:
[-12.798337, 13.947801]
[-2.948683, 3.944275]
[5.600066, -4.946920]
[17.075876, -18.802451]
[-24.699686, 24.575952]
[-20.346176, 18.884416]
[-14.948612, 15.887082]
[6.755556, -4.592479]
[5.182375, -4.492844]
[27.341334, -25.515862]
 

Self-Attention Matrix: 
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-2.948683 3.944275  
-2.948683 3.944275  
-2.948683 3.944275  
-12.798337 13.947801  
-12.798337 13.947801  
-12.798337 13.947801  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.402155 31.708681 
-9.697419 18.055394 
-23.913586 14.003659 
-11.712531 -25.132876 
20.139443 32.802677 
28.862673 13.360986 
1.491417 32.107348 
-39.143956 28.532188 
-23.885138 14.897878 
-54.240410 -0.172233 




semi_final_layer_weights[0] = -0.036191
semi_final_layer_weights[1] = 0.004625
semi_final_layer_weights[2] = 0.004959
semi_final_layer_weights[3] = -0.003069
semi_final_layer_weights[4] = 0.003560
semi_final_layer_weights[5] = -0.003545
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = 0.003775
semi_final_layer_weights[9] = -0.004216
.
.
.
semi_final_layer_weights[511] = -0.004428
semi_final_layer_weights[512] = 0.121305


 Semi Final Layer Node Values: 
-0.010968 
0.043283 
-0.000442 
0.116148 
0.192033 
-0.001532 
0.171353 
0.043495 
-0.000302 
0.233633 
.
.
.
-0.001359 
-0.000097 
0.010294 
0.010671 
-0.000079 
0.010738 
0.010666 
-0.000100 
0.008882 
-0.000076 
-0.000092 
0.251408 


final_layer_weights[0] = 1.057141
final_layer_weights[1] = 0.875401
final_layer_weights[2] = -0.367683
final_layer_weights[3] = 0.547177
final_layer_weights[4] = 1.011029
final_layer_weights[5] = 0.420055
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 1.062428
final_layer_weights[9] = 1.410991
.
.
.
final_layer_weights[1022] = 0.276331
final_layer_weights[1023] = -0.913709


Output Embedding: -0.072539, -0.040692 

 Expected Embedding: 1.038754, 36.179375 
 loss: 656.564097 


Updated weights for final layer:
final_layer_weights[0] = 1.067141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.923709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.026191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[510] = 0.005572
semi_final_layer_weights[511] = 0.111305
Updating K Matrix:
k_matrix[0][0] = 0.512146
k_matrix[0][1] = -0.493551
k_matrix[1][0] = 0.507732
k_matrix[1][1] = -0.562497

Updating Q Matrix:
q_matrix[0][0] = -0.543000
q_matrix[0][1] = 0.573667
q_matrix[1][0] = -0.526331
q_matrix[1][1] = 0.525508

Updating V Matrix:
v_matrix[0][0] = 0.493915
v_matrix[0][1] = -0.552463
v_matrix[1][0] = 0.549000
v_matrix[1][1] = -0.506447

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.512146 -0.493551 
0.507732 -0.562497 

Q Matrix:
-0.543000 0.573667 
-0.526331 0.525508 

V Matrix:
0.493915 -0.552463 
0.549000 -0.506447 
Final K Matrix:
[23.725861, -24.874643]
[21.630015, -22.309702]
[19.842088, -18.880822]
[-19.286431, 21.446866]
[0.000000, 0.000000]
[9.664691, -9.947738]
[-6.799926, 6.660970]
[-20.488329, 22.678852]
[6.994107, -8.145371]
[-24.295332, 25.807444]
Final Q Matrix:
[-24.825964, 25.389150]
[-22.693157, 23.363413]
[-21.076939, 22.367836]
[19.979770, -19.914321]
[0.000000, 0.000000]
[-10.143105, 10.451397]
[7.191913, -7.553053]
[21.242000, -21.217071]
[-7.185297, 7.004666]
[25.366843, -25.800317]
Final V Matrix:
[24.510942, -24.460659]
[22.047727, -22.507095]
[18.940532, -21.539850]
[-20.918996, 19.192420]
[0.000000, 0.000000]
[9.834592, -10.068225]
[-6.645372, 7.274402]
[-22.137879, 20.447382]
[7.884310, -6.752725]
[-25.371447, 24.858534]
 

Self-Attention Matrix: 
22.047727 -22.507095  
22.047727 -22.507095  
22.047727 -22.507095  
24.510942 -24.460659  
23.279334 -23.483877  
22.047727 -22.507095  
24.510942 -24.460659  
24.510942 -24.460659  
22.047727 -22.507095  
24.510942 -24.460659  


Context Matrix (embedding_matrix + self_attention_matrix:
41.148728 4.954953 
44.440308 -2.493093 
64.052968 -25.797624 
25.596747 -63.541336 
23.279334 -23.483877 
32.332829 -13.846582 
12.695341 -25.935096 
24.248469 -64.548527 
16.672674 -3.310134 
9.498845 -57.168751 




semi_final_layer_weights[0] = -0.026191
semi_final_layer_weights[1] = -0.005375
semi_final_layer_weights[2] = -0.005041
semi_final_layer_weights[3] = 0.006931
semi_final_layer_weights[4] = -0.006440
semi_final_layer_weights[5] = 0.006455
semi_final_layer_weights[6] = -0.005047
semi_final_layer_weights[7] = 0.005475
semi_final_layer_weights[8] = -0.006225
semi_final_layer_weights[9] = 0.005784
.
.
.
semi_final_layer_weights[511] = 0.005572
semi_final_layer_weights[512] = 0.111305


 Semi Final Layer Node Values: 
-0.012337 
-0.002201 
-0.001878 
-0.002699 
0.007757 
0.112881 
0.071874 
-0.002261 
-0.000770 
-0.002815 
.
.
.
0.066951 
-0.000064 
0.006062 
0.005843 
-0.000074 
0.005805 
0.005846 
-0.000062 
0.006883 
-0.000077 
-0.000067 
-0.001341 


final_layer_weights[0] = 1.067141
final_layer_weights[1] = 0.865401
final_layer_weights[2] = -0.377683
final_layer_weights[3] = 0.557177
final_layer_weights[4] = 1.001029
final_layer_weights[5] = 0.430055
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 1.052428
final_layer_weights[9] = 1.420991
.
.
.
final_layer_weights[1022] = 0.286331
final_layer_weights[1023] = -0.923709


Output Embedding: 7.588750, 4.164357 

 Expected Embedding: -1.493413, -5.218942 
 loss: 85.265995 


Updated weights for final layer:
final_layer_weights[0] = 1.077141
final_layer_weights[1] = 0.869984
final_layer_weights[2] = -0.373385
final_layer_weights[3] = 0.551267
final_layer_weights[4] = 1.006520
final_layer_weights[5] = 0.424551
final_layer_weights[6] = 0.192508
final_layer_weights[7] = 0.830618
final_layer_weights[8] = 1.057736
final_layer_weights[9] = 1.416059
.
.
.
final_layer_weights[1022] = 0.281580
final_layer_weights[1023] = -0.933709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.016191
semi_final_layer_weights[1] = -0.000792
semi_final_layer_weights[2] = -0.000743
semi_final_layer_weights[3] = 0.001021
semi_final_layer_weights[4] = -0.000949
semi_final_layer_weights[5] = 0.000951
semi_final_layer_weights[6] = -0.000744
semi_final_layer_weights[7] = 0.000807
semi_final_layer_weights[8] = -0.000917
semi_final_layer_weights[9] = 0.000852
.
.
.
semi_final_layer_weights[510] = 0.000821
semi_final_layer_weights[511] = 0.101305
Updating K Matrix:
k_matrix[0][0] = 0.075460
k_matrix[0][1] = -0.072720
k_matrix[1][0] = 0.074809
k_matrix[1][1] = -0.082878

Updating Q Matrix:
q_matrix[0][0] = -0.080006
q_matrix[0][1] = 0.084524
q_matrix[1][0] = -0.077550
q_matrix[1][1] = 0.077428

Updating V Matrix:
v_matrix[0][0] = 0.072774
v_matrix[0][1] = -0.081400
v_matrix[1][0] = 0.080890
v_matrix[1][1] = -0.074620

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
0.075460 -0.072720 
0.074809 -0.082878 

Q Matrix:
-0.080006 0.084524 
-0.077550 0.077428 

V Matrix:
0.072774 -0.081400 
0.080890 -0.074620 
Final K Matrix:
[0.519367, -0.654021]
[2.724367, -3.108360]
[-1.250548, 1.022336]
[-2.841664, 3.159982]
[-2.327051, 2.591558]
[-0.259717, 0.226887]
[-2.355018, 2.417256]
[0.000000, 0.000000]
[1.573704, -1.897149]
[1.349235, -1.400021]
Final Q Matrix:
[-0.525511, 0.491125]
[-2.809397, 2.766531]
[1.355832, -1.508698]
[2.943820, -2.934177]
[2.410079, -2.400546]
[0.279197, -0.304731]
[2.472697, -2.550692]
[0.000000, 0.000000]
[-1.606173, 1.538055]
[-1.414178, 1.452409]
Final V Matrix:
[0.625328, -0.473734]
[3.018873, -2.666668]
[-1.057835, 1.452024]
[-3.082206, 2.827812]
[-2.527140, 2.313546]
[-0.231502, 0.293351]
[-2.390960, 2.457143]
[0.000000, 0.000000]
[1.826216, -1.483092]
[1.382091, -1.399218]
 

Self-Attention Matrix: 
0.853970 -0.683213  
0.625334 -0.473740  
3.017103 -2.665046  
3.018870 -2.666665  
3.018839 -2.666637  
2.564024 -2.249942  
3.018857 -2.666654  
1.822100 -1.570201  
0.627328 -0.475566  
0.627958 -0.476144  


Context Matrix (embedding_matrix + self_attention_matrix:
-6.374192 13.550324 
-7.660268 44.301373 
-30.359240 14.284933 
4.104676 -41.747342 
4.260499 -35.025524 
-3.028800 -0.080225 
-14.609633 -16.365221 
1.822100 -1.570201 
-13.501237 34.812012 
9.337058 8.774717 




semi_final_layer_weights[0] = -0.016191
semi_final_layer_weights[1] = -0.000792
semi_final_layer_weights[2] = -0.000743
semi_final_layer_weights[3] = 0.001021
semi_final_layer_weights[4] = -0.000949
semi_final_layer_weights[5] = 0.000951
semi_final_layer_weights[6] = -0.000744
semi_final_layer_weights[7] = 0.000807
semi_final_layer_weights[8] = -0.000917
semi_final_layer_weights[9] = 0.000852
.
.
.
semi_final_layer_weights[511] = 0.000821
semi_final_layer_weights[512] = 0.101305


 Semi Final Layer Node Values: 
-0.001324 
-0.000298 
0.011195 
-0.000395 
0.030141 
-0.000039 
0.023779 
-0.000006 
-0.000205 
0.016287 
.
.
.
0.034100 
-0.000006 
0.000555 
0.000535 
-0.000007 
0.000531 
0.000535 
-0.000006 
0.000630 
-0.000007 
-0.000006 
-0.000758 


final_layer_weights[0] = 1.077141
final_layer_weights[1] = 0.869984
final_layer_weights[2] = -0.373385
final_layer_weights[3] = 0.551267
final_layer_weights[4] = 1.006520
final_layer_weights[5] = 0.424551
final_layer_weights[6] = 0.192508
final_layer_weights[7] = 0.830618
final_layer_weights[8] = 1.057736
final_layer_weights[9] = 1.416059
.
.
.
final_layer_weights[1022] = 0.281580
final_layer_weights[1023] = -0.933709


Output Embedding: 4.511404, 2.305603 

 Expected Embedding: -37.748848, 10.192697 
 loss: 924.067556 


Updated weights for final layer:
final_layer_weights[0] = 1.087141
final_layer_weights[1] = 0.877302
final_layer_weights[2] = -0.366522
final_layer_weights[3] = 0.541831
final_layer_weights[4] = 1.015288
final_layer_weights[5] = 0.415762
final_layer_weights[6] = 0.199381
final_layer_weights[7] = 0.823164
final_layer_weights[8] = 1.066211
final_layer_weights[9] = 1.408184
.
.
.
final_layer_weights[1022] = 0.273994
final_layer_weights[1023] = -0.943709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.006191
semi_final_layer_weights[1] = 0.006526
semi_final_layer_weights[2] = 0.006120
semi_final_layer_weights[3] = -0.008415
semi_final_layer_weights[4] = 0.007819
semi_final_layer_weights[5] = -0.007838
semi_final_layer_weights[6] = 0.006129
semi_final_layer_weights[7] = -0.006647
semi_final_layer_weights[8] = 0.007558
semi_final_layer_weights[9] = -0.007023
.
.
.
semi_final_layer_weights[510] = -0.006765
semi_final_layer_weights[511] = 0.091305
Updating K Matrix:
k_matrix[0][0] = -0.621838
k_matrix[0][1] = 0.599261
k_matrix[1][0] = -0.616479
k_matrix[1][1] = 0.682974

Updating Q Matrix:
q_matrix[0][0] = 0.659301
q_matrix[0][1] = -0.696536
q_matrix[1][0] = 0.639061
q_matrix[1][1] = -0.638062

Updating V Matrix:
v_matrix[0][0] = -0.599703
v_matrix[0][1] = 0.670790
v_matrix[1][0] = -0.666586
v_matrix[1][1] = 0.614918

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.621838 0.599261 
-0.616479 0.682974 

Q Matrix:
0.659301 -0.696536 
0.639061 -0.638062 

V Matrix:
-0.599703 0.670790 
-0.666586 0.614918 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.006191
semi_final_layer_weights[1] = 0.006526
semi_final_layer_weights[2] = 0.006120
semi_final_layer_weights[3] = -0.008415
semi_final_layer_weights[4] = 0.007819
semi_final_layer_weights[5] = -0.007838
semi_final_layer_weights[6] = 0.006129
semi_final_layer_weights[7] = -0.006647
semi_final_layer_weights[8] = 0.007558
semi_final_layer_weights[9] = -0.007023
.
.
.
semi_final_layer_weights[511] = -0.006765
semi_final_layer_weights[512] = 0.091305


 Semi Final Layer Node Values: 
-0.000062 
0.006526 
0.006120 
-0.000084 
0.007819 
-0.000078 
0.006129 
-0.000066 
0.007558 
-0.000070 
.
.
.
-0.000356 
-0.000064 
0.006111 
0.005890 
-0.000075 
0.005851 
0.005893 
-0.000063 
0.006939 
-0.000077 
-0.000068 
0.091305 


final_layer_weights[0] = 1.087141
final_layer_weights[1] = 0.877302
final_layer_weights[2] = -0.366522
final_layer_weights[3] = 0.541831
final_layer_weights[4] = 1.015288
final_layer_weights[5] = 0.415762
final_layer_weights[6] = 0.199381
final_layer_weights[7] = 0.823164
final_layer_weights[8] = 1.066211
final_layer_weights[9] = 1.408184
.
.
.
final_layer_weights[1022] = 0.273994
final_layer_weights[1023] = -0.943709


Output Embedding: -0.277581, -0.274566 

 Expected Embedding: -10.885829, -17.296432 
 loss: 201.139439 


Updated weights for final layer:
final_layer_weights[0] = 1.097141
final_layer_weights[1] = 0.867302
final_layer_weights[2] = -0.376522
final_layer_weights[3] = 0.551831
final_layer_weights[4] = 1.005288
final_layer_weights[5] = 0.425762
final_layer_weights[6] = 0.189381
final_layer_weights[7] = 0.833164
final_layer_weights[8] = 1.056211
final_layer_weights[9] = 1.418184
.
.
.
final_layer_weights[1022] = 0.283994
final_layer_weights[1023] = -0.953709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.003809
semi_final_layer_weights[1] = -0.003474
semi_final_layer_weights[2] = -0.003880
semi_final_layer_weights[3] = 0.001585
semi_final_layer_weights[4] = -0.002181
semi_final_layer_weights[5] = 0.002162
semi_final_layer_weights[6] = -0.003871
semi_final_layer_weights[7] = 0.003353
semi_final_layer_weights[8] = -0.002442
semi_final_layer_weights[9] = 0.002977
.
.
.
semi_final_layer_weights[510] = 0.003235
semi_final_layer_weights[511] = 0.081305
Updating K Matrix:
k_matrix[0][0] = 0.378162
k_matrix[0][1] = -0.400739
k_matrix[1][0] = 0.383521
k_matrix[1][1] = -0.317026

Updating Q Matrix:
q_matrix[0][0] = -0.340699
q_matrix[0][1] = 0.303464
q_matrix[1][0] = -0.360939
q_matrix[1][1] = 0.361938

Updating V Matrix:
v_matrix[0][0] = 0.400297
v_matrix[0][1] = -0.329210
v_matrix[1][0] = 0.333414
v_matrix[1][1] = -0.385082

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 8  total loss: 5264.677437 ******************************************************************* 

Epoch: 9
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.378162 -0.400739 
0.383521 -0.317026 

Q Matrix:
-0.340699 0.303464 
-0.360939 0.361938 

V Matrix:
0.400297 -0.329210 
0.333414 -0.385082 
Final K Matrix:
[9.020159, -9.979398]
[5.489448, -6.848922]
[14.205692, -12.328927]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-8.054033, 6.983429]
[-4.767749, 3.779841]
[-13.268186, 13.050992]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[9.889611, -7.611512]
[6.648180, -4.187827]
[12.825531, -13.927701]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
6.648180 -4.187827  
6.648180 -4.187827  
6.648180 -4.187827  
8.268895 -5.899670  
8.268895 -5.899670  
8.268895 -5.899670  
8.268895 -5.899670  
8.268895 -5.899670  
8.268895 -5.899670  
8.268895 -5.899670  


Context Matrix (embedding_matrix + self_attention_matrix:
35.273992 -8.894341 
32.869743 -15.729732 
13.298819 26.294663 
8.268895 -5.899670 
8.268895 -5.899670 
8.268895 -5.899670 
8.268895 -5.899670 
8.268895 -5.899670 
8.268895 -5.899670 
8.268895 -5.899670 




semi_final_layer_weights[0] = 0.003809
semi_final_layer_weights[1] = -0.003474
semi_final_layer_weights[2] = -0.003880
semi_final_layer_weights[3] = 0.001585
semi_final_layer_weights[4] = -0.002181
semi_final_layer_weights[5] = 0.002162
semi_final_layer_weights[6] = -0.003871
semi_final_layer_weights[7] = 0.003353
semi_final_layer_weights[8] = -0.002442
semi_final_layer_weights[9] = 0.002977
.
.
.
semi_final_layer_weights[511] = 0.003235
semi_final_layer_weights[512] = 0.081305


 Semi Final Layer Node Values: 
0.096671 
-0.000561 
-0.001575 
0.002170 
-0.000030 
0.002960 
-0.000053 
0.004591 
-0.000033 
0.004077 
.
.
.
-0.000350 
0.004873 
-0.000053 
-0.000056 
0.003443 
-0.000057 
-0.000056 
0.005078 
-0.000042 
0.003130 
0.004429 
0.111325 


final_layer_weights[0] = 1.097141
final_layer_weights[1] = 0.867302
final_layer_weights[2] = -0.376522
final_layer_weights[3] = 0.551831
final_layer_weights[4] = 1.005288
final_layer_weights[5] = 0.425762
final_layer_weights[6] = 0.189381
final_layer_weights[7] = 0.833164
final_layer_weights[8] = 1.056211
final_layer_weights[9] = 1.418184
.
.
.
final_layer_weights[1022] = 0.283994
final_layer_weights[1023] = -0.953709


Output Embedding: -0.258476, -0.262945 

 Expected Embedding: 12.470385, -17.532656 
 loss: 230.133402 


Updated weights for final layer:
final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.548184
final_layer_weights[4] = 1.010307
final_layer_weights[5] = 0.420787
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.061831
final_layer_weights[9] = 1.411332
.
.
.
final_layer_weights[1022] = 0.276550
final_layer_weights[1023] = -0.963709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.002062
semi_final_layer_weights[4] = 0.002838
semi_final_layer_weights[5] = -0.002813
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.003178
semi_final_layer_weights[9] = -0.003875
.
.
.
semi_final_layer_weights[510] = -0.004209
semi_final_layer_weights[511] = 0.071305
Updating K Matrix:
k_matrix[0][0] = -0.492116
k_matrix[0][1] = 0.521496
k_matrix[1][0] = -0.499089
k_matrix[1][1] = 0.412557

Updating Q Matrix:
q_matrix[0][0] = 0.443364
q_matrix[0][1] = -0.394908
q_matrix[1][0] = 0.469702
q_matrix[1][1] = -0.471002

Updating V Matrix:
v_matrix[0][0] = -0.520920
v_matrix[0][1] = 0.428412
v_matrix[1][0] = -0.433883
v_matrix[1][1] = 0.501120

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.492116 0.521496 
-0.499089 0.412557 

Q Matrix:
0.443364 -0.394908 
0.469702 -0.471002 

V Matrix:
-0.520920 0.428412 
-0.433883 0.501120 
Final K Matrix:
[-11.750002, 12.157080]
[-7.160349, 6.046329]
[-18.486350, 16.044053]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.636739, -9.607442]
[6.716773, -6.680163]
[17.266342, -16.983699]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.198793, 10.397641]
[-6.328290, 7.116489]
[-16.690300, 18.124592]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-6.328290 7.116489  
-6.328290 7.116489  
-6.328290 7.116489  
-9.263542 8.757065  
-9.263542 8.757065  
-9.263542 8.757065  
-9.263542 8.757065  
-9.263542 8.757065  
-9.263542 8.757065  
-9.263542 8.757065  


Context Matrix (embedding_matrix + self_attention_matrix:
14.981432 9.647414 
-5.217303 20.367871 
0.322349 37.598979 
-9.263542 8.757065 
-9.263542 8.757065 
-9.263542 8.757065 
-9.263542 8.757065 
-9.263542 8.757065 
-9.263542 8.757065 
-9.263542 8.757065 




semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.002062
semi_final_layer_weights[4] = 0.002838
semi_final_layer_weights[5] = -0.002813
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.003178
semi_final_layer_weights[9] = -0.003875
.
.
.
semi_final_layer_weights[511] = -0.004209
semi_final_layer_weights[512] = 0.071305


 Semi Final Layer Node Values: 
-0.001270 
0.073017 
0.196515 
-0.000010 
0.001400 
-0.000014 
0.002486 
-0.000022 
0.001568 
-0.000019 
.
.
.
-0.000077 
-0.000023 
0.002498 
0.002640 
-0.000016 
0.002665 
0.002638 
-0.000024 
0.001966 
-0.000015 
-0.000021 
0.035191 


final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.548184
final_layer_weights[4] = 1.010307
final_layer_weights[5] = 0.420787
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.061831
final_layer_weights[9] = 1.411332
.
.
.
final_layer_weights[1022] = 0.276550
final_layer_weights[1023] = -0.963709


Output Embedding: -0.189962, -0.198354 

 Expected Embedding: 12.470385, -17.532656 
 loss: 230.381202 


Updated weights for final layer:
final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -0.973709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[510] = 0.005488
semi_final_layer_weights[511] = 0.061305
Updating K Matrix:
k_matrix[0][0] = 0.507884
k_matrix[0][1] = -0.478504
k_matrix[1][0] = 0.500911
k_matrix[1][1] = -0.537897

Updating Q Matrix:
q_matrix[0][0] = -0.556636
q_matrix[0][1] = 0.514885
q_matrix[1][0] = -0.530298
q_matrix[1][1] = 0.528998

Updating V Matrix:
v_matrix[0][0] = 0.479080
v_matrix[0][1] = -0.558569
v_matrix[1][0] = 0.565702
v_matrix[1][1] = -0.498880

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
0.507884 -0.478504 
0.500911 -0.537897 

Q Matrix:
-0.556636 0.514885 
-0.530298 0.528998 

V Matrix:
0.479080 -0.558569 
0.565702 -0.498880 
Final K Matrix:
[-3.504800, 3.169450]
[20.373373, -18.853255]
[-6.378759, 7.280086]
[5.948655, -5.017159]
[24.762468, -23.295051]
[22.646138, -22.976390]
[-12.253145, 12.202549]
[-21.952399, 21.097103]
[26.487595, -26.324064]
[-17.058461, 18.227728]
Final Q Matrix:
[3.878810, -3.510529]
[-22.425821, 20.544524]
[6.631006, -6.874616]
[-6.686148, 5.842033]
[-27.149324, 25.092576]
[-24.355032, 23.485033]
[13.242762, -12.633417]
[23.942096, -22.388137]
[-28.642198, 27.292237]
[18.084806, -17.985962]
Final V Matrix:
[-3.118664, 3.959138]
[18.735299, -22.675902]
[-7.811865, 6.013521]
[4.781354, -7.005524]
[23.308672, -27.261215]
[23.679367, -23.612550]
[-12.488231, 12.956860]
[-21.293177, 23.816177]
[26.919259, -28.051525]
[-19.137354, 17.060492]
 

Self-Attention Matrix: 
18.735299 -22.675902  
-3.118664 3.959138  
18.735299 -22.675902  
-3.118664 3.959138  
-3.118664 3.959138  
-3.118664 3.959138  
18.735299 -22.675902  
18.735299 -22.675902  
-3.118664 3.959138  
18.735299 -22.675902  


Context Matrix (embedding_matrix + self_attention_matrix:
9.851820 -20.665604 
42.102489 -1.218920 
25.169702 -41.934186 
17.376409 -4.945580 
46.160102 3.429200 
16.944980 28.826066 
4.451205 -32.654666 
-18.288718 -28.961399 
28.568988 24.709176 
17.385440 -55.362105 




semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[511] = 0.005488
semi_final_layer_weights[512] = 0.061305


 Semi Final Layer Node Values: 
-0.000596 
-0.002185 
0.087951 
0.030731 
-0.001872 
0.171562 
0.144904 
-0.002720 
-0.002249 
-0.001969 
.
.
.
0.014234 
-0.000137 
0.012595 
0.011862 
-0.000109 
0.011733 
0.011871 
-0.000132 
0.013246 
-0.000099 
-0.000140 
-0.001563 


final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -0.973709


Output Embedding: 2.627033, 0.346132 

 Expected Embedding: 19.465172, 25.668072 
 loss: 462.361761 


Updated weights for final layer:
final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -0.983709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[510] = -0.004512
semi_final_layer_weights[511] = 0.051305
Updating K Matrix:
k_matrix[0][0] = -0.492116
k_matrix[0][1] = 0.521496
k_matrix[1][0] = -0.499089
k_matrix[1][1] = 0.462103

Updating Q Matrix:
q_matrix[0][0] = 0.443364
q_matrix[0][1] = -0.485115
q_matrix[1][0] = 0.469702
q_matrix[1][1] = -0.471002

Updating V Matrix:
v_matrix[0][0] = -0.520920
v_matrix[0][1] = 0.441431
v_matrix[1][0] = -0.434298
v_matrix[1][1] = 0.501120

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
-0.492116 0.521496 
-0.499089 0.462103 

Q Matrix:
0.443364 -0.485115 
0.469702 -0.471002 

V Matrix:
-0.520920 0.441431 
-0.434298 0.501120 
Final K Matrix:
[-18.301679, 17.327931]
[-2.073706, 1.745185]
[-13.145794, 12.923145]
[-32.679476, 32.227952]
[-18.711282, 17.648797]
[-16.709987, 16.351826]
[4.707801, -5.029851]
[-11.822033, 11.618976]
[7.755217, -8.599323]
[-9.404660, 9.330149]
Final Q Matrix:
[17.109185, -17.391943]
[2.004114, -1.902059]
[12.146059, -12.642176]
[30.163588, -31.459563]
[17.512204, -17.760148]
[15.461764, -16.046210]
[-4.229110, 4.653708]
[10.923820, -11.368240]
[-6.872487, 7.764656]
[8.663978, -9.071007]
Final V Matrix:
[-16.464232, 18.070087]
[-1.558385, 2.222041]
[-12.497109, 12.597966]
[-31.210419, 31.236033]
[-16.738480, 18.528070]
[-15.779648, 16.073754]
[5.041053, -4.190136]
[-11.234687, 11.331635]
[8.745601, -6.651552]
[-9.059913, 8.944907]
 

Self-Attention Matrix: 
-1.558385 2.222041  
-1.558385 2.222041  
-1.558385 2.222041  
-1.558385 2.222041  
-1.558385 2.222041  
-1.558385 2.222041  
-16.464232 18.070087  
-1.558385 2.222041  
-16.464232 18.070087  
-1.558385 2.222041  


Context Matrix (embedding_matrix + self_attention_matrix:
4.250784 33.164230 
-4.213619 8.995162 
9.854714 17.307998 
28.362178 38.197814 
3.364171 34.859157 
11.807935 22.523469 
-26.653111 18.683816 
8.662460 15.831239 
-38.010735 23.776793 
7.894092 11.745297 




semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[511] = -0.004512
semi_final_layer_weights[512] = 0.051305


 Semi Final Layer Node Values: 
-0.001904 
0.026138 
0.142195 
-0.004940 
0.247111 
-0.002237 
-0.000351 
-0.001112 
-0.000775 
-0.001021 
.
.
.
0.009431 
-0.000099 
0.010804 
0.011417 
-0.000122 
0.011526 
0.011410 
-0.000103 
0.010259 
-0.000131 
-0.000096 
0.109524 


final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -0.983709


Output Embedding: -0.267209, -0.277090 

 Expected Embedding: 4.643339, -27.140186 
 loss: 372.869711 


Updated weights for final layer:
final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -0.993709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[510] = 0.005488
semi_final_layer_weights[511] = 0.041305
Updating K Matrix:
k_matrix[0][0] = 0.507884
k_matrix[0][1] = -0.478504
k_matrix[1][0] = 0.500911
k_matrix[1][1] = -0.537897

Updating Q Matrix:
q_matrix[0][0] = -0.556636
q_matrix[0][1] = 0.514885
q_matrix[1][0] = -0.530298
q_matrix[1][1] = 0.528998

Updating V Matrix:
v_matrix[0][0] = 0.479080
v_matrix[0][1] = -0.558569
v_matrix[1][0] = 0.565702
v_matrix[1][1] = -0.498880

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
0.507884 -0.478504 
0.500911 -0.537897 

Q Matrix:
-0.556636 0.514885 
-0.530298 0.528998 

V Matrix:
0.479080 -0.558569 
0.565702 -0.498880 
Final K Matrix:
[10.160051, -10.262389]
[-11.145835, 11.036105]
[-18.868520, 20.323901]
[17.952065, -19.534143]
[19.240897, -17.847707]
[-11.257668, 12.050441]
[12.231022, -12.417648]
[32.027577, -32.481461]
[22.653709, -24.383440]
[-5.045186, 4.661451]
Final Q Matrix:
[-10.939724, 10.521700]
[12.064078, -11.471285]
[19.957841, -19.946473]
[-18.932532, 19.041046]
[-21.167231, 19.416160]
[11.929015, -11.876551]
[-13.151643, 12.686755]
[-34.448136, 33.209762]
[-23.966549, 23.942253]
[5.555516, -5.085226]
Final V Matrix:
[10.558853, -10.629765]
[-11.269664, 11.836197]
[-21.396958, 18.742975]
[20.636588, -17.676955]
[17.753828, -21.381976]
[-12.659466, 11.242353]
[12.800731, -12.746461]
[33.470214, -33.404728]
[25.664490, -22.516868]
[-4.629227, 5.621132]
 

Self-Attention Matrix: 
-11.269664 11.836197  
10.558853 -10.629765  
10.558853 -10.629765  
-11.269664 11.836197  
-11.269664 11.836197  
10.558853 -10.629765  
-11.269664 11.836197  
-11.269664 11.836197  
-11.269664 11.836197  
10.558853 -10.629765  


Context Matrix (embedding_matrix + self_attention_matrix:
-1.583033 22.297854 
-3.386936 -18.740954 
11.488634 -49.240876 
-15.105600 51.564343 
30.803403 7.589219 
9.983797 -32.521079 
-0.556916 25.391859 
17.302621 46.804779 
-12.122643 57.926041 
-0.748677 -9.236840 




semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[511] = 0.005488
semi_final_layer_weights[512] = 0.041305


 Semi Final Layer Node Values: 
0.109513 
0.126717 
0.191861 
0.100705 
-0.001457 
-0.000863 
-0.001282 
0.367017 
-0.001939 
-0.000555 
.
.
.
-0.000070 
0.006699 
-0.000062 
-0.000058 
0.005323 
-0.000057 
-0.000058 
0.006456 
-0.000065 
0.004840 
0.006848 
0.051541 


final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -0.993709


Output Embedding: -0.096222, -0.091577 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1013.809495 


Updated weights for final layer:
final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -1.003709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[510] = -0.004512
semi_final_layer_weights[511] = 0.031305
Updating K Matrix:
k_matrix[0][0] = -0.492116
k_matrix[0][1] = 0.521496
k_matrix[1][0] = -0.499089
k_matrix[1][1] = 0.462103

Updating Q Matrix:
q_matrix[0][0] = 0.443364
q_matrix[0][1] = -0.485115
q_matrix[1][0] = 0.469702
q_matrix[1][1] = -0.471002

Updating V Matrix:
v_matrix[0][0] = -0.520920
v_matrix[0][1] = 0.441431
v_matrix[1][0] = -0.434298
v_matrix[1][1] = 0.501120

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
-0.492116 0.521496 
-0.499089 0.462103 

Q Matrix:
0.443364 -0.485115 
0.469702 -0.471002 

V Matrix:
-0.520920 0.441431 
-0.434298 0.501120 
Final K Matrix:
[23.830189, -22.982810]
[0.389193, -0.847899]
[16.182019, -16.865745]
[3.756967, -4.093089]
[5.886084, -6.177349]
[15.645021, -15.428448]
[13.862454, -12.977986]
[17.238374, -18.026897]
[-4.283340, 4.152743]
[14.967259, -13.259306]
Final Q Matrix:
[-22.151182, 22.777793]
[-0.219857, 0.520506]
[-14.663735, 15.863078]
[-3.351198, 3.738662]
[-5.321037, 5.783448]
[-14.440684, 15.060869]
[-13.003330, 13.127215]
[-15.602908, 16.917519]
[3.975027, -4.101002]
[-14.265804, 13.936789]
Final V Matrix:
[22.029618, -23.192177]
[1.024950, -0.000681]
[16.731725, -14.741310]
[4.134277, -3.280553]
[6.145951, -5.327980]
[14.941146, -14.954312]
[12.263901, -13.804583]
[17.908653, -15.655477]
[-3.990263, 4.151288]
[12.181374, -15.507262]
 

Self-Attention Matrix: 
1.024950 -0.000681  
1.026305 -0.002177  
1.024950 -0.000681  
1.024950 -0.000681  
1.024950 -0.000681  
1.024950 -0.000681  
1.024950 -0.000681  
1.024950 -0.000681  
22.029618 -23.192177  
1.024950 -0.000681  


Context Matrix (embedding_matrix + self_attention_matrix:
-12.925076 -33.992964 
-6.377700 6.518574 
-27.569508 -4.228871 
-8.307555 1.673777 
-10.022474 -0.901272 
-13.293047 -17.229906 
-1.144139 -25.637430 
-30.349816 -3.603976 
24.866803 -17.407399 
10.118147 -38.956007 




semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[511] = -0.004512
semi_final_layer_weights[512] = 0.031305


 Semi Final Layer Node Values: 
0.237519 
0.005158 
-0.001656 
0.041192 
-0.000751 
0.199599 
-0.001400 
0.152500 
0.037829 
0.147643 
.
.
.
-0.000047 
0.004951 
-0.000054 
-0.000057 
0.006130 
-0.000058 
-0.000057 
0.005160 
-0.000051 
0.006544 
0.004824 
-0.000335 


final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -1.003709


Output Embedding: 0.711206, 0.707053 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1017.403576 


Updated weights for final layer:
final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -1.013709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[510] = 0.005488
semi_final_layer_weights[511] = 0.021305
Updating K Matrix:
k_matrix[0][0] = 0.507884
k_matrix[0][1] = -0.478504
k_matrix[1][0] = 0.500911
k_matrix[1][1] = -0.537897

Updating Q Matrix:
q_matrix[0][0] = -0.556636
q_matrix[0][1] = 0.514885
q_matrix[1][0] = -0.530298
q_matrix[1][1] = 0.528998

Updating V Matrix:
v_matrix[0][0] = 0.479080
v_matrix[0][1] = -0.558569
v_matrix[1][0] = 0.565702
v_matrix[1][1] = -0.498880

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
0.507884 -0.478504 
0.500911 -0.537897 

Q Matrix:
-0.556636 0.514885 
-0.530298 0.528998 

V Matrix:
0.479080 -0.558569 
0.565702 -0.498880 
Final K Matrix:
[14.185078, -15.195910]
[3.640841, -4.361031]
[-5.617283, 5.288649]
[-19.024489, 20.501821]
[26.181600, -26.570619]
[20.873430, -20.287093]
[16.362259, -17.273442]
[-6.075046, 4.761593]
[-5.154910, 4.794039]
[-28.120670, 27.425337]
Final Q Matrix:
[-15.027624, 14.968717]
[-3.726505, 3.989930]
[6.157530, -5.693530]
[20.120003, -20.114538]
[-28.155238, 27.153776]
[-22.701016, 21.360621]
[-17.406339, 17.184353]
[6.930857, -5.849859]
[5.667492, -5.205842]
[30.555986, -28.807385]
Final V Matrix:
[15.968222, -14.156389]
[4.749511, -3.270124]
[-5.293492, 6.180767]
[-21.587845, 18.890083]
[27.386321, -27.293199]
[20.567232, -22.466648]
[18.059079, -16.530104]
[-4.371230, 7.439982]
[-4.774001, 5.718768]
[-27.841795, 30.192463]
 

Self-Attention Matrix: 
4.749511 -3.270124  
4.749511 -3.270124  
15.968222 -14.156389  
15.968222 -14.156389  
4.749511 -3.270124  
4.749511 -3.270124  
4.749511 -3.270124  
15.968222 -14.156389  
15.968222 -14.156389  
15.968222 -14.156389  


Context Matrix (embedding_matrix + self_attention_matrix:
5.296039 24.494282 
-1.999226 10.840995 
4.852972 -14.100531 
17.054028 -53.237066 
27.837636 25.588278 
36.560867 6.146586 
9.189610 24.892948 
-10.377397 0.427998 
4.881420 -13.206312 
-25.473852 -28.276423 




semi_final_layer_weights[0] = 0.005043
semi_final_layer_weights[1] = -0.005479
semi_final_layer_weights[2] = -0.004951
semi_final_layer_weights[3] = 0.002688
semi_final_layer_weights[4] = -0.003700
semi_final_layer_weights[5] = 0.003668
semi_final_layer_weights[6] = -0.004962
semi_final_layer_weights[7] = 0.005637
semi_final_layer_weights[8] = -0.004144
semi_final_layer_weights[9] = 0.005052
.
.
.
semi_final_layer_weights[511] = 0.005488
semi_final_layer_weights[512] = 0.021305


 Semi Final Layer Node Values: 
0.155282 
-0.000539 
0.050735 
-0.001000 
-0.002014 
0.160325 
-0.001741 
-0.000504 
0.038639 
-0.002766 
.
.
.
-0.000036 
0.003466 
-0.000032 
-0.000030 
0.002754 
-0.000030 
-0.000030 
0.003340 
-0.000034 
0.002504 
0.003543 
0.013755 


final_layer_weights[0] = 1.098375
final_layer_weights[1] = 0.865297
final_layer_weights[2] = -0.377593
final_layer_weights[3] = 0.552934
final_layer_weights[4] = 1.003769
final_layer_weights[5] = 0.427268
final_layer_weights[6] = 0.188290
final_layer_weights[7] = 0.835448
final_layer_weights[8] = 1.054509
final_layer_weights[9] = 1.420259
.
.
.
final_layer_weights[1022] = 0.286247
final_layer_weights[1023] = -1.013709


Output Embedding: -0.083569, 0.163711 

 Expected Embedding: 1.038754, 36.179375 
 loss: 649.193835 


Updated weights for final layer:
final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -1.023709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[510] = -0.004512
semi_final_layer_weights[511] = 0.011305
Updating K Matrix:
k_matrix[0][0] = -0.492116
k_matrix[0][1] = 0.521496
k_matrix[1][0] = -0.499089
k_matrix[1][1] = 0.462103

Updating Q Matrix:
q_matrix[0][0] = 0.443364
q_matrix[0][1] = -0.485115
q_matrix[1][0] = 0.469702
q_matrix[1][1] = -0.471002

Updating V Matrix:
v_matrix[0][0] = -0.520920
v_matrix[0][1] = 0.441431
v_matrix[1][0] = -0.434298
v_matrix[1][1] = 0.501120

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
-0.492116 0.521496 
-0.499089 0.462103 

Q Matrix:
0.443364 -0.485115 
0.469702 -0.471002 

V Matrix:
-0.520920 0.441431 
-0.434298 0.501120 
Final K Matrix:
[-23.105898, 22.651381]
[-21.008500, 20.926163]
[-19.029167, 20.384989]
[18.970382, -17.493050]
[0.000000, 0.000000]
[-9.383823, 9.365685]
[6.550516, -6.843126]
[20.136569, -18.661597]
[-6.935839, 6.067904]
[23.711925, -22.943247]
Final Q Matrix:
[21.367673, -22.200854]
[19.328677, -20.289604]
[17.078033, -18.827515]
[-17.874869, 17.880333]
[0.000000, 0.000000]
[8.627903, -9.068573]
[-5.931155, 6.426384]
[-18.945726, 19.008792]
[6.633749, -6.434288]
[-22.018878, 22.688163]
Final V Matrix:
[-21.876800, 22.193548]
[-20.356783, 19.914191]
[-20.452312, 16.893468]
[16.407026, -19.104789]
[0.000000, 0.000000]
[-9.118958, 8.880117]
[6.795330, -5.954642]
[17.546792, -20.204684]
[-5.537220, 7.247260]
[22.025151, -23.017474]
 

Self-Attention Matrix: 
-20.356783 19.914191  
-20.356783 19.914191  
-21.876330 22.192843  
-21.876800 22.193548  
-21.116791 21.053869  
-20.356783 19.914192  
-21.679935 21.898337  
-21.876800 22.193548  
-20.356783 19.914191  
-21.876800 22.193548  


Context Matrix (embedding_matrix + self_attention_matrix:
-1.255781 47.376239 
2.035799 39.928193 
20.128911 18.902314 
-20.790994 -16.887129 
-21.116791 21.053869 
-10.071680 28.574705 
-33.495536 20.423900 
-22.139273 -17.894320 
-25.731835 39.111152 
-36.888896 -10.514544 




semi_final_layer_weights[0] = -0.004957
semi_final_layer_weights[1] = 0.004521
semi_final_layer_weights[2] = 0.005049
semi_final_layer_weights[3] = -0.007312
semi_final_layer_weights[4] = 0.006300
semi_final_layer_weights[5] = -0.006332
semi_final_layer_weights[6] = 0.005038
semi_final_layer_weights[7] = -0.004363
semi_final_layer_weights[8] = 0.005856
semi_final_layer_weights[9] = -0.004948
.
.
.
semi_final_layer_weights[511] = -0.004512
semi_final_layer_weights[512] = 0.011305


 Semi Final Layer Node Values: 
-0.002336 
0.194241 
0.202119 
0.282798 
0.005904 
-0.001235 
-0.000608 
0.179026 
0.084211 
0.239508 
.
.
.
0.004140 
-0.000043 
0.004742 
0.005012 
-0.000054 
0.005059 
0.005008 
-0.000045 
0.004503 
-0.000057 
-0.000042 
0.010594 


final_layer_weights[0] = 1.088375
final_layer_weights[1] = 0.875297
final_layer_weights[2] = -0.367593
final_layer_weights[3] = 0.542934
final_layer_weights[4] = 1.013769
final_layer_weights[5] = 0.417268
final_layer_weights[6] = 0.198290
final_layer_weights[7] = 0.825448
final_layer_weights[8] = 1.064509
final_layer_weights[9] = 1.410259
.
.
.
final_layer_weights[1022] = 0.276247
final_layer_weights[1023] = -1.023709


Output Embedding: 0.313738, 0.402802 

 Expected Embedding: -1.493413, -5.218942 
 loss: 17.434900 


Updated weights for final layer:
final_layer_weights[0] = 1.089239
final_layer_weights[1] = 0.874509
final_layer_weights[2] = -0.368473
final_layer_weights[3] = 0.544209
final_layer_weights[4] = 1.012671
final_layer_weights[5] = 0.418372
final_layer_weights[6] = 0.197412
final_layer_weights[7] = 0.826209
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.411122
.
.
.
final_layer_weights[1022] = 0.277034
final_layer_weights[1023] = -1.025680


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004093
semi_final_layer_weights[1] = 0.003733
semi_final_layer_weights[2] = 0.004169
semi_final_layer_weights[3] = -0.006037
semi_final_layer_weights[4] = 0.005202
semi_final_layer_weights[5] = -0.005228
semi_final_layer_weights[6] = 0.004160
semi_final_layer_weights[7] = -0.003602
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004085
.
.
.
semi_final_layer_weights[510] = -0.003725
semi_final_layer_weights[511] = 0.009334
Updating K Matrix:
k_matrix[0][0] = -0.406316
k_matrix[0][1] = 0.430573
k_matrix[1][0] = -0.412073
k_matrix[1][1] = 0.381536

Updating Q Matrix:
q_matrix[0][0] = 0.366064
q_matrix[0][1] = -0.400535
q_matrix[1][0] = 0.387810
q_matrix[1][1] = -0.388883

Updating V Matrix:
v_matrix[0][0] = -0.430098
v_matrix[0][1] = 0.364468
v_matrix[1][0] = -0.358578
v_matrix[1][1] = 0.413750

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.406316 0.430573 
-0.412073 0.381536 

Q Matrix:
0.366064 -0.400535 
0.387810 -0.388883 

V Matrix:
-0.430098 0.364468 
-0.358578 0.413750 
Final K Matrix:
[-2.928342, 2.318348]
[-15.084049, 13.515744]
[6.576701, -7.903944]
[15.662915, -14.443155]
[12.829720, -11.811445]
[1.378370, -1.580297]
[12.807543, -12.816852]
[0.000000, 0.000000]
[-8.800404, 7.380087]
[-7.350675, 7.279441]
Final Q Matrix:
[2.873940, -2.640048]
[14.331178, -14.093612]
[-5.644496, 6.776846]
[-14.758403, 14.762915]
[-12.094573, 12.086499]
[-1.205892, 1.396358]
[-11.765592, 12.387978]
[0.000000, 0.000000]
[8.512921, -8.063755]
[6.775662, -7.085808]
Final V Matrix:
[-1.995016, 3.254692]
[-12.491758, 15.505865]
[8.277215, -5.151554]
[13.546477, -15.773888]
[11.069157, -12.935943]
[1.627451, -1.140685]
[12.493992, -12.092802]
[0.000000, 0.000000]
[-6.576686, 9.450826]
[-7.062927, 7.001732]
 

Self-Attention Matrix: 
-1.995016 3.254692  
-1.995016 3.254692  
-12.491758 15.505865  
-12.491758 15.505865  
-12.491758 15.505865  
-12.491758 15.505865  
-12.491758 15.505865  
-7.243387 9.380279  
-1.995016 3.254692  
-1.995016 3.254692  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.223178 17.488229 
-10.280618 48.029804 
-45.868101 32.455845 
-11.405952 -23.574812 
-11.250098 -16.853021 
-18.084582 17.675582 
-30.120248 1.807299 
-7.243387 9.380279 
-16.123581 38.542270 
6.714084 12.505553 




semi_final_layer_weights[0] = -0.004093
semi_final_layer_weights[1] = 0.003733
semi_final_layer_weights[2] = 0.004169
semi_final_layer_weights[3] = -0.006037
semi_final_layer_weights[4] = 0.005202
semi_final_layer_weights[5] = -0.005228
semi_final_layer_weights[6] = 0.004160
semi_final_layer_weights[7] = -0.003602
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004085
.
.
.
semi_final_layer_weights[511] = -0.003725
semi_final_layer_weights[512] = 0.009334


 Semi Final Layer Node Values: 
-0.000379 
0.144642 
-0.000517 
0.217209 
-0.001514 
-0.000031 
-0.001136 
-0.000113 
0.113237 
-0.000826 
.
.
.
0.011443 
-0.000120 
0.013108 
0.013852 
-0.000149 
0.013983 
0.013843 
-0.000125 
0.012446 
-0.000159 
-0.000117 
0.029280 


final_layer_weights[0] = 1.089239
final_layer_weights[1] = 0.874509
final_layer_weights[2] = -0.368473
final_layer_weights[3] = 0.544209
final_layer_weights[4] = 1.012671
final_layer_weights[5] = 0.418372
final_layer_weights[6] = 0.197412
final_layer_weights[7] = 0.826209
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.411122
.
.
.
final_layer_weights[1022] = 0.277034
final_layer_weights[1023] = -1.025680


Output Embedding: 0.036650, 0.096340 

 Expected Embedding: -37.748848, 10.192697 
 loss: 764.840152 


Updated weights for final layer:
final_layer_weights[0] = 1.099239
final_layer_weights[1] = 0.864509
final_layer_weights[2] = -0.378473
final_layer_weights[3] = 0.554209
final_layer_weights[4] = 1.002671
final_layer_weights[5] = 0.428372
final_layer_weights[6] = 0.187412
final_layer_weights[7] = 0.836209
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.421122
.
.
.
final_layer_weights[1022] = 0.287034
final_layer_weights[1023] = -1.035680


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005907
semi_final_layer_weights[1] = -0.006267
semi_final_layer_weights[2] = -0.005831
semi_final_layer_weights[3] = 0.003963
semi_final_layer_weights[4] = -0.004798
semi_final_layer_weights[5] = 0.004772
semi_final_layer_weights[6] = -0.005840
semi_final_layer_weights[7] = 0.006398
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005915
.
.
.
semi_final_layer_weights[510] = 0.006275
semi_final_layer_weights[511] = -0.000666
Updating K Matrix:
k_matrix[0][0] = 0.593684
k_matrix[0][1] = -0.569427
k_matrix[1][0] = 0.587927
k_matrix[1][1] = -0.618464

Updating Q Matrix:
q_matrix[0][0] = -0.633936
q_matrix[0][1] = 0.599465
q_matrix[1][0] = -0.612190
q_matrix[1][1] = 0.611117

Updating V Matrix:
v_matrix[0][0] = 0.569902
v_matrix[0][1] = -0.635532
v_matrix[1][0] = 0.641422
v_matrix[1][1] = -0.586250

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.593684 -0.569427 
0.587927 -0.618464 

Q Matrix:
-0.633936 0.599465 
-0.612190 0.611117 

V Matrix:
0.569902 -0.635532 
0.641422 -0.586250 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = 0.005907
semi_final_layer_weights[1] = -0.006267
semi_final_layer_weights[2] = -0.005831
semi_final_layer_weights[3] = 0.003963
semi_final_layer_weights[4] = -0.004798
semi_final_layer_weights[5] = 0.004772
semi_final_layer_weights[6] = -0.005840
semi_final_layer_weights[7] = 0.006398
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005915
.
.
.
semi_final_layer_weights[511] = 0.006275
semi_final_layer_weights[512] = -0.000666


 Semi Final Layer Node Values: 
0.005907 
-0.000063 
-0.000058 
0.003963 
-0.000048 
0.004772 
-0.000058 
0.006398 
-0.000052 
0.005915 
.
.
.
-0.000064 
0.006176 
-0.000058 
-0.000056 
0.005266 
-0.000055 
-0.000056 
0.006015 
-0.000060 
0.004946 
0.006275 
-0.000007 


final_layer_weights[0] = 1.099239
final_layer_weights[1] = 0.864509
final_layer_weights[2] = -0.378473
final_layer_weights[3] = 0.554209
final_layer_weights[4] = 1.002671
final_layer_weights[5] = 0.428372
final_layer_weights[6] = 0.187412
final_layer_weights[7] = 0.836209
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.421122
.
.
.
final_layer_weights[1022] = 0.287034
final_layer_weights[1023] = -1.035680


Output Embedding: -0.060501, -0.010494 

 Expected Embedding: -10.885829, -17.296432 
 loss: 207.995702 


Updated weights for final layer:
final_layer_weights[0] = 1.089239
final_layer_weights[1] = 0.874509
final_layer_weights[2] = -0.368473
final_layer_weights[3] = 0.545966
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197412
final_layer_weights[7] = 0.826209
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.411122
.
.
.
final_layer_weights[1022] = 0.277034
final_layer_weights[1023] = -1.034295


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004093
semi_final_layer_weights[1] = 0.003733
semi_final_layer_weights[2] = 0.004169
semi_final_layer_weights[3] = -0.004280
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004160
semi_final_layer_weights[7] = -0.003602
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004085
.
.
.
semi_final_layer_weights[510] = -0.003725
semi_final_layer_weights[511] = 0.000719
Updating K Matrix:
k_matrix[0][0] = -0.406316
k_matrix[0][1] = 0.430573
k_matrix[1][0] = -0.412073
k_matrix[1][1] = 0.381536

Updating Q Matrix:
q_matrix[0][0] = 0.366064
q_matrix[0][1] = -0.400535
q_matrix[1][0] = 0.387810
q_matrix[1][1] = -0.388883

Updating V Matrix:
v_matrix[0][0] = -0.430098
v_matrix[0][1] = 0.364468
v_matrix[1][0] = -0.358578
v_matrix[1][1] = 0.413750

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 9  total loss: 4966.423737 ******************************************************************* 

Epoch: 10
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.406316 0.430573 
-0.412073 0.381536 

Q Matrix:
0.366064 -0.400535 
0.387810 -0.388883 

V Matrix:
-0.430098 0.364468 
-0.358578 0.413750 
Final K Matrix:
[-9.691689, 10.529811]
[-5.898125, 6.886660]
[-15.263273, 14.493747]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[8.653638, -9.635368]
[5.122697, -6.014213]
[14.255972, -14.517945]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-10.624261, 8.485872]
[-7.139175, 4.781457]
[-13.790787, 15.036075]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-7.139175 4.781457  
-7.139175 4.781457  
-7.139175 4.781457  
-8.881718 6.633664  
-8.881718 6.633664  
-8.881718 6.633664  
-8.881718 6.633664  
-8.881718 6.633664  
-8.881718 6.633664  
-8.881718 6.633664  


Context Matrix (embedding_matrix + self_attention_matrix:
21.486638 0.074943 
19.082389 -6.760447 
-0.488535 35.263948 
-8.881718 6.633664 
-8.881718 6.633664 
-8.881718 6.633664 
-8.881718 6.633664 
-8.881718 6.633664 
-8.881718 6.633664 
-8.881718 6.633664 




semi_final_layer_weights[0] = -0.004093
semi_final_layer_weights[1] = 0.003733
semi_final_layer_weights[2] = 0.004169
semi_final_layer_weights[3] = -0.004280
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004160
semi_final_layer_weights[7] = -0.003602
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004085
.
.
.
semi_final_layer_weights[511] = -0.003725
semi_final_layer_weights[512] = 0.000719


 Semi Final Layer Node Values: 
-0.000923 
0.042262 
0.149138 
0.005342 
-0.000065 
0.006432 
-0.000052 
0.004496 
-0.000060 
0.005099 
.
.
.
-0.000046 
0.004772 
-0.000052 
-0.000055 
0.005909 
-0.000056 
-0.000055 
0.004973 
-0.000050 
0.006307 
0.004649 
-0.000009 


final_layer_weights[0] = 1.089239
final_layer_weights[1] = 0.874509
final_layer_weights[2] = -0.368473
final_layer_weights[3] = 0.545966
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197412
final_layer_weights[7] = 0.826209
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.411122
.
.
.
final_layer_weights[1022] = 0.277034
final_layer_weights[1023] = -1.034295


Output Embedding: -0.044960, -0.002472 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.970596 


Updated weights for final layer:
final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.035963


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[510] = 0.004916
semi_final_layer_weights[511] = -0.000949
Updating K Matrix:
k_matrix[0][0] = 0.536217
k_matrix[0][1] = -0.568230
k_matrix[1][0] = 0.543815
k_matrix[1][1] = -0.503515

Updating Q Matrix:
q_matrix[0][0] = -0.483096
q_matrix[0][1] = 0.528589
q_matrix[1][0] = -0.511795
q_matrix[1][1] = 0.513211

Updating V Matrix:
v_matrix[0][0] = 0.567603
v_matrix[0][1] = -0.480991
v_matrix[1][0] = 0.473218
v_matrix[1][1] = -0.546028

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.536217 -0.568230 
0.543815 -0.503515 

Q Matrix:
-0.483096 0.528589 
-0.511795 0.513211 

V Matrix:
0.567603 -0.480991 
0.473218 -0.546028 
Final K Matrix:
[12.802997, -13.383189]
[7.802035, -7.303565]
[20.143032, -19.127485]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-11.589967, 12.562985]
[-7.318707, 7.388017]
[-18.813692, 19.159419]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[13.293148, -11.631732]
[6.901390, -7.770004]
[18.199784, -19.843198]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
6.901390 -7.770004  
6.901390 -7.770004  
6.901390 -7.770004  
10.097269 -9.700868  
10.097269 -9.700868  
10.097269 -9.700868  
10.097269 -9.700868  
10.097269 -9.700868  
10.097269 -9.700868  
10.097269 -9.700868  


Context Matrix (embedding_matrix + self_attention_matrix:
28.211113 -5.239079 
8.012378 5.481378 
13.552030 22.712487 
10.097269 -9.700868 
10.097269 -9.700868 
10.097269 -9.700868 
10.097269 -9.700868 
10.097269 -9.700868 
10.097269 -9.700868 
10.097269 -9.700868 




semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[511] = 0.004916
semi_final_layer_weights[512] = -0.000949


 Semi Final Layer Node Values: 
0.118671 
-0.000714 
-0.002050 
-0.000034 
0.002908 
-0.000029 
0.003314 
-0.000029 
0.003117 
-0.000033 
.
.
.
0.002906 
-0.000030 
0.003329 
0.003371 
-0.000032 
0.003345 
0.003372 
-0.000032 
0.003161 
-0.000030 
-0.000030 
0.000573 


final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.035963


Output Embedding: 0.109667, 0.116945 

 Expected Embedding: 12.470385, -17.532656 
 loss: 232.147876 


Updated weights for final layer:
final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.033760


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[510] = -0.005084
semi_final_layer_weights[511] = 0.001254
Updating K Matrix:
k_matrix[0][0] = -0.463783
k_matrix[0][1] = 0.431770
k_matrix[1][0] = -0.456185
k_matrix[1][1] = 0.496485

Updating Q Matrix:
q_matrix[0][0] = 0.516904
q_matrix[0][1] = -0.471411
q_matrix[1][0] = 0.488205
q_matrix[1][1] = -0.486789

Updating V Matrix:
v_matrix[0][0] = -0.432397
v_matrix[0][1] = 0.519009
v_matrix[1][0] = -0.526782
v_matrix[1][1] = 0.453972

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
.
.
.
semi_final_layer_weights[511] = 0.004069
semi_final_layer_weights[512] = 0.731305


 Semi Final Layer Node Values:
30.943528
-0.001414
-0.058452
3.210205
1.462129
-0.046649
0.140064
-0.001912
-0.023495
5.876434
.
.
.
-0.002116
0.095322
-0.000016
-0.000015
-0.000497
-0.000830
0.116043
0.044651
0.086826
0.163941
0.001274
0.229070


final_layer_weights[0] = 0.447141
final_layer_weights[1] = 0.750194
final_layer_weights[2] = -0.194253
final_layer_weights[3] = -0.022823
final_layer_weights[4] = 0.859656
final_layer_weights[5] = 0.571736
final_layer_weights[6] = 0.188205
final_layer_weights[7] = 0.835286
final_layer_weights[8] = 0.731983
final_layer_weights[9] = 1.218050
.
.
.
final_layer_weights[1022] = 0.284828
final_layer_weights[1023] = -0.303709


Output Embedding: 15.542229, -0.029070

 Expected Embedding: 28.923067, 35.905037
 loss: 735.153450


Updated weights for final layer:
final_layer_weights[0] = 0.457141
final_layer_weights[1] = 0.760194
final_layer_weights[2] = -0.204253
final_layer_weights[3] = -0.012823
final_layer_weights[4] = 0.869656
final_layer_weights[5] = 0.561736
final_layer_weights[6] = 0.198205
final_layer_weights[7] = 0.825286
final_layer_weights[8] = 0.741983
final_layer_weights[9] = 1.228050
.
.
.
final_layer_weights[1022] = 0.274828
final_layer_weights[1023] = -0.313709


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.636191
semi_final_layer_weights[1] = -0.110582
semi_final_layer_weights[2] = 0.168389
semi_final_layer_weights[3] = -0.563069
semi_final_layer_weights[4] = -0.137813
semi_final_layer_weights[5] = 0.138136
semi_final_layer_weights[6] = 0.004953
semi_final_layer_weights[7] = -0.004525
semi_final_layer_weights[8] = -0.316670
semi_final_layer_weights[9] = -0.187157
.
.
.
semi_final_layer_weights[510] = -0.005931
semi_final_layer_weights[511] = 0.721305
Updating K Matrix:
k_matrix[0][0] = -0.487854
k_matrix[0][1] = 0.534659
k_matrix[1][0] = -0.492268
k_matrix[1][1] = 0.598009

Updating Q Matrix:
q_matrix[0][0] = 0.457000
q_matrix[0][1] = -0.608272
q_matrix[1][0] = 0.473669
q_matrix[1][1] = -0.474492

Updating V Matrix:
v_matrix[0][0] = -0.506085
v_matrix[0][1] = 0.447537
v_matrix[1][0] = -0.451000
v_matrix[1][1] = 0.546508

UPDATED WEIGHTS FOR THE LAST LAYER


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000,
y_actual token: 78
max sentence length: 512
Embedding Matrix:
 [0.546528, 26.764406]
 [-7.228162, 13.233537]
 [-11.956720, -0.484444]
 [0.088311, -39.151413]
 [22.178827, 29.274549]
 [31.212883, 10.217854]
 [4.298979, 29.153065]
 [-25.994837, 15.520844]
 [-10.329999, 1.603721]
 [-40.464542, -13.909239]
Processing Sentence 7...
Adding positional encoding values to the matrix
Matrix Post Positional Encoding:
 [0.546528, 27.764406]
 [-6.748737, 14.111119]
 [-11.115250, 0.055858]
 [1.085806, -39.080677]
 [23.088125, 28.858402]
 [31.811356, 9.416711]
 [4.440099, 28.163073]
 [-26.345619, 14.584387]
 [-11.086802, 0.950077]
 [-41.442074, -14.120034]
 Self attention block
K Matrix:
-0.487854 0.534659
-0.492268 0.598009

Q Matrix:
0.457000 -0.608272
0.473669 -0.474492

V Matrix:
-0.506085 0.447537
-0.451000 0.546508
Final K Matrix:
[-13.934147, 16.895562]
[-3.654047, 4.830299]
[5.395127, -5.909465]
[18.708440, -22.790049]
[-25.469705, 29.601850]
[-20.154855, 22.639503]
[-16.029894, 19.215701]
[5.673406, -5.364333]
[4.941053, -5.359504]
[27.168539, -30.601282]
Final Q Matrix:
[13.400909, -13.506424]
[3.599831, -2.590542]
[-5.053211, 6.734594]
[-18.015102, 17.882999]
[24.220611, -27.736945]
[18.998196, -23.818119]
[15.369107, -16.063939]
[-5.131772, 9.105137]
[-4.616646, 6.292990]
[-25.627254, 31.907907]
Final V Matrix:
[-12.798337, 15.418062]
[-2.948683, 4.691528]
[5.600066, -4.943962]
[17.075876, -20.871965]
[-24.699686, 26.104146]
[-20.346176, 19.383077]
[-14.948612, 17.378455]
[6.755556, -3.820164]
[5.182375, -4.442533]
[27.341334, -26.263587]


Self-Attention Matrix:
-2.948683 4.691528
-2.948683 4.691528
-12.798337 15.418062
-12.798337 15.418062
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.463783 0.431770 
-0.456185 0.496485 

Q Matrix:
0.516904 -0.471411 
0.488205 -0.486789 

V Matrix:
-0.432397 0.519009 
-0.526782 0.453972 
Final K Matrix:
[3.202937, -2.837534]
[-18.610641, 16.954295]
[5.801169, -6.783270]
[-5.443066, 4.428092]
[-22.612893, 21.013972]
[-20.649084, 21.008932]
[11.176876, -11.121746]
[20.038448, -19.106503]
[-24.162037, 23.983852]
[15.536987, -16.811040]
Final Q Matrix:
[-3.610464, 3.209180]
[20.847021, -18.797130]
[-6.076022, 6.341463]
[6.246650, -5.326888]
[25.213651, -22.972584]
[22.511123, -21.563158]
[-12.255179, 11.591226]
[-22.206455, 20.513236]
[26.509729, -25.038788]
[-16.655309, 16.547607]
Final V Matrix:
[2.782198, -3.697991]
[-16.825768, 21.119514]
[7.362706, -5.403200]
[-4.171155, 6.594647]
[-21.028813, 25.335569]
[-21.774907, 21.702102]
[11.433029, -11.943656]
[19.320149, -22.069252]
[-24.632385, 25.866121]
[17.802183, -15.539200]
 

Self-Attention Matrix: 
-16.825768 21.119514  
2.782198 -3.697991  
-16.825768 21.119514  
2.782198 -3.697991  
2.782198 -3.697991  
2.782198 -3.697991  
-16.825768 21.119514  
-16.825768 21.119514  
2.782198 -3.697991  
-16.825768 21.119514  


Context Matrix (embedding_matrix + self_attention_matrix:
-25.709247 23.129813 
48.003351 -8.876048 
-10.391365 1.861231 
23.277271 -12.602709 
52.060964 -4.227928 
22.845842 21.168937 
-31.109862 11.140751 
-53.849785 14.834018 
34.469850 17.052047 
-18.175627 -11.566689 




semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[511] = -0.005084
semi_final_layer_weights[512] = 0.001254


 Semi Final Layer Node Values: 
0.007264 
0.193451 
-0.000339 
-0.000421 
0.242688 
-0.002320 
-0.000856 
0.199435 
0.253961 
0.141673 
.
.
.
0.013945 
-0.000133 
0.012062 
0.011874 
-0.000127 
0.011987 
0.011866 
-0.000127 
0.012810 
-0.000136 
-0.000137 
0.003373 


final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.033760


Output Embedding: 0.924076, -0.175920 

 Expected Embedding: 19.465172, 25.668072 
 loss: 505.842082 


Updated weights for final layer:
final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[510] = 0.004916
semi_final_layer_weights[511] = -0.005091
Updating K Matrix:
k_matrix[0][0] = 0.536217
k_matrix[0][1] = -0.568230
k_matrix[1][0] = 0.543815
k_matrix[1][1] = -0.503515

Updating Q Matrix:
q_matrix[0][0] = -0.483096
q_matrix[0][1] = 0.528589
q_matrix[1][0] = -0.511795
q_matrix[1][1] = 0.513211

Updating V Matrix:
v_matrix[0][0] = 0.567603
v_matrix[0][1] = -0.480991
v_matrix[1][0] = 0.473218
v_matrix[1][1] = -0.546028

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.536217 -0.568230 
0.543815 -0.503515 

Q Matrix:
-0.483096 0.528589 
-0.511795 0.513211 

V Matrix:
0.567603 -0.480991 
0.473218 -0.546028 
Final K Matrix:
[19.941812, -18.880800]
[2.259544, -1.901583]
[14.323874, -14.081273]
[35.608098, -35.116110]
[20.388122, -19.230421]
[18.207478, -17.817220]
[-5.129698, 5.480609]
[12.881483, -12.660228]
[-8.450213, 9.369965]
[10.247473, -10.166285]
Final Q Matrix:
[-18.642451, 18.950549]
[-2.183716, 2.072515]
[-13.234547, 13.775124]
[-32.866745, 34.278860]
[-19.081587, 19.351751]
[-16.847394, 17.484215]
[4.608108, -5.070757]
[-11.902775, 12.387022]
[7.488375, -8.460498]
[-9.440414, 9.883919]
Final V Matrix:
[17.939699, -19.689466]
[1.698042, -2.421173]
[13.617057, -13.726952]
[34.007389, -34.035299]
[18.238525, -20.188492]
[17.193766, -17.514228]
[-5.492814, 4.565641]
[12.241500, -12.347137]
[-9.529352, 7.247642]
[9.871832, -9.746519]
 

Self-Attention Matrix: 
1.698042 -2.421173  
1.698042 -2.421173  
1.698042 -2.421173  
1.698042 -2.421173  
1.698042 -2.421173  
1.698042 -2.421173  
17.939699 -19.689466  
1.698042 -2.421173  
17.939699 -19.689466  
1.698042 -2.421173  


Context Matrix (embedding_matrix + self_attention_matrix:
7.507210 28.521015 
-0.957193 4.351948 
13.111140 12.664784 
31.618604 33.554600 
6.620597 30.215943 
15.064361 17.880255 
7.750820 -19.075738 
11.918886 11.188025 
-3.606804 -13.982761 
11.150518 7.102082 




semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[511] = 0.004916
semi_final_layer_weights[512] = -0.005091


 Semi Final Layer Node Values: 
0.199989 
-0.000216 
-0.001473 
0.373776 
-0.001823 
0.164507 
0.067659 
0.114602 
0.096009 
0.103802 
.
.
.
0.010766 
-0.000113 
0.012333 
0.012489 
-0.000118 
0.012395 
0.012495 
-0.000118 
0.011711 
-0.000111 
-0.000110 
0.011385 


final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Output Embedding: 0.549713, 0.841581 

 Expected Embedding: 4.643339, -27.140186 
 loss: 399.868526 


Updated weights for final layer:
final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.030105


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[510] = -0.005084
semi_final_layer_weights[511] = 0.004909
Updating K Matrix:
k_matrix[0][0] = -0.463783
k_matrix[0][1] = 0.431770
k_matrix[1][0] = -0.456185
k_matrix[1][1] = 0.496485

Updating Q Matrix:
q_matrix[0][0] = 0.516904
q_matrix[0][1] = -0.471411
q_matrix[1][0] = 0.488205
q_matrix[1][1] = -0.486789

Updating V Matrix:
v_matrix[0][0] = -0.432397
v_matrix[0][1] = 0.519009
v_matrix[1][0] = -0.526782
v_matrix[1][1] = 0.453972

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.463783 0.431770 
-0.456185 0.496485 

Q Matrix:
0.516904 -0.471411 
0.488205 -0.486789 

V Matrix:
-0.432397 0.519009 
-0.526782 0.453972 
Final K Matrix:
[-9.264940, 9.376449]
[10.168016, -10.048453]
[17.182581, -18.768388]
[-16.344331, 18.068190]
[-17.575356, 16.057313]
[10.253183, -11.117001]
[-11.152273, 11.355623]
[-29.203465, 29.698025]
[-20.629884, 22.514628]
[4.608806, -4.190682]
Final Q Matrix:
[10.114485, -9.658998]
[-11.168549, 10.522632]
[-18.369524, 18.357136]
[17.412664, -17.530903]
[19.674321, -17.766325]
[-10.984693, 10.927528]
[12.155397, -11.648848]
[31.840946, -30.491593]
[22.060377, -22.033904]
[-5.164869, 4.652434]
Final V Matrix:
[-9.699481, 9.776748]
[10.302942, -10.920246]
[19.937609, -17.045786]
[-19.269432, 16.044566]
[-15.955022, 19.908311]
[11.780605, -10.236496]
[-11.773037, 11.713904]
[-30.775386, 30.704032]
[-23.910482, 20.480780]
[4.155570, -5.236366]
 

Self-Attention Matrix: 
10.302942 -10.920246  
-9.699481 9.776748  
-9.699481 9.776748  
10.302942 -10.920246  
10.302942 -10.920246  
-9.699481 9.776748  
10.302942 -10.920246  
10.302942 -10.920246  
10.302942 -10.920246  
-9.699481 9.776748  


Context Matrix (embedding_matrix + self_attention_matrix:
19.989572 -0.458589 
-23.645269 1.665560 
-8.769699 -28.834362 
6.467006 28.807900 
52.376009 -15.167224 
-10.274536 -12.114565 
21.015690 2.635416 
38.875227 24.048336 
9.449963 35.169598 
-21.007010 11.169674 




semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[511] = -0.005084
semi_final_layer_weights[512] = 0.004909


 Semi Final Layer Node Values: 
-0.000852 
-0.001064 
-0.001737 
-0.001579 
0.187633 
0.120539 
0.111186 
-0.003353 
0.220586 
0.040726 
.
.
.
-0.000066 
0.006291 
-0.000057 
-0.000056 
0.006013 
-0.000057 
-0.000056 
0.006022 
-0.000061 
0.006418 
0.006456 
-0.000062 


final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.030105


Output Embedding: 0.347949, 0.074806 

 Expected Embedding: 44.423141, -6.847703 
 loss: 995.271853 


Updated weights for final layer:
final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[510] = 0.004916
semi_final_layer_weights[511] = -0.005091
Updating K Matrix:
k_matrix[0][0] = 0.536217
k_matrix[0][1] = -0.568230
k_matrix[1][0] = 0.543815
k_matrix[1][1] = -0.503515

Updating Q Matrix:
q_matrix[0][0] = -0.483096
q_matrix[0][1] = 0.528589
q_matrix[1][0] = -0.511795
q_matrix[1][1] = 0.513211

Updating V Matrix:
v_matrix[0][0] = 0.567603
v_matrix[0][1] = -0.480991
v_matrix[1][0] = 0.473218
v_matrix[1][1] = -0.546028

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.536217 -0.568230 
0.543815 -0.503515 

Q Matrix:
-0.483096 0.528589 
-0.511795 0.513211 

V Matrix:
0.567603 -0.480991 
0.473218 -0.546028 
Final K Matrix:
[-25.965768, 25.042450]
[-0.424071, 0.923885]
[-17.632196, 18.377195]
[-4.093654, 4.459897]
[-6.413574, 6.730942]
[-17.047073, 16.811092]
[-15.104759, 14.141028]
[-18.783217, 19.642406]
[4.667199, -4.524897]
[-16.308574, 14.447559]
Final Q Matrix:
[24.136294, -24.819060]
[0.239560, -0.567152]
[15.977849, -17.284672]
[3.651520, -4.073708]
[5.797890, -6.301740]
[15.734808, -16.410572]
[14.168643, -14.303630]
[17.001187, -18.433609]
[-4.331256, 4.468520]
[15.544256, -15.185756]
Final V Matrix:
[-24.003836, 25.270579]
[-1.116802, 0.000742]
[-18.231164, 16.062375]
[-4.504777, 3.574545]
[-6.696730, 5.805455]
[-16.280120, 16.294466]
[-13.362949, 15.041702]
[-19.513565, 17.058467]
[4.347856, -4.523312]
[-13.273027, 16.896969]
 

Self-Attention Matrix: 
-1.116802 0.000742  
-1.117045 0.001010  
-1.116802 0.000742  
-1.116802 0.000742  
-1.116802 0.000742  
-1.116802 0.000742  
-1.116802 0.000742  
-1.116802 0.000742  
-24.003836 25.270579  
-1.116802 0.000742  


Context Matrix (embedding_matrix + self_attention_matrix:
-15.066828 -33.991541 
-8.521050 6.521761 
-29.711260 -4.227448 
-10.449306 1.675200 
-12.164226 -0.899850 
-15.434799 -17.228483 
-3.285891 -25.636007 
-32.491568 -3.602553 
-21.166651 31.055357 
7.976395 -38.954584 




semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[511] = 0.004916
semi_final_layer_weights[512] = -0.005091


 Semi Final Layer Node Values: 
-0.002704 
0.004923 
0.192216 
-0.000439 
0.067761 
-0.001631 
0.164259 
-0.001763 
-0.000562 
-0.001724 
.
.
.
-0.000052 
0.005426 
-0.000059 
-0.000060 
0.005663 
-0.000060 
-0.000060 
0.005655 
-0.000056 
0.005319 
0.005287 
-0.000055 


final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Output Embedding: -0.028316, 0.047864 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1061.959704 


Updated weights for final layer:
final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.030105


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[510] = -0.005084
semi_final_layer_weights[511] = 0.004909
Updating K Matrix:
k_matrix[0][0] = -0.463783
k_matrix[0][1] = 0.431770
k_matrix[1][0] = -0.456185
k_matrix[1][1] = 0.496485

Updating Q Matrix:
q_matrix[0][0] = 0.516904
q_matrix[0][1] = -0.471411
q_matrix[1][0] = 0.488205
q_matrix[1][1] = -0.486789

Updating V Matrix:
v_matrix[0][0] = -0.432397
v_matrix[0][1] = 0.519009
v_matrix[1][0] = -0.526782
v_matrix[1][1] = 0.453972

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.463783 0.431770 
-0.456185 0.496485 

Q Matrix:
0.516904 -0.471411 
0.488205 -0.486789 

V Matrix:
-0.432397 0.519009 
-0.526782 0.453972 
Final K Matrix:
[-12.919167, 14.020587]
[-3.307329, 4.092060]
[5.129580, -4.771495]
[17.324429, -18.934154]
[-23.872636, 24.296518]
[-19.049318, 18.410435]
[-14.906804, 15.899645]
[5.565470, -4.134310]
[4.708457, -4.315246]
[25.661463, -24.903817]
Final Q Matrix:
[13.837220, -13.773034]
[3.400671, -3.687702]
[-5.718242, 5.212660]
[-18.518119, 18.512164]
[26.023145, -24.931935]
[21.040686, -19.580169]
[16.044451, -15.802572]
[-6.497975, 5.320102]
[-5.266975, 4.763953]
[-28.315023, 26.409719]
Final V Matrix:
[-14.862111, 12.887908]
[-4.515354, 2.903390]
[4.776772, -5.743562]
[20.117504, -17.177977]
[-25.185320, 25.083853]
[-18.715679, 20.785314]
[-16.755688, 15.089691]
[3.708963, -7.052727]
[4.293412, -5.322847]
[25.357597, -27.918924]
 

Self-Attention Matrix: 
-4.515354 2.903390  
-4.515354 2.903390  
-14.862111 12.887908  
-14.862111 12.887908  
-4.515354 2.903390  
-4.515354 2.903390  
-4.515354 2.903390  
-14.862111 12.887908  
-14.862111 12.887908  
-14.862111 12.887908  


Context Matrix (embedding_matrix + self_attention_matrix:
-3.968826 30.667797 
-11.264091 17.014510 
-25.977360 12.943765 
-13.776305 -26.192770 
18.572771 31.761793 
27.296001 12.320101 
-0.075255 31.066463 
-41.207730 27.472294 
-25.948912 13.837985 
-56.304184 -1.232127 




semi_final_layer_weights[0] = -0.004599
semi_final_layer_weights[1] = 0.005074
semi_final_layer_weights[2] = 0.004498
semi_final_layer_weights[3] = -0.004352
semi_final_layer_weights[4] = 0.005182
semi_final_layer_weights[5] = -0.005154
semi_final_layer_weights[6] = 0.004510
semi_final_layer_weights[7] = -0.005246
semi_final_layer_weights[8] = 0.004835
semi_final_layer_weights[9] = -0.004608
.
.
.
semi_final_layer_weights[511] = -0.005084
semi_final_layer_weights[512] = 0.004909


 Semi Final Layer Node Values: 
-0.001274 
0.034250 
-0.000541 
0.178279 
0.266014 
-0.002093 
0.144293 
0.066811 
-0.000537 
0.269759 
.
.
.
-0.000041 
0.003929 
-0.000036 
-0.000035 
0.003755 
-0.000035 
-0.000035 
0.003760 
-0.000038 
0.004008 
0.004032 
-0.000039 


final_layer_weights[0] = 1.088733
final_layer_weights[1] = 0.875850
final_layer_weights[2] = -0.368144
final_layer_weights[3] = 0.545894
final_layer_weights[4] = 1.012651
final_layer_weights[5] = 0.418446
final_layer_weights[6] = 0.197762
final_layer_weights[7] = 0.824565
final_layer_weights[8] = 1.063488
final_layer_weights[9] = 1.410599
.
.
.
final_layer_weights[1022] = 0.275675
final_layer_weights[1023] = -1.030105


Output Embedding: 0.750161, 0.125810 

 Expected Embedding: 1.038754, 36.179375 
 loss: 649.971416 


Updated weights for final layer:
final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[510] = 0.004916
semi_final_layer_weights[511] = -0.005091
Updating K Matrix:
k_matrix[0][0] = 0.536217
k_matrix[0][1] = -0.568230
k_matrix[1][0] = 0.543815
k_matrix[1][1] = -0.503515

Updating Q Matrix:
q_matrix[0][0] = -0.483096
q_matrix[0][1] = 0.528589
q_matrix[1][0] = -0.511795
q_matrix[1][1] = 0.513211

Updating V Matrix:
v_matrix[0][0] = 0.567603
v_matrix[0][1] = -0.480991
v_matrix[1][0] = 0.473218
v_matrix[1][1] = -0.546028

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.536217 -0.568230 
0.543815 -0.503515 

Q Matrix:
-0.483096 0.528589 
-0.511795 0.513211 

V Matrix:
0.567603 -0.480991 
0.473218 -0.546028 
Final K Matrix:
[25.176568, -24.681320]
[22.891209, -22.801493]
[20.734495, -22.211821]
[-20.670442, 19.060717]
[0.000000, 0.000000]
[10.224769, -10.205005]
[-7.137550, 7.456383]
[-21.941138, 20.333985]
[7.557405, -6.611688]
[-25.836906, 24.999341]
Final Q Matrix:
[-23.282570, 24.190418]
[-21.060847, 22.107888]
[-18.608507, 20.514772]
[19.476753, -19.482707]
[0.000000, 0.000000]
[-9.401105, 9.881267]
[6.462685, -7.002295]
[20.643576, -20.712294]
[-7.228242, 7.010906]
[23.992134, -24.721397]
Final V Matrix:
[23.837323, -24.182457]
[22.181087, -21.698832]
[22.285178, -18.407402]
[-17.877367, 20.816894]
[0.000000, 0.000000]
[9.936168, -9.675923]
[-7.404304, 6.488277]
[-19.119275, 22.015358]
[6.033446, -7.896734]
[-23.998969, 25.080221]
 

Self-Attention Matrix: 
22.181087 -21.698832  
22.181087 -21.698832  
23.837210 -24.182288  
23.837323 -24.182457  
23.009205 -22.940644  
22.181087 -21.698832  
23.681114 -23.948212  
23.837323 -24.182457  
22.181087 -21.698832  
23.837323 -24.182457  


Context Matrix (embedding_matrix + self_attention_matrix:
41.282089 5.763216 
44.573669 -1.684830 
65.842452 -27.472817 
24.923129 -63.263134 
23.009205 -22.940644 
32.466190 -13.038319 
11.865514 -25.422649 
23.574851 -64.270325 
16.806034 -2.501872 
8.825227 -56.890549 




semi_final_layer_weights[0] = 0.005401
semi_final_layer_weights[1] = -0.004926
semi_final_layer_weights[2] = -0.005502
semi_final_layer_weights[3] = 0.005648
semi_final_layer_weights[4] = -0.004818
semi_final_layer_weights[5] = 0.004846
semi_final_layer_weights[6] = -0.005490
semi_final_layer_weights[7] = 0.004754
semi_final_layer_weights[8] = -0.005165
semi_final_layer_weights[9] = 0.005392
.
.
.
semi_final_layer_weights[511] = 0.004916
semi_final_layer_weights[512] = -0.005091


 Semi Final Layer Node Values: 
0.259492 
-0.002064 
-0.002056 
-0.002222 
0.004488 
0.089308 
0.079913 
-0.001982 
-0.000687 
-0.002645 
.
.
.
0.004484 
-0.000047 
0.005136 
0.005201 
-0.000049 
0.005162 
0.005204 
-0.000049 
0.004877 
-0.000046 
-0.000046 
0.004742 


final_layer_weights[0] = 1.098733
final_layer_weights[1] = 0.865850
final_layer_weights[2] = -0.378144
final_layer_weights[3] = 0.555894
final_layer_weights[4] = 1.002651
final_layer_weights[5] = 0.428446
final_layer_weights[6] = 0.187762
final_layer_weights[7] = 0.834565
final_layer_weights[8] = 1.053488
final_layer_weights[9] = 1.420599
.
.
.
final_layer_weights[1022] = 0.285675
final_layer_weights[1023] = -1.040105


Output Embedding: 0.737581, 0.497976 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.830244 


Updated weights for final layer:
final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866777
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.003558
final_layer_weights[5] = 0.427534
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.833670
final_layer_weights[8] = 1.054461
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.284750
final_layer_weights[1023] = -1.039146


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.003999
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.003911
semi_final_layer_weights[5] = 0.003934
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.003859
semi_final_layer_weights[8] = -0.004192
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[510] = 0.003991
semi_final_layer_weights[511] = -0.004132
Updating K Matrix:
k_matrix[0][0] = 0.435246
k_matrix[0][1] = -0.461231
k_matrix[1][0] = 0.441414
k_matrix[1][1] = -0.408702

Updating Q Matrix:
q_matrix[0][0] = -0.392128
q_matrix[0][1] = 0.429054
q_matrix[1][0] = -0.415423
q_matrix[1][1] = 0.416572

Updating V Matrix:
v_matrix[0][0] = 0.460722
v_matrix[0][1] = -0.390419
v_matrix[1][0] = 0.384110
v_matrix[1][1] = -0.443210

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
0.435246 -0.461231 
0.441414 -0.408702 

Q Matrix:
-0.392128 0.429054 
-0.415423 0.416572 

V Matrix:
0.460722 -0.390419 
0.384110 -0.443210 
Final K Matrix:
[3.136846, -2.483419]
[16.158065, -14.478094]
[-7.044976, 8.466721]
[-16.778148, 15.471538]
[-13.743223, 12.652445]
[-1.476513, 1.692818]
[-13.719467, 13.729439]
[0.000000, 0.000000]
[9.427011, -7.905564]
[7.874058, -7.797752]
Final Q Matrix:
[-3.078570, 2.828025]
[-15.351588, 15.097106]
[6.046396, -7.259372]
[15.809232, -15.814066]
[12.955732, -12.947083]
[1.291754, -1.495782]
[12.603327, -13.270028]
[0.000000, 0.000000]
[-9.119059, 8.637912]
[-7.258103, 7.590332]
Final V Matrix:
[2.137065, -3.486433]
[13.381197, -16.609916]
[-8.866570, 5.518356]
[-14.511015, 16.897022]
[-11.857304, 13.857010]
[-1.743329, 1.221904]
[-13.383591, 12.953835]
[0.000000, 0.000000]
[7.044960, -10.123744]
[7.565822, -7.500270]
 

Self-Attention Matrix: 
2.137065 -3.486433  
2.137065 -3.486433  
13.381197 -16.609916  
13.381197 -16.609916  
13.381197 -16.609916  
13.381197 -16.609916  
13.381197 -16.609916  
7.759131 -10.048174  
2.137065 -3.486433  
2.137065 -3.486433  


Context Matrix (embedding_matrix + self_attention_matrix:
-5.091097 10.747104 
-6.148536 41.288679 
-19.995146 0.340064 
14.467003 -55.690593 
14.622857 -48.968802 
7.788373 -14.440199 
-4.247293 -30.308482 
7.759131 -10.048174 
-11.991499 31.801146 
10.846165 5.764428 




semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.003999
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.003911
semi_final_layer_weights[5] = 0.003934
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.003859
semi_final_layer_weights[8] = -0.004192
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[511] = 0.003991
semi_final_layer_weights[512] = -0.004132


 Semi Final Layer Node Values: 
0.029180 
-0.001445 
0.083305 
-0.001936 
0.138230 
-0.000301 
0.158433 
-0.000127 
-0.000872 
0.077070 
.
.
.
0.012852 
-0.000135 
0.014722 
0.014908 
-0.000141 
0.014796 
0.014916 
-0.000140 
0.013979 
-0.000132 
-0.000131 
0.013591 


final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866777
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.003558
final_layer_weights[5] = 0.427534
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.833670
final_layer_weights[8] = 1.054461
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.284750
final_layer_weights[1023] = -1.039146


Output Embedding: 0.830505, 0.263563 

 Expected Embedding: -37.748848, 10.192697 
 loss: 793.477097 


Updated weights for final layer:
final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876777
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.013558
final_layer_weights[5] = 0.417534
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.823670
final_layer_weights[8] = 1.064461
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.274750
final_layer_weights[1023] = -1.029146


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.006001
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.006089
semi_final_layer_weights[5] = -0.006066
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.006141
semi_final_layer_weights[8] = 0.005808
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[510] = -0.006009
semi_final_layer_weights[511] = 0.005868
Updating K Matrix:
k_matrix[0][0] = -0.564754
k_matrix[0][1] = 0.538769
k_matrix[1][0] = -0.558586
k_matrix[1][1] = 0.591298

Updating Q Matrix:
q_matrix[0][0] = 0.607872
q_matrix[0][1] = -0.570946
q_matrix[1][0] = 0.584577
q_matrix[1][1] = -0.583428

Updating V Matrix:
v_matrix[0][0] = -0.539278
v_matrix[0][1] = 0.609581
v_matrix[1][0] = -0.615890
v_matrix[1][1] = 0.556790

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.564754 0.538769 
-0.558586 0.591298 

Q Matrix:
0.607872 -0.570946 
0.584577 -0.583428 

V Matrix:
-0.539278 0.609581 
-0.615890 0.556790 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.006001
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.006089
semi_final_layer_weights[5] = -0.006066
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.006141
semi_final_layer_weights[8] = 0.005808
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[511] = -0.006009
semi_final_layer_weights[512] = 0.005868


 Semi Final Layer Node Values: 
-0.000056 
0.006001 
0.005534 
-0.000054 
0.006089 
-0.000061 
0.005544 
-0.000061 
0.005808 
-0.000056 
.
.
.
0.006093 
-0.000059 
0.005524 
0.005467 
-0.000057 
0.005501 
0.005465 
-0.000057 
0.005750 
-0.000060 
-0.000060 
0.005868 


final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876777
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.013558
final_layer_weights[5] = 0.417534
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.823670
final_layer_weights[8] = 1.064461
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.274750
final_layer_weights[1023] = -1.029146


Output Embedding: 0.079381, 0.046787 

 Expected Embedding: -10.885829, -17.296432 
 loss: 210.511544 


Updated weights for final layer:
final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866777
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.003558
final_layer_weights[5] = 0.427534
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.833670
final_layer_weights[8] = 1.054461
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.284750
final_layer_weights[1023] = -1.039146


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.003999
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.003911
semi_final_layer_weights[5] = 0.003934
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.003859
semi_final_layer_weights[8] = -0.004192
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[510] = 0.003991
semi_final_layer_weights[511] = -0.004132
Updating K Matrix:
k_matrix[0][0] = 0.435246
k_matrix[0][1] = -0.461231
k_matrix[1][0] = 0.441414
k_matrix[1][1] = -0.408702

Updating Q Matrix:
q_matrix[0][0] = -0.392128
q_matrix[0][1] = 0.429054
q_matrix[1][0] = -0.415423
q_matrix[1][1] = 0.416572

Updating V Matrix:
v_matrix[0][0] = 0.460722
v_matrix[0][1] = -0.390419
v_matrix[1][0] = 0.384110
v_matrix[1][1] = -0.443210

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 10  total loss: 5099.850939 ******************************************************************* 

Epoch: 11
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.435246 -0.461231 
0.441414 -0.408702 

Q Matrix:
-0.392128 0.429054 
-0.415423 0.416572 

V Matrix:
0.460722 -0.390419 
0.384110 -0.443210 
Final K Matrix:
[10.381758, -11.279556]
[6.318083, -7.377004]
[16.350050, -15.525732]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-9.269795, 10.321427]
[-5.487444, 6.442437]
[-15.271028, 15.551654]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[11.380730, -9.090083]
[7.647499, -5.121907]
[14.772720, -16.106675]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.647499 -5.121907  
7.647499 -5.121907  
7.647499 -5.121907  
9.514115 -7.105995  
9.514115 -7.105995  
9.514115 -7.105995  
9.514115 -7.105995  
9.514115 -7.105995  
9.514115 -7.105995  
9.514115 -7.105995  


Context Matrix (embedding_matrix + self_attention_matrix:
36.273312 -9.828421 
33.869062 -16.663812 
14.298139 25.360583 
9.514115 -7.105995 
9.514115 -7.105995 
9.514115 -7.105995 
9.514115 -7.105995 
9.514115 -7.105995 
9.514115 -7.105995 
9.514115 -7.105995 




semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.003999
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.003911
semi_final_layer_weights[5] = 0.003934
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.003859
semi_final_layer_weights[8] = -0.004192
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[511] = 0.003991
semi_final_layer_weights[512] = -0.004132


 Semi Final Layer Node Values: 
0.111550 
-0.000648 
-0.001816 
0.006456 
-0.000055 
0.005539 
-0.000063 
0.005434 
-0.000059 
0.006162 
.
.
.
-0.000055 
0.005768 
-0.000063 
-0.000064 
0.006019 
-0.000063 
-0.000064 
0.006010 
-0.000060 
0.005653 
0.005619 
-0.000058 


final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866777
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.003558
final_layer_weights[5] = 0.427534
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.833670
final_layer_weights[8] = 1.054461
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.284750
final_layer_weights[1023] = -1.039146


Output Embedding: 0.019724, 0.089578 

 Expected Embedding: 12.470385, -17.532656 
 loss: 232.781022 


Updated weights for final layer:
final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[510] = -0.005299
semi_final_layer_weights[511] = 0.005487
Updating K Matrix:
k_matrix[0][0] = -0.564754
k_matrix[0][1] = 0.538769
k_matrix[1][0] = -0.558586
k_matrix[1][1] = 0.542678

Updating Q Matrix:
q_matrix[0][0] = 0.520672
q_matrix[0][1] = -0.569703
q_matrix[1][0] = 0.551603
q_matrix[1][1] = -0.553129

Updating V Matrix:
v_matrix[0][0] = -0.539278
v_matrix[0][1] = 0.518402
v_matrix[1][0] = -0.510025
v_matrix[1][1] = 0.556790

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.564754 0.538769 
-0.558586 0.542678 

Q Matrix:
0.520672 -0.569703 
0.551603 -0.553129 

V Matrix:
-0.539278 0.518402 
-0.510025 0.556790 
Final K Matrix:
[-13.448487, 12.854493]
[-8.029477, 7.789805]
[-20.783080, 20.125349]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[12.491438, -13.540138]
[7.887958, -7.962659]
[20.277026, -20.649644]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-12.782694, 12.456200]
[-7.357665, 7.954177]
[-19.133371, 20.420056]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-7.357665 7.954177  
-7.357665 7.954177  
-7.357665 7.954177  
-10.070180 10.205189  
-10.070180 10.205189  
-10.070180 10.205189  
-10.070180 10.205189  
-10.070180 10.205189  
-10.070180 10.205189  
-10.070180 10.205189  


Context Matrix (embedding_matrix + self_attention_matrix:
13.952058 10.485102 
-6.246678 21.205559 
-0.707026 38.436668 
-10.070180 10.205189 
-10.070180 10.205189 
-10.070180 10.205189 
-10.070180 10.205189 
-10.070180 10.205189 
-10.070180 10.205189 
-10.070180 10.205189 




semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[511] = -0.005299
semi_final_layer_weights[512] = 0.005487


 Semi Final Layer Node Values: 
-0.001429 
0.084731 
0.214347 
-0.000061 
0.005894 
-0.000059 
0.006293 
-0.000058 
0.006318 
-0.000064 
.
.
.
0.005889 
-0.000062 
0.006270 
0.006206 
-0.000064 
0.006244 
0.006203 
-0.000064 
0.006405 
-0.000061 
-0.000060 
0.006228 


final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Output Embedding: 0.081089, 0.051908 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.355768 


Updated weights for final layer:
final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[510] = 0.004701
semi_final_layer_weights[511] = -0.004513
Updating K Matrix:
k_matrix[0][0] = 0.435246
k_matrix[0][1] = -0.461231
k_matrix[1][0] = 0.441414
k_matrix[1][1] = -0.457322

Updating Q Matrix:
q_matrix[0][0] = -0.479328
q_matrix[0][1] = 0.430297
q_matrix[1][0] = -0.448397
q_matrix[1][1] = 0.446871

Updating V Matrix:
v_matrix[0][0] = 0.460722
v_matrix[0][1] = -0.481598
v_matrix[1][0] = 0.489975
v_matrix[1][1] = -0.443210

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
0.435246 -0.461231 
0.441414 -0.457322 

Q Matrix:
-0.479328 0.430297 
-0.448397 0.446871 

V Matrix:
0.460722 -0.481598 
0.489975 -0.443210 
Final K Matrix:
[-2.979128, 3.177985]
[17.396672, -18.489368]
[-5.700318, 5.839480]
[4.989740, -5.380648]
[21.214476, -22.486551]
[19.709225, -20.626159]
[-10.621859, 11.151772]
[-18.889067, 19.951123]
[22.951279, -24.104771]
[-15.015654, 15.570701]
Final Q Matrix:
[3.356689, -2.924192]
[-19.353945, 17.144612]
[5.551171, -5.837259]
[-5.831015, 4.839714]
[-23.383077, 20.967700]
[-20.767332, 19.745633]
[11.321218, -10.605623]
[20.565053, -18.740134]
[-24.493044, 22.907693]
[15.303429, -15.187350]
Final V Matrix:
[-3.107820, 3.387280]
[18.297272, -19.483444]
[-6.471608, 5.436667]
[5.079446, -5.923725]
[22.444167, -23.497675]
[21.427943, -20.683875]
[-11.470346, 11.301875]
[-20.137525, 20.616481]
[24.766209, -24.457327]
[-16.637336, 15.136936]
 

Self-Attention Matrix: 
18.297272 -19.483444  
-3.107820 3.387280  
18.297272 -19.483444  
-3.107820 3.387280  
-3.107820 3.387280  
-3.107820 3.387280  
18.297272 -19.483444  
18.297272 -19.483444  
-3.107820 3.387280  
18.297272 -19.483444  


Context Matrix (embedding_matrix + self_attention_matrix:
9.413793 -17.473146 
42.113333 -1.790777 
24.731674 -38.741728 
17.387253 -5.517437 
46.170946 2.857343 
16.955824 28.254209 
4.013178 -29.462208 
-18.726746 -25.768941 
28.579832 24.137319 
16.947412 -52.169647 




semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[511] = 0.004701
semi_final_layer_weights[512] = -0.004513


 Semi Final Layer Node Values: 
-0.000397 
-0.001844 
0.067028 
0.049836 
-0.002405 
0.220731 
0.117854 
-0.002219 
-0.002382 
-0.001585 
.
.
.
0.006993 
-0.000066 
0.006505 
0.006588 
-0.000063 
0.006538 
0.006591 
-0.000063 
0.006331 
-0.000068 
-0.000068 
0.006559 


final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Output Embedding: -0.119055, 0.022496 

 Expected Embedding: 19.465172, 25.668072 
 loss: 520.618749 


Updated weights for final layer:
final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[510] = -0.005299
semi_final_layer_weights[511] = 0.005487
Updating K Matrix:
k_matrix[0][0] = -0.564754
k_matrix[0][1] = 0.538769
k_matrix[1][0] = -0.558586
k_matrix[1][1] = 0.542678

Updating Q Matrix:
q_matrix[0][0] = 0.520672
q_matrix[0][1] = -0.569703
q_matrix[1][0] = 0.551603
q_matrix[1][1] = -0.553129

Updating V Matrix:
v_matrix[0][0] = -0.539278
v_matrix[0][1] = 0.518402
v_matrix[1][0] = -0.510025
v_matrix[1][1] = 0.556790

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
-0.564754 0.538769 
-0.558586 0.542678 

Q Matrix:
0.520672 -0.569703 
0.551603 -0.553129 

V Matrix:
-0.539278 0.518402 
-0.510025 0.556790 
Final K Matrix:
[-20.564637, 19.921459]
[-2.283820, 2.245069]
[-14.872401, 14.335846]
[-36.993330, 35.643545]
[-21.010682, 20.363580]
[-18.888782, 18.218505]
[5.411388, -5.156393]
[-13.374174, 12.892091]
[8.980781, -8.511678]
[-10.657883, 10.260766]
Final Q Matrix:
[20.092467, -20.424528]
[2.353566, -2.233716]
[14.263934, -14.846558]
[35.423130, -36.945081]
[20.565759, -20.856936]
[18.157790, -18.844143]
[-4.966528, 5.465162]
[12.828577, -13.350489]
[-8.070823, 9.118558]
[10.174692, -10.652693]
Final V Matrix:
[-18.914041, 20.239791]
[-2.022551, 2.394728]
[-13.849043, 14.316287]
[-34.484033, 35.541840]
[-19.300366, 20.723888]
[-17.562392, 18.232764]
[5.181619, -4.940219]
[-12.452903, 12.875975]
[8.708988, -7.992316]
[-9.954608, 10.202639]
 

Self-Attention Matrix: 
-2.022551 2.394728  
-2.022551 2.394728  
-2.022551 2.394728  
-2.022551 2.394728  
-2.022551 2.394728  
-2.022551 2.394728  
-18.914041 20.239791  
-2.022551 2.394728  
-18.914041 20.239791  
-2.022551 2.394728  


Context Matrix (embedding_matrix + self_attention_matrix:
3.786618 33.336916 
-4.677786 9.167848 
9.390547 17.480684 
27.898012 38.370500 
2.900005 35.031843 
11.343769 22.696155 
-29.102920 20.853520 
8.198293 16.003925 
-40.460544 25.946497 
7.429925 11.917983 




semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[511] = -0.005299
semi_final_layer_weights[512] = 0.005487


 Semi Final Layer Node Values: 
-0.002141 
0.029149 
0.154252 
-0.003643 
0.202164 
-0.001830 
-0.000402 
-0.001291 
-0.000752 
-0.001144 
.
.
.
0.009593 
-0.000101 
0.010214 
0.010109 
-0.000105 
0.010172 
0.010104 
-0.000105 
0.010435 
-0.000099 
-0.000098 
0.010145 


final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Output Embedding: 0.339716, 0.092687 

 Expected Embedding: 4.643339, -27.140186 
 loss: 380.075281 


Updated weights for final layer:
final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[510] = 0.004701
semi_final_layer_weights[511] = -0.004513
Updating K Matrix:
k_matrix[0][0] = 0.435246
k_matrix[0][1] = -0.461231
k_matrix[1][0] = 0.441414
k_matrix[1][1] = -0.457322

Updating Q Matrix:
q_matrix[0][0] = -0.479328
q_matrix[0][1] = 0.430297
q_matrix[1][0] = -0.448397
q_matrix[1][1] = 0.446871

Updating V Matrix:
v_matrix[0][0] = 0.460722
v_matrix[0][1] = -0.481598
v_matrix[1][0] = 0.489975
v_matrix[1][1] = -0.443210

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
0.435246 -0.461231 
0.441414 -0.457322 

Q Matrix:
-0.479328 0.430297 
-0.448397 0.446871 

V Matrix:
0.460722 -0.481598 
0.489975 -0.443210 
Final K Matrix:
[8.833986, -9.252116]
[-9.650240, 10.141653]
[-16.638784, 17.228847]
[15.866965, -16.399282]
[16.437470, -17.463175]
[-9.913413, 10.276602]
[10.646336, -11.140349]
[27.871585, -29.170313]
[19.973426, -20.684456]
[-4.306703, 4.578370]
Final Q Matrix:
[-9.334053, 8.843138]
[10.321644, -9.625487]
[16.867445, -16.854095]
[-15.975319, 16.102755]
[-18.262471, 16.206071]
[10.091645, -10.030033]
[-11.213243, 10.667294]
[-29.375316, 27.921011]
[-20.257702, 20.229170]
[4.795433, -4.243140]
Final V Matrix:
[9.588797, -9.301770]
[-10.399416, 10.311221]
[-18.490113, 16.665044]
[17.698502, -15.760527]
[17.303084, -18.379996]
[-10.991140, 9.979391]
[11.577538, -11.167239]
[30.297622, -29.258771]
[22.189890, -20.016680]
[-4.527132, 4.828324]
 

Self-Attention Matrix: 
-10.399416 10.311221  
9.588797 -9.301770  
9.588797 -9.301770  
-10.399416 10.311221  
-10.399416 10.311221  
9.588797 -9.301770  
-10.399416 10.311221  
-10.399416 10.311221  
-10.399416 10.311221  
9.588797 -9.301770  


Context Matrix (embedding_matrix + self_attention_matrix:
-0.712785 20.772877 
-4.356991 -17.412958 
10.518579 -47.912880 
-14.235351 50.039366 
31.673651 6.064242 
9.013742 -31.193083 
0.313332 23.866883 
18.172869 45.279803 
-11.252394 56.401064 
-1.718732 -7.908844 




semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[511] = 0.004701
semi_final_layer_weights[512] = -0.004513


 Semi Final Layer Node Values: 
0.092327 
0.106806 
0.171452 
0.168740 
-0.001862 
-0.001107 
-0.001122 
0.314294 
-0.002046 
-0.000465 
.
.
.
-0.000053 
0.005015 
-0.000049 
-0.000050 
0.004755 
-0.000049 
-0.000050 
0.004763 
-0.000048 
0.005133 
0.005169 
-0.000050 


final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Output Embedding: 0.273613, 0.296000 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1000.106661 


Updated weights for final layer:
final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[510] = -0.005299
semi_final_layer_weights[511] = 0.005487
Updating K Matrix:
k_matrix[0][0] = -0.564754
k_matrix[0][1] = 0.538769
k_matrix[1][0] = -0.558586
k_matrix[1][1] = 0.542678

Updating Q Matrix:
q_matrix[0][0] = 0.520672
q_matrix[0][1] = -0.569703
q_matrix[1][0] = 0.551603
q_matrix[1][1] = -0.553129

Updating V Matrix:
v_matrix[0][0] = -0.539278
v_matrix[0][1] = 0.518402
v_matrix[1][0] = -0.510025
v_matrix[1][1] = 0.556790

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
-0.564754 0.538769 
-0.558586 0.542678 

Q Matrix:
0.520672 -0.569703 
0.551603 -0.553129 

V Matrix:
-0.539278 0.518402 
-0.510025 0.556790 
Final K Matrix:
[26.865958, -25.962720]
[0.539037, -0.450376]
[18.510637, -17.700351]
[4.335237, -4.119370]
[6.742132, -6.440739]
[17.710154, -17.064021]
[15.545342, -15.081150]
[19.731769, -18.859177]
[-4.833610, 4.667862]
[16.624500, -16.241087]
Final Q Matrix:
[-26.013622, 26.749494]
[-0.258193, 0.611265]
[-17.220611, 18.629079]
[-3.935537, 4.390562]
[-6.248851, 6.791892]
[-16.958666, 17.686991]
[-15.270685, 15.416171]
[-18.323545, 19.867380]
[4.668142, -4.816082]
[-16.753293, 16.366909]
Final V Matrix:
[24.859849, -26.158292]
[0.667070, -0.207562]
[17.576837, -17.177643]
[4.178797, -3.905668]
[6.416954, -6.228448]
[16.508711, -17.015543]
[14.245121, -15.398750]
[18.757483, -18.271025]
[-4.480412, 4.691710]
[14.964427, -16.976009]
 

Self-Attention Matrix: 
0.667070 -0.207562  
0.667123 -0.207619  
0.667070 -0.207562  
0.667070 -0.207562  
0.667070 -0.207562  
0.667070 -0.207562  
0.667070 -0.207562  
0.667070 -0.207562  
24.859849 -26.158292  
0.667070 -0.207562  


Context Matrix (embedding_matrix + self_attention_matrix:
-13.282956 -34.199845 
-6.736883 6.313133 
-27.927388 -4.435752 
-8.665434 1.466896 
-10.380353 -1.108154 
-13.650927 -17.436787 
-1.502019 -25.844311 
-30.707695 -3.810857 
27.697034 -20.373513 
9.760267 -39.162888 




semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[511] = -0.005299
semi_final_layer_weights[512] = 0.005487


 Semi Final Layer Node Values: 
0.272281 
0.003060 
-0.001846 
0.033566 
-0.000648 
0.167604 
-0.001572 
0.181985 
0.035199 
0.170974 
.
.
.
-0.000074 
0.007720 
-0.000078 
-0.000078 
0.008056 
-0.000078 
-0.000078 
0.008045 
-0.000080 
0.007567 
0.007521 
-0.000078 


final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Output Embedding: 0.279258, 0.239004 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1046.266838 


Updated weights for final layer:
final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[510] = 0.004701
semi_final_layer_weights[511] = -0.004513
Updating K Matrix:
k_matrix[0][0] = 0.435246
k_matrix[0][1] = -0.461231
k_matrix[1][0] = 0.441414
k_matrix[1][1] = -0.457322

Updating Q Matrix:
q_matrix[0][0] = -0.479328
q_matrix[0][1] = 0.430297
q_matrix[1][0] = -0.448397
q_matrix[1][1] = 0.446871

Updating V Matrix:
v_matrix[0][0] = 0.460722
v_matrix[0][1] = -0.481598
v_matrix[1][0] = 0.489975
v_matrix[1][1] = -0.443210

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
0.435246 -0.461231 
0.441414 -0.457322 

Q Matrix:
-0.479328 0.430297 
-0.448397 0.446871 

V Matrix:
0.460722 -0.481598 
0.489975 -0.443210 
Final K Matrix:
[12.493459, -12.949336]
[3.291477, -3.340591]
[-4.813214, 5.101155]
[-16.778148, 17.371627]
[22.787509, -23.846531]
[18.002436, -18.978853]
[14.364098, -14.927491]
[-5.029085, 5.481667]
[-4.406112, 4.679088]
[-24.270281, 25.571771]
Final Q Matrix:
[-12.711450, 12.642271]
[-3.092528, 3.401885]
[5.302805, -4.757899]
[17.003211, -16.996793]
[-24.006816, 22.830732]
[-19.470505, 17.896389]
[-14.756509, 14.495816]
[6.088598, -4.819109]
[4.888204, -4.346058]
[26.195737, -24.142238]
Final V Matrix:
[13.855666, -12.568665]
[3.804804, -3.004010]
[-5.093674, 5.328324]
[-18.648305, 16.798019]
[24.777112, -23.909520]
[19.270153, -19.493860]
[15.844857, -14.620493]
[-4.992026, 6.224050]
[-4.642422, 4.918296]
[-26.011750, 26.216552]
 

Self-Attention Matrix: 
3.804804 -3.004010  
3.804804 -3.004010  
13.855666 -12.568665  
13.855666 -12.568665  
3.804804 -3.004010  
3.804804 -3.004010  
3.804804 -3.004010  
13.855666 -12.568665  
13.855666 -12.568665  
13.855666 -12.568665  


Context Matrix (embedding_matrix + self_attention_matrix:
4.351332 24.760396 
-2.943933 11.107109 
2.740416 -12.512807 
14.941472 -51.649342 
26.892929 25.854392 
35.616160 6.412701 
8.244903 25.159063 
-12.489953 2.015722 
2.768864 -11.618588 
-27.586408 -26.688699 




semi_final_layer_weights[0] = 0.004384
semi_final_layer_weights[1] = -0.004691
semi_final_layer_weights[2] = -0.004466
semi_final_layer_weights[3] = 0.004585
semi_final_layer_weights[4] = -0.004807
semi_final_layer_weights[5] = 0.004777
semi_final_layer_weights[6] = -0.004456
semi_final_layer_weights[7] = 0.004876
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004376
.
.
.
semi_final_layer_weights[511] = 0.004701
semi_final_layer_weights[512] = -0.004513


 Semi Final Layer Node Values: 
0.132009 
-0.000430 
0.048105 
-0.001729 
-0.002584 
0.205536 
-0.001533 
-0.000462 
0.043670 
-0.002419 
.
.
.
-0.000002 
0.000200 
-0.000002 
-0.000002 
0.000190 
-0.000002 
-0.000002 
0.000190 
-0.000002 
0.000205 
0.000206 
-0.000002 


final_layer_weights[0] = 1.097716
final_layer_weights[1] = 0.866085
final_layer_weights[2] = -0.377108
final_layer_weights[3] = 0.554831
final_layer_weights[4] = 1.002662
final_layer_weights[5] = 0.428377
final_layer_weights[6] = 0.188796
final_layer_weights[7] = 0.834687
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419583
.
.
.
final_layer_weights[1022] = 0.285460
final_layer_weights[1023] = -1.039527


Output Embedding: 0.022673, 0.237685 

 Expected Embedding: 1.038754, 36.179375 
 loss: 646.418757 


Updated weights for final layer:
final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[510] = -0.005299
semi_final_layer_weights[511] = 0.005487
Updating K Matrix:
k_matrix[0][0] = -0.564754
k_matrix[0][1] = 0.538769
k_matrix[1][0] = -0.558586
k_matrix[1][1] = 0.542678

Updating Q Matrix:
q_matrix[0][0] = 0.520672
q_matrix[0][1] = -0.569703
q_matrix[1][0] = 0.551603
q_matrix[1][1] = -0.553129

Updating V Matrix:
v_matrix[0][0] = -0.539278
v_matrix[0][1] = 0.518402
v_matrix[1][0] = -0.510025
v_matrix[1][1] = 0.556790

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
-0.564754 0.538769 
-0.558586 0.542678 

Q Matrix:
0.520672 -0.569703 
0.551603 -0.553129 

V Matrix:
-0.539278 0.518402 
-0.510025 0.556790 
Final K Matrix:
[-26.127290, 25.194087]
[-23.825845, 22.925594]
[-21.884574, 20.845416]
[21.216724, -20.623245]
[0.000000, 0.000000]
[-10.646196, 10.241167]
[7.496505, -7.166023]
[22.540772, -21.896236]
[-7.687580, 7.521867]
[26.748435, -25.838028]
Final Q Matrix:
[25.093495, -26.071956]
[22.698966, -23.827447]
[20.055882, -22.110417]
[-20.991660, 20.998078]
[0.000000, 0.000000]
[10.132326, -10.649835]
[-6.965354, 7.546935]
[-22.249240, 22.323302]
[7.790457, -7.556217]
[-25.858249, 26.644235]
Final V Matrix:
[-24.307073, 25.192598]
[-22.283460, 22.751962]
[-20.974241, 19.943473]
[19.346567, -21.196852]
[0.000000, 0.000000]
[-9.963604, 10.153908]
[7.123890, -6.946185]
[20.587355, -22.456596]
[-6.892281, 7.902240]
[24.777630, -25.993846]
 

Self-Attention Matrix: 
-22.283460 22.751962  
-22.283460 22.751962  
-24.282489 25.162948  
-24.307073 25.192598  
-23.295267 23.972280  
-22.283460 22.751962  
-24.287745 25.169287  
-24.307073 25.192598  
-22.283460 22.751962  
-24.307073 25.192598  


Context Matrix (embedding_matrix + self_attention_matrix:
-3.182459 50.214009 
0.109122 42.765964 
17.722752 21.872419 
-23.221267 -13.888079 
-23.295267 23.972280 
-11.998358 31.412475 
-36.103345 23.694850 
-24.569545 -14.895270 
-27.658513 41.948922 
-39.319169 -7.515494 




semi_final_layer_weights[0] = -0.005616
semi_final_layer_weights[1] = 0.005309
semi_final_layer_weights[2] = 0.005534
semi_final_layer_weights[3] = -0.005415
semi_final_layer_weights[4] = 0.005193
semi_final_layer_weights[5] = -0.005223
semi_final_layer_weights[6] = 0.005544
semi_final_layer_weights[7] = -0.005124
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005624
.
.
.
semi_final_layer_weights[511] = -0.005299
semi_final_layer_weights[512] = 0.005487


 Semi Final Layer Node Values: 
-0.002697 
0.232947 
0.224671 
0.206369 
0.008708 
-0.001066 
-0.000632 
0.207328 
0.085112 
0.269005 
.
.
.
0.008701 
-0.000091 
0.009264 
0.009169 
-0.000095 
0.009226 
0.009165 
-0.000095 
0.009464 
-0.000089 
-0.000089 
0.009201 


final_layer_weights[0] = 1.087716
final_layer_weights[1] = 0.876085
final_layer_weights[2] = -0.367108
final_layer_weights[3] = 0.544831
final_layer_weights[4] = 1.012662
final_layer_weights[5] = 0.418377
final_layer_weights[6] = 0.198796
final_layer_weights[7] = 0.824687
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409583
.
.
.
final_layer_weights[1022] = 0.275460
final_layer_weights[1023] = -1.029527


Output Embedding: 0.531862, 0.537796 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.620889 


Updated weights for final layer:
final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011695
final_layer_weights[5] = 0.419349
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825641
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276447
final_layer_weights[1023] = -1.030549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004226
semi_final_layer_weights[5] = -0.004251
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004170
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[510] = -0.004312
semi_final_layer_weights[511] = 0.004465
Updating K Matrix:
k_matrix[0][0] = -0.459592
k_matrix[0][1] = 0.438445
k_matrix[1][0] = -0.454573
k_matrix[1][1] = 0.441627

Updating Q Matrix:
q_matrix[0][0] = 0.423718
q_matrix[0][1] = -0.463619
q_matrix[1][0] = 0.448889
q_matrix[1][1] = -0.450132

Updating V Matrix:
v_matrix[0][0] = -0.438859
v_matrix[0][1] = 0.421871
v_matrix[1][0] = -0.415054
v_matrix[1][1] = 0.453111

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.459592 0.438445 
-0.454573 0.441627 

Q Matrix:
0.423718 -0.463619 
0.448889 -0.450132 

V Matrix:
-0.438859 0.421871 
-0.415054 0.453111 
Final K Matrix:
[-3.148174, 3.116759]
[-16.545550, 16.141112]
[7.634489, -7.148133]
[17.265981, -16.783013]
[14.138810, -13.746156]
[1.584121, -1.493942]
[14.328900, -13.778785]
[0.000000, 0.000000]
[-9.547400, 9.389342]
[-8.207818, 7.903893]
Final Q Matrix:
[3.326581, -3.055851]
[16.588314, -16.313332]
[-6.533495, 7.844188]
[-17.082827, 17.088049]
[-13.999448, 13.990102]
[-1.395818, 1.616282]
[-13.618652, 14.339063]
[0.000000, 0.000000]
[9.853691, -9.333783]
[7.842817, -8.201810]
Final V Matrix:
[-2.735535, 3.400018]
[-14.947862, 16.792635]
[7.612371, -6.400293]
[15.744064, -17.249810]
[12.885762, -14.138343]
[1.553915, -1.376328]
[13.422070, -13.643919]
[0.000000, 0.000000]
[-8.445786, 10.028753]
[-7.661675, 7.865783]
 

Self-Attention Matrix: 
-2.735535 3.400018  
-2.735535 3.400018  
-14.947862 16.792635  
-14.947862 16.792635  
-14.947862 16.792635  
-14.947862 16.792635  
-14.947862 16.792635  
-8.841698 10.096326  
-2.735535 3.400018  
-2.735535 3.400018  


Context Matrix (embedding_matrix + self_attention_matrix:
-9.963697 17.633555 
-11.021137 48.175130 
-48.324205 33.742615 
-13.862056 -22.288042 
-13.706202 -15.566252 
-20.540686 18.962352 
-32.576352 3.094068 
-8.841698 10.096326 
-16.864100 38.687596 
5.973565 12.650879 




semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004226
semi_final_layer_weights[5] = -0.004251
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004170
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[511] = -0.004312
semi_final_layer_weights[512] = 0.004465


 Semi Final Layer Node Values: 
-0.000396 
0.164851 
-0.000612 
0.163714 
-0.001279 
0.002458 
-0.001285 
-0.000094 
0.103387 
-0.000898 
.
.
.
0.009520 
-0.000100 
0.010135 
0.010031 
-0.000104 
0.010094 
0.010027 
-0.000104 
0.010355 
-0.000098 
-0.000097 
0.010067 


final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011695
final_layer_weights[5] = 0.419349
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825641
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276447
final_layer_weights[1023] = -1.030549


Output Embedding: 0.262165, 0.296752 

 Expected Embedding: -37.748848, 10.192697 
 loss: 771.383397 


Updated weights for final layer:
final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001695
final_layer_weights[5] = 0.429349
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835641
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286447
final_layer_weights[1023] = -1.040549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005774
semi_final_layer_weights[5] = 0.005749
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005830
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[510] = 0.005688
semi_final_layer_weights[511] = -0.005535
Updating K Matrix:
k_matrix[0][0] = 0.540408
k_matrix[0][1] = -0.561555
k_matrix[1][0] = 0.545427
k_matrix[1][1] = -0.558373

Updating Q Matrix:
q_matrix[0][0] = -0.576282
q_matrix[0][1] = 0.536381
q_matrix[1][0] = -0.551111
q_matrix[1][1] = 0.549868

Updating V Matrix:
v_matrix[0][0] = 0.561141
v_matrix[0][1] = -0.578129
v_matrix[1][0] = 0.584946
v_matrix[1][1] = -0.546889

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.540408 -0.561555 
0.545427 -0.558373 

Q Matrix:
-0.576282 0.536381 
-0.551111 0.549868 

V Matrix:
0.561141 -0.578129 
0.584946 -0.546889 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005774
semi_final_layer_weights[5] = 0.005749
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005830
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[511] = 0.005688
semi_final_layer_weights[512] = -0.005535


 Semi Final Layer Node Values: 
0.005430 
-0.000057 
-0.000055 
0.005593 
-0.000058 
0.005749 
-0.000055 
0.005830 
-0.000055 
0.005424 
.
.
.
-0.000058 
0.005574 
-0.000055 
-0.000056 
0.005381 
-0.000055 
-0.000056 
0.005388 
-0.000054 
0.005662 
0.005688 
-0.000055 


final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001695
final_layer_weights[5] = 0.429349
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835641
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286447
final_layer_weights[1023] = -1.040549


Output Embedding: -0.035060, 0.008860 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.606162 


Updated weights for final layer:
final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011695
final_layer_weights[5] = 0.419349
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825641
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276447
final_layer_weights[1023] = -1.030549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004226
semi_final_layer_weights[5] = -0.004251
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004170
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[510] = -0.004312
semi_final_layer_weights[511] = 0.004465
Updating K Matrix:
k_matrix[0][0] = -0.459592
k_matrix[0][1] = 0.438445
k_matrix[1][0] = -0.454573
k_matrix[1][1] = 0.441627

Updating Q Matrix:
q_matrix[0][0] = 0.423718
q_matrix[0][1] = -0.463619
q_matrix[1][0] = 0.448889
q_matrix[1][1] = -0.450132

Updating V Matrix:
v_matrix[0][0] = -0.438859
v_matrix[0][1] = 0.421871
v_matrix[1][0] = -0.415054
v_matrix[1][1] = 0.453111

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 11  total loss: 5056.233525 ******************************************************************* 

Epoch: 12
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.459592 0.438445 
-0.454573 0.441627 

Q Matrix:
0.423718 -0.463619 
0.448889 -0.450132 

V Matrix:
-0.438859 0.421871 
-0.415054 0.453111 
Final K Matrix:
[-11.016730, 10.472329]
[-6.804576, 6.399505]
[-16.913086, 16.377830]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.016571, -11.152922]
[5.929513, -6.961441]
[16.501264, -16.804497]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-10.609252, 9.943829]
[-6.717071, 5.832356]
[-15.570567, 16.617660]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-6.717071 5.832356  
-6.717071 5.832356  
-6.717071 5.832356  
-8.663161 7.888093  
-8.663161 7.888093  
-8.663161 7.888093  
-8.663161 7.888093  
-8.663161 7.888093  
-8.663161 7.888093  
-8.663161 7.888093  


Context Matrix (embedding_matrix + self_attention_matrix:
21.908742 1.125842 
19.504493 -5.709548 
-0.066431 36.314847 
-8.663161 7.888093 
-8.663161 7.888093 
-8.663161 7.888093 
-8.663161 7.888093 
-8.663161 7.888093 
-8.663161 7.888093 
-8.663161 7.888093 




semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004226
semi_final_layer_weights[5] = -0.004251
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004170
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[511] = -0.004312
semi_final_layer_weights[512] = 0.004465


 Semi Final Layer Node Values: 
-0.001098 
0.055283 
0.167762 
-0.000010 
0.000951 
-0.000010 
0.001015 
-0.000009 
0.001019 
-0.000010 
.
.
.
0.000950 
-0.000010 
0.001011 
0.001001 
-0.000010 
0.001007 
0.001000 
-0.000010 
0.001033 
-0.000010 
-0.000010 
0.001004 


final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011695
final_layer_weights[5] = 0.419349
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825641
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276447
final_layer_weights[1023] = -1.030549


Output Embedding: 0.005244, 0.008978 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.544321 


Updated weights for final layer:
final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[510] = 0.005672
semi_final_layer_weights[511] = -0.005535
Updating K Matrix:
k_matrix[0][0] = 0.540408
k_matrix[0][1] = -0.561555
k_matrix[1][0] = 0.545427
k_matrix[1][1] = -0.558373

Updating Q Matrix:
q_matrix[0][0] = -0.557377
q_matrix[0][1] = 0.536381
q_matrix[1][0] = -0.551111
q_matrix[1][1] = 0.549868

Updating V Matrix:
v_matrix[0][0] = 0.561141
v_matrix[0][1] = -0.554947
v_matrix[1][0] = 0.545980
v_matrix[1][1] = -0.546889

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.540408 -0.561555 
0.545427 -0.558373 

Q Matrix:
-0.557377 0.536381 
-0.551111 0.549868 

V Matrix:
0.561141 -0.554947 
0.545980 -0.546889 
Final K Matrix:
[12.896389, -13.379775]
[7.828052, -8.023095]
[20.220045, -20.755300]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-13.272372, 12.821804]
[-7.922216, 7.882428]
[-20.506138, 20.328633]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[13.339583, -13.209911]
[7.858404, -7.863576]
[20.374761, -20.361298]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.858404 -7.863576  
7.858404 -7.863576  
7.858404 -7.863576  
10.598993 -10.536744  
10.598993 -10.536744  
10.598993 -10.536744  
10.598993 -10.536744  
10.598993 -10.536744  
10.598993 -10.536744  
10.598993 -10.536744  


Context Matrix (embedding_matrix + self_attention_matrix:
29.168127 -5.332651 
8.969391 5.387806 
14.509044 22.618914 
10.598993 -10.536744 
10.598993 -10.536744 
10.598993 -10.536744 
10.598993 -10.536744 
10.598993 -10.536744 
10.598993 -10.536744 
10.598993 -10.536744 




semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[511] = 0.005672
semi_final_layer_weights[512] = -0.005535


 Semi Final Layer Node Values: 
0.123990 
-0.000872 
-0.002096 
-0.000052 
0.005213 
-0.000052 
0.005147 
-0.000051 
0.005130 
-0.000051 
.
.
.
0.005208 
-0.000052 
0.005162 
0.005205 
-0.000050 
0.005179 
0.005207 
-0.000051 
0.005071 
-0.000053 
-0.000053 
0.005190 


final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Output Embedding: 0.140066, 0.133540 

 Expected Embedding: 12.470385, -17.532656 
 loss: 232.065612 


Updated weights for final layer:
final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[510] = -0.004328
semi_final_layer_weights[511] = 0.004465
Updating K Matrix:
k_matrix[0][0] = -0.459592
k_matrix[0][1] = 0.438445
k_matrix[1][0] = -0.454573
k_matrix[1][1] = 0.441627

Updating Q Matrix:
q_matrix[0][0] = 0.442623
q_matrix[0][1] = -0.463619
q_matrix[1][0] = 0.448889
q_matrix[1][1] = -0.450132

Updating V Matrix:
v_matrix[0][0] = -0.438859
v_matrix[0][1] = 0.445053
v_matrix[1][0] = -0.454020
v_matrix[1][1] = 0.453111

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.459592 0.438445 
-0.454573 0.441627 

Q Matrix:
0.442623 -0.463619 
0.448889 -0.450132 

V Matrix:
-0.438859 0.445053 
-0.454020 0.453111 
Final K Matrix:
[3.168946, -3.007118]
[-18.429459, 17.540232]
[5.797092, -5.683843]
[-5.371522, 5.053405]
[-22.407212, 21.372009]
[-20.524909, 19.778716]
[11.100923, -10.669684]
[19.873142, -19.008851]
[-23.995779, 23.057078]
[15.478639, -15.026947]
Final Q Matrix:
[-3.029629, 3.213652]
[17.691541, -18.634583]
[-5.796826, 5.685651]
[5.074355, -5.493612]
[21.574025, -22.608036]
[20.043128, -20.495280]
[-10.801828, 11.114136]
[-19.209169, 19.994342]
[23.340152, -24.031250]
[-15.269969, 15.338915]
Final V Matrix:
[2.985882, -3.042727]
[-17.494787, 17.779557]
[5.919856, -5.862490]
[-4.951533, 5.086561]
[-21.385850, 21.691522]
[-20.095213, 20.196852]
[10.799272, -10.878659]
[19.102083, -19.325661]
[-23.327367, 23.504739]
[15.432602, -15.411232]
 

Self-Attention Matrix: 
-17.494787 17.779557  
2.985882 -3.042727  
-17.494787 17.779557  
2.985882 -3.042727  
2.985882 -3.042727  
2.985882 -3.042727  
-17.494787 17.779557  
-17.494787 17.779557  
2.985882 -3.042727  
-17.494787 17.779557  


Context Matrix (embedding_matrix + self_attention_matrix:
-26.378266 19.789855 
48.207036 -8.220784 
-11.060384 -1.478727 
23.480956 -11.947445 
52.264649 -3.572665 
23.049527 21.824201 
-31.778881 7.800793 
-54.518804 11.494061 
34.673535 17.707311 
-18.844646 -14.906646 




semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[511] = -0.004328
semi_final_layer_weights[512] = 0.004465


 Semi Final Layer Node Values: 
0.025541 
0.168447 
-0.000610 
-0.000464 
0.211808 
-0.002022 
-0.001037 
0.189748 
0.241808 
0.159038 
.
.
.
0.004953 
-0.000049 
0.005008 
0.004956 
-0.000051 
0.004987 
0.004954 
-0.000051 
0.005116 
-0.000048 
-0.000048 
0.004974 


final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Output Embedding: 0.806419, -0.187024 

 Expected Embedding: 19.465172, 25.668072 
 loss: 508.317513 


Updated weights for final layer:
final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[510] = 0.005672
semi_final_layer_weights[511] = -0.005535
Updating K Matrix:
k_matrix[0][0] = 0.540408
k_matrix[0][1] = -0.561555
k_matrix[1][0] = 0.545427
k_matrix[1][1] = -0.558373

Updating Q Matrix:
q_matrix[0][0] = -0.557377
q_matrix[0][1] = 0.536381
q_matrix[1][0] = -0.551111
q_matrix[1][1] = 0.549868

Updating V Matrix:
v_matrix[0][0] = 0.561141
v_matrix[0][1] = -0.554947
v_matrix[1][0] = 0.545980
v_matrix[1][1] = -0.546889

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.540408 -0.561555 
0.545427 -0.558373 

Q Matrix:
-0.557377 0.536381 
-0.551111 0.549868 

V Matrix:
0.561141 -0.554947 
0.545980 -0.546889 
Final K Matrix:
[20.016038, -20.539451]
[2.259334, -2.290869]
[14.396027, -14.832671]
[35.791493, -36.889936]
[20.461365, -20.987971]
[18.296225, -18.841690]
[-5.171412, 5.378924]
[12.946258, -13.338573]
[-8.531318, 8.913070]
[10.302441, -10.625612]
Final Q Matrix:
[-20.290465, 20.130057]
[-2.252771, 2.300108]
[-14.675430, 14.417058]
[-36.503667, 35.830758]
[-20.730380, 20.586482]
[-18.638413, 18.332551]
[5.340816, -5.127650]
[-13.197038, 12.965533]
[8.864502, -8.419196]
[-10.516961, 10.306665]
Final V Matrix:
[20.153563, -20.145729]
[2.208026, -2.230630]
[14.640977, -14.584015]
[36.431679, -36.279098]
[20.581445, -20.580643]
[18.584549, -18.520235]
[-5.382310, 5.318651]
[13.165674, -13.114753]
[-8.974872, 8.836241]
[10.503671, -10.453792]
 

Self-Attention Matrix: 
2.208026 -2.230630  
2.208026 -2.230630  
2.208026 -2.230630  
2.208026 -2.230630  
2.208026 -2.230630  
2.208026 -2.230630  
20.153563 -20.145729  
2.208026 -2.230630  
20.153563 -20.145729  
2.208026 -2.230630  


Context Matrix (embedding_matrix + self_attention_matrix:
8.017195 28.711558 
-0.447209 4.542490 
13.621124 12.855326 
32.128589 33.745142 
7.130582 30.406485 
15.574345 18.070797 
9.964684 -19.532001 
12.428870 11.378567 
-1.392940 -14.439024 
11.660502 7.292625 




semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[511] = 0.005672
semi_final_layer_weights[512] = -0.005535


 Semi Final Layer Node Values: 
0.204857 
-0.000289 
-0.001510 
0.374037 
-0.002142 
0.193719 
0.057996 
0.136065 
0.092073 
0.108216 
.
.
.
0.005595 
-0.000056 
0.005545 
0.005592 
-0.000054 
0.005564 
0.005594 
-0.000054 
0.005447 
-0.000057 
-0.000057 
0.005576 


final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Output Embedding: 0.470569, 0.788526 

 Expected Embedding: 4.643339, -27.140186 
 loss: 398.712481 


Updated weights for final layer:
final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[510] = -0.004328
semi_final_layer_weights[511] = 0.004465
Updating K Matrix:
k_matrix[0][0] = -0.459592
k_matrix[0][1] = 0.438445
k_matrix[1][0] = -0.454573
k_matrix[1][1] = 0.441627

Updating Q Matrix:
q_matrix[0][0] = 0.442623
q_matrix[0][1] = -0.463619
q_matrix[1][0] = 0.448889
q_matrix[1][1] = -0.450132

Updating V Matrix:
v_matrix[0][0] = -0.438859
v_matrix[0][1] = 0.445053
v_matrix[1][0] = -0.454020
v_matrix[1][1] = 0.453111

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.459592 0.438445 
-0.454573 0.441627 

Q Matrix:
0.442623 -0.463619 
0.448889 -0.450132 

V Matrix:
-0.438859 0.445053 
-0.454020 0.453111 
Final K Matrix:
[-9.207477, 8.867207]
[10.096492, -9.696585]
[17.124236, -16.644048]
[-16.296366, 15.863171]
[-17.405868, 16.571158]
[10.215484, -9.919924]
[-11.085523, 10.683499]
[-29.027344, 27.970451]
[-20.559162, 19.980532]
[4.563660, -4.342580]
Final Q Matrix:
[8.983651, -9.200029]
[-9.813752, 10.116637]
[-16.920576, 16.949018]
[16.135671, -16.104482]
[16.716078, -17.594178]
[-10.081312, 10.120580]
[10.826700, -11.068467]
[28.343773, -28.987122]
[20.311695, -20.351040]
[-4.379702, 4.615387]
Final V Matrix:
[-9.000875, 9.051350]
[9.802886, -9.881877]
[17.122189, -17.081312]
[-16.353952, 16.294062]
[-16.535948, 16.800375]
[10.191472, -10.175122]
[-10.855938, 10.909954]
[-28.415667, 28.560814]
[-20.551392, 20.504189]
[4.330000, -4.401296]
 

Self-Attention Matrix: 
9.802886 -9.881877  
-9.000875 9.051350  
-9.000875 9.051350  
9.802886 -9.881877  
9.802886 -9.881877  
-9.000875 9.051350  
9.802886 -9.881877  
9.802886 -9.881877  
9.802886 -9.881877  
-9.000875 9.051350  


Context Matrix (embedding_matrix + self_attention_matrix:
19.489517 0.579780 
-22.946663 0.940161 
-8.071093 -29.559761 
5.966950 29.846269 
51.875953 -14.128855 
-9.575930 -12.839963 
20.515634 3.673785 
38.375171 25.086705 
8.949907 36.207967 
-20.308404 10.444275 




semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[511] = -0.004328
semi_final_layer_weights[512] = 0.004465


 Semi Final Layer Node Values: 
-0.000963 
-0.000908 
-0.001740 
-0.001622 
0.163200 
0.103228 
0.113648 
-0.002911 
0.209089 
0.040566 
.
.
.
-0.000045 
0.004489 
-0.000046 
-0.000045 
0.004684 
-0.000045 
-0.000045 
0.004678 
-0.000047 
0.004400 
0.004389 
-0.000045 


final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Output Embedding: 0.334674, 0.080822 

 Expected Embedding: 44.423141, -6.847703 
 loss: 995.898711 


Updated weights for final layer:
final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[510] = 0.005672
semi_final_layer_weights[511] = -0.005535
Updating K Matrix:
k_matrix[0][0] = 0.540408
k_matrix[0][1] = -0.561555
k_matrix[1][0] = 0.545427
k_matrix[1][1] = -0.558373

Updating Q Matrix:
q_matrix[0][0] = -0.557377
q_matrix[0][1] = 0.536381
q_matrix[1][0] = -0.551111
q_matrix[1][1] = 0.549868

Updating V Matrix:
v_matrix[0][0] = 0.561141
v_matrix[0][1] = -0.554947
v_matrix[1][0] = 0.545980
v_matrix[1][1] = -0.546889

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.540408 -0.561555 
0.545427 -0.558373 

Q Matrix:
-0.557377 0.536381 
-0.551111 0.549868 

V Matrix:
0.561141 -0.554947 
0.545980 -0.546889 
Final K Matrix:
[-26.079031, 26.814078]
[-0.444591, 0.516742]
[-17.758855, 18.418260]
[-4.130068, 4.305739]
[-6.461328, 6.706599]
[-17.134856, 17.660674]
[-15.155177, 15.532932]
[-18.920522, 19.630630]
[4.688415, -4.823299]
[-16.333259, 16.645277]
Final Q Matrix:
[26.508932, -26.173808]
[0.533168, -0.385812]
[18.268097, -17.662469]
[4.278913, -4.085045]
[6.653907, -6.420834]
[17.475733, -17.153706]
[15.337684, -15.260295]
[19.473391, -18.810163]
[-4.769435, 4.702678]
[16.400353, -16.542884]
Final V Matrix:
[-26.387018, 26.331541]
[-0.594491, 0.542706]
[-18.354015, 18.180772]
[-4.322627, 4.263306]
[-6.690862, 6.623263]
[-17.441214, 17.368211]
[-15.214306, 15.224190]
[-19.572979, 19.381948]
[4.750431, -4.738121]
[-16.166251, 16.257998]
 

Self-Attention Matrix: 
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-0.594491 0.542706  
-26.387018 26.331541  
-0.594491 0.542706  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.544516 -33.449577 
-7.998496 7.063457 
-29.188948 -3.685484 
-9.926995 2.217164 
-11.641914 -0.357886 
-14.912488 -16.686519 
-2.763579 -25.094044 
-31.969256 -3.060589 
-23.549832 32.116319 
8.498706 -38.412620 




semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[511] = 0.005672
semi_final_layer_weights[512] = -0.005535


 Semi Final Layer Node Values: 
-0.002660 
-0.000004 
0.186178 
-0.000375 
0.072264 
-0.001823 
0.158378 
-0.001976 
-0.000523 
-0.001677 
.
.
.
-0.000053 
0.005275 
-0.000052 
-0.000053 
0.005093 
-0.000052 
-0.000053 
0.005099 
-0.000051 
0.005358 
0.005368 
-0.000052 


final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Output Embedding: -0.024437, 0.045604 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1061.928449 


Updated weights for final layer:
final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[510] = -0.004328
semi_final_layer_weights[511] = 0.004465
Updating K Matrix:
k_matrix[0][0] = -0.459592
k_matrix[0][1] = 0.438445
k_matrix[1][0] = -0.454573
k_matrix[1][1] = 0.441627

Updating Q Matrix:
q_matrix[0][0] = 0.442623
q_matrix[0][1] = -0.463619
q_matrix[1][0] = 0.448889
q_matrix[1][1] = -0.450132

Updating V Matrix:
v_matrix[0][0] = -0.438859
v_matrix[0][1] = 0.445053
v_matrix[1][0] = -0.454020
v_matrix[1][1] = 0.453111

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.459592 0.438445 
-0.454573 0.441627 

Q Matrix:
0.442623 -0.463619 
0.448889 -0.450132 

V Matrix:
-0.438859 0.445053 
-0.454020 0.453111 
Final K Matrix:
[-12.872120, 12.501132]
[-3.312867, 3.272898]
[5.083084, -4.848761]
[17.265981, -16.783013]
[-23.729350, 22.867527]
[-18.900811, 18.106212]
[-14.842796, 14.384312]
[5.478561, -5.110255]
[4.663522, -4.441376]
[25.465011, -24.405870]
Final Q Matrix:
[12.705054, -12.751019]
[3.347187, -3.223018]
[-4.894790, 5.128099]
[-17.062300, 17.088049]
[23.173563, -23.694176]
[18.307495, -18.987112]
[14.607395, -14.735605]
[-5.114397, 5.649438]
[-4.480792, 4.712393]
[-24.681543, 25.569211]
Final V Matrix:
[-12.845456, 12.823588]
[-3.444989, 3.390359]
[4.852672, -4.921561]
[17.266909, -17.224639]
[-23.234746, 23.351486]
[-18.236093, 18.424540]
[-14.735189, 14.737072]
[4.940415, -5.116842]
[4.434193, -4.503719]
[24.598029, -24.841843]
 

Self-Attention Matrix: 
-3.444989 3.390359  
-3.444989 3.390359  
-12.845456 12.823588  
-12.845456 12.823588  
-3.444989 3.390359  
-3.444989 3.390359  
-3.444989 3.390359  
-12.845456 12.823588  
-12.845456 12.823588  
-12.845456 12.823588  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.898461 31.154765 
-10.193726 17.501478 
-23.960706 12.879446 
-11.759650 -26.257089 
19.643136 32.248761 
28.366366 12.807070 
0.995110 31.553431 
-39.191075 27.407975 
-23.932258 13.773665 
-54.287530 -1.296446 




semi_final_layer_weights[0] = -0.004570
semi_final_layer_weights[1] = 0.004321
semi_final_layer_weights[2] = 0.004504
semi_final_layer_weights[3] = -0.004407
semi_final_layer_weights[4] = 0.004441
semi_final_layer_weights[5] = -0.004408
semi_final_layer_weights[6] = 0.004512
semi_final_layer_weights[7] = -0.004515
semi_final_layer_weights[8] = 0.004530
semi_final_layer_weights[9] = -0.004576
.
.
.
semi_final_layer_weights[511] = -0.004328
semi_final_layer_weights[512] = 0.004465


 Semi Final Layer Node Values: 
-0.001337 
0.035895 
-0.000454 
0.171940 
0.234901 
-0.001859 
0.151362 
0.048687 
-0.000415 
0.258955 
.
.
.
0.004276 
-0.000043 
0.004323 
0.004279 
-0.000044 
0.004306 
0.004277 
-0.000044 
0.004417 
-0.000042 
-0.000042 
0.004294 


final_layer_weights[0] = 1.088762
final_layer_weights[1] = 0.875097
final_layer_weights[2] = -0.368138
final_layer_weights[3] = 0.545839
final_layer_weights[4] = 1.011910
final_layer_weights[5] = 0.419192
final_layer_weights[6] = 0.197764
final_layer_weights[7] = 0.825296
final_layer_weights[8] = 1.063183
final_layer_weights[9] = 1.410631
.
.
.
final_layer_weights[1022] = 0.276431
final_layer_weights[1023] = -1.030549


Output Embedding: 0.847335, 0.176925 

 Expected Embedding: 1.038754, 36.179375 
 loss: 648.106507 


Updated weights for final layer:
final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[510] = 0.005672
semi_final_layer_weights[511] = -0.005535
Updating K Matrix:
k_matrix[0][0] = 0.540408
k_matrix[0][1] = -0.561555
k_matrix[1][0] = 0.545427
k_matrix[1][1] = -0.558373

Updating Q Matrix:
q_matrix[0][0] = -0.557377
q_matrix[0][1] = 0.536381
q_matrix[1][0] = -0.551111
q_matrix[1][1] = 0.549868

Updating V Matrix:
v_matrix[0][0] = 0.561141
v_matrix[0][1] = -0.554947
v_matrix[1][0] = 0.545980
v_matrix[1][1] = -0.546889

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.540408 -0.561555 
0.545427 -0.558373 

Q Matrix:
-0.557377 0.536381 
-0.551111 0.549868 

V Matrix:
0.561141 -0.554947 
0.545980 -0.546889 
Final K Matrix:
[25.300893, -26.060325]
[23.017323, -23.749939]
[20.905241, -21.750898]
[-20.728890, 21.211858]
[0.000000, 0.000000]
[10.281836, -10.611445]
[-7.189448, 7.458392]
[-22.006860, 22.531378]
[7.565823, -7.700679]
[-25.952550, 26.693431]
Final Q Matrix:
[-25.781087, 25.345924]
[-23.511042, 23.016020]
[-21.599316, 20.721451]
[20.932572, -20.906822]
[0.000000, 0.000000]
[-10.505582, 10.278875]
[7.398323, -7.148409]
[22.239145, -22.183835]
[-7.583717, 7.672725]
[26.393175, -26.037346]
Final V Matrix:
[25.712064, -25.618747]
[23.492622, -23.372146]
[21.774283, -21.511146]
[-20.727962, 20.770232]
[0.000000, 0.000000]
[10.499852, -10.444032]
[-7.435225, 7.363391]
[-22.034441, 22.069277]
[7.464988, -7.515737]
[-26.281847, 26.218624]
 

Self-Attention Matrix: 
23.492622 -23.372146  
23.492622 -23.372146  
23.492622 -23.372146  
25.712064 -25.618747  
24.602343 -24.495446  
23.492622 -23.372146  
25.712064 -25.618747  
25.712064 -25.618747  
23.492622 -23.372146  
25.712064 -25.618747  


Context Matrix (embedding_matrix + self_attention_matrix:
42.593624 4.089902 
45.885204 -3.358144 
65.497864 -26.662675 
26.797870 -64.699424 
24.602343 -24.495446 
33.777725 -14.711633 
13.896464 -27.093184 
25.449592 -65.706615 
18.117569 -4.175185 
10.699968 -58.326839 




semi_final_layer_weights[0] = 0.005430
semi_final_layer_weights[1] = -0.005679
semi_final_layer_weights[2] = -0.005496
semi_final_layer_weights[3] = 0.005593
semi_final_layer_weights[4] = -0.005559
semi_final_layer_weights[5] = 0.005592
semi_final_layer_weights[6] = -0.005488
semi_final_layer_weights[7] = 0.005485
semi_final_layer_weights[8] = -0.005470
semi_final_layer_weights[9] = 0.005424
.
.
.
semi_final_layer_weights[511] = 0.005672
semi_final_layer_weights[512] = -0.005535


 Semi Final Layer Node Values: 
0.258908 
-0.002358 
-0.002079 
-0.002176 
0.004965 
0.101017 
0.077915 
-0.002263 
-0.000708 
-0.002637 
.
.
.
0.004960 
-0.000050 
0.004916 
0.004957 
-0.000048 
0.004933 
0.004959 
-0.000048 
0.004829 
-0.000051 
-0.000051 
0.004943 


final_layer_weights[0] = 1.098762
final_layer_weights[1] = 0.865097
final_layer_weights[2] = -0.378138
final_layer_weights[3] = 0.555839
final_layer_weights[4] = 1.001910
final_layer_weights[5] = 0.429192
final_layer_weights[6] = 0.187764
final_layer_weights[7] = 0.835296
final_layer_weights[8] = 1.053183
final_layer_weights[9] = 1.420631
.
.
.
final_layer_weights[1022] = 0.286431
final_layer_weights[1023] = -1.040549


Output Embedding: 0.765740, 0.505883 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.938698 


Updated weights for final layer:
final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[510] = 0.004598
semi_final_layer_weights[511] = -0.004487
Updating K Matrix:
k_matrix[0][0] = 0.438062
k_matrix[0][1] = -0.455204
k_matrix[1][0] = 0.442130
k_matrix[1][1] = -0.452624

Updating Q Matrix:
q_matrix[0][0] = -0.451817
q_matrix[0][1] = 0.434797
q_matrix[1][0] = -0.446737
q_matrix[1][1] = 0.445730

Updating V Matrix:
v_matrix[0][0] = 0.454868
v_matrix[0][1] = -0.449848
v_matrix[1][0] = 0.442578
v_matrix[1][1] = -0.443315

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
0.438062 -0.455204 
0.442130 -0.452624 

Q Matrix:
-0.451817 0.434797 
-0.446737 0.445730 

V Matrix:
0.454868 -0.449848 
0.442578 -0.443315 
Final K Matrix:
[3.126697, -3.152162]
[16.166834, -16.494676]
[-7.126808, 7.521054]
[-16.803108, 17.194608]
[-13.762926, 14.081216]
[-1.490706, 1.563807]
[-13.778927, 14.224858]
[0.000000, 0.000000]
[9.412526, -9.540649]
[7.905214, -8.151579]
Final Q Matrix:
[-3.092846, 3.201535]
[-16.259141, 16.355073]
[7.507814, -6.956823]
[16.968215, -16.947342]
[13.894922, -13.883471]
[1.557640, -1.464636]
[14.084517, -13.770689]
[0.000000, 0.000000]
[-9.380754, 9.585686]
[-8.067627, 7.910084]
Final V Matrix:
[3.011594, -3.058375]
[16.047633, -16.122241]
[-7.680134, 7.500080]
[-16.802356, 16.836621]
[-13.756545, 13.786637]
[-1.583727, 1.554050]
[-14.081320, 14.002920]
[0.000000, 0.000000]
[9.190882, -9.287828]
[8.055718, -8.018817]
 

Self-Attention Matrix: 
3.011594 -3.058375  
3.011594 -3.058375  
16.047633 -16.122241  
16.047633 -16.122241  
16.047633 -16.122241  
16.047633 -16.122241  
16.047633 -16.122241  
9.529614 -9.590308  
3.011594 -3.058375  
3.011594 -3.058375  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.216568 11.175162 
-5.274008 41.716737 
-17.328710 0.827739 
17.133439 -55.202918 
17.289293 -48.481127 
10.454809 -13.952524 
-1.580857 -29.820807 
9.529614 -9.590308 
-11.116971 32.229203 
11.720694 6.192486 




semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[511] = 0.004598
semi_final_layer_weights[512] = -0.004487


 Semi Final Layer Node Values: 
0.035029 
-0.001724 
0.069060 
-0.001771 
0.145059 
-0.000204 
0.144150 
-0.000047 
-0.000980 
0.083149 
.
.
.
0.004775 
-0.000048 
0.004733 
0.004773 
-0.000046 
0.004749 
0.004774 
-0.000046 
0.004649 
-0.000049 
-0.000049 
0.004759 


final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Output Embedding: 0.696463, 0.198234 

 Expected Embedding: -37.748848, 10.192697 
 loss: 788.965599 


Updated weights for final layer:
final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[510] = -0.005402
semi_final_layer_weights[511] = 0.005513
Updating K Matrix:
k_matrix[0][0] = -0.561938
k_matrix[0][1] = 0.544796
k_matrix[1][0] = -0.557870
k_matrix[1][1] = 0.547376

Updating Q Matrix:
q_matrix[0][0] = 0.548183
q_matrix[0][1] = -0.565203
q_matrix[1][0] = 0.553263
q_matrix[1][1] = -0.554270

Updating V Matrix:
v_matrix[0][0] = -0.545132
v_matrix[0][1] = 0.550152
v_matrix[1][0] = -0.557422
v_matrix[1][1] = 0.556685

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.561938 0.544796 
-0.557870 0.547376 

Q Matrix:
0.548183 -0.565203 
0.553263 -0.554270 

V Matrix:
-0.545132 0.550152 
-0.557422 0.556685 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[511] = -0.005402
semi_final_layer_weights[512] = 0.005513


 Semi Final Layer Node Values: 
-0.000056 
0.005396 
0.005545 
-0.000055 
0.005494 
-0.000055 
0.005551 
-0.000056 
0.005566 
-0.000056 
.
.
.
0.005498 
-0.000055 
0.005538 
0.005501 
-0.000056 
0.005523 
0.005499 
-0.000056 
0.005617 
-0.000054 
-0.000054 
0.005513 


final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Output Embedding: 0.075991, 0.045459 

 Expected Embedding: -10.885829, -17.296432 
 loss: 210.451351 


Updated weights for final layer:
final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[510] = 0.004598
semi_final_layer_weights[511] = -0.004487
Updating K Matrix:
k_matrix[0][0] = 0.438062
k_matrix[0][1] = -0.455204
k_matrix[1][0] = 0.442130
k_matrix[1][1] = -0.452624

Updating Q Matrix:
q_matrix[0][0] = -0.451817
q_matrix[0][1] = 0.434797
q_matrix[1][0] = -0.446737
q_matrix[1][1] = 0.445730

Updating V Matrix:
v_matrix[0][0] = 0.454868
v_matrix[0][1] = -0.449848
v_matrix[1][0] = 0.442578
v_matrix[1][1] = -0.443315

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 12  total loss: 5094.929243 ******************************************************************* 

Epoch: 13
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.438062 -0.455204 
0.442130 -0.452624 

Q Matrix:
-0.451817 0.434797 
-0.446737 0.445730 

V Matrix:
0.454868 -0.449848 
0.442578 -0.443315 
Final K Matrix:
[10.458990, -10.900288]
[6.383645, -6.712000]
[16.390631, -16.824516]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-10.831057, 10.348590]
[-6.691152, 6.256488]
[-16.622542, 16.478655]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[10.937961, -10.790783]
[6.819151, -6.679003]
[16.516047, -16.505133]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
6.819151 -6.679003  
6.819151 -6.679003  
6.819151 -6.679003  
8.878556 -8.734893  
8.878556 -8.734893  
8.878556 -8.734893  
8.878556 -8.734893  
8.878556 -8.734893  
8.878556 -8.734893  
8.878556 -8.734893  


Context Matrix (embedding_matrix + self_attention_matrix:
35.444963 -11.385517 
33.040714 -18.220907 
13.469790 23.803488 
8.878556 -8.734893 
8.878556 -8.734893 
8.878556 -8.734893 
8.878556 -8.734893 
8.878556 -8.734893 
8.878556 -8.734893 
8.878556 -8.734893 




semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[511] = 0.004598
semi_final_layer_weights[512] = -0.004487


 Semi Final Layer Node Values: 
0.101494 
-0.000636 
-0.001705 
-0.000039 
0.003859 
-0.000039 
0.003810 
-0.000038 
0.003797 
-0.000038 
.
.
.
0.003855 
-0.000039 
0.003821 
0.003853 
-0.000037 
0.003834 
0.003854 
-0.000037 
0.003754 
-0.000039 
-0.000039 
0.003842 


final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Output Embedding: 0.107717, 0.104737 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.956596 


Updated weights for final layer:
final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[510] = -0.005402
semi_final_layer_weights[511] = 0.005513
Updating K Matrix:
k_matrix[0][0] = -0.561938
k_matrix[0][1] = 0.544796
k_matrix[1][0] = -0.557870
k_matrix[1][1] = 0.547376

Updating Q Matrix:
q_matrix[0][0] = 0.548183
q_matrix[0][1] = -0.565203
q_matrix[1][0] = 0.553263
q_matrix[1][1] = -0.554270

Updating V Matrix:
v_matrix[0][0] = -0.545132
v_matrix[0][1] = 0.550152
v_matrix[1][0] = -0.557422
v_matrix[1][1] = 0.556685

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.561938 0.544796 
-0.557870 0.547376 

Q Matrix:
0.548183 -0.565203 
0.553263 -0.554270 

V Matrix:
-0.545132 0.550152 
-0.557422 0.556685 
Final K Matrix:
[-13.386667, 12.994828]
[-8.016848, 7.858744]
[-20.742499, 20.308614]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[13.081890, -13.447127]
[7.940518, -7.972770]
[20.510588, -20.654476]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-13.027408, 13.132522]
[-7.992245, 7.988052]
[-20.617083, 20.627997]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-7.992245 7.988052  
-7.992245 7.988052  
-7.992245 7.988052  
-10.509826 10.560287  
-10.509826 10.560287  
-10.509826 10.560287  
-10.509826 10.560287  
-10.509826 10.560287  
-10.509826 10.560287  
-10.509826 10.560287  


Context Matrix (embedding_matrix + self_attention_matrix:
13.317478 10.518977 
-6.881257 21.239434 
-1.341605 38.470542 
-10.509826 10.560287 
-10.509826 10.560287 
-10.509826 10.560287 
-10.509826 10.560287 
-10.509826 10.560287 
-10.509826 10.560287 
-10.509826 10.560287 




semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[511] = -0.005402
semi_final_layer_weights[512] = 0.005513


 Semi Final Layer Node Values: 
-0.001390 
0.082877 
0.211416 
-0.000057 
0.005771 
-0.000057 
0.005831 
-0.000058 
0.005847 
-0.000059 
.
.
.
0.005775 
-0.000058 
0.005817 
0.005778 
-0.000059 
0.005802 
0.005776 
-0.000059 
0.005900 
-0.000057 
-0.000057 
0.005792 


final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Output Embedding: 0.074666, 0.048304 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.371989 


Updated weights for final layer:
final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[510] = 0.004598
semi_final_layer_weights[511] = -0.004487
Updating K Matrix:
k_matrix[0][0] = 0.438062
k_matrix[0][1] = -0.455204
k_matrix[1][0] = 0.442130
k_matrix[1][1] = -0.452624

Updating Q Matrix:
q_matrix[0][0] = -0.451817
q_matrix[0][1] = 0.434797
q_matrix[1][0] = -0.446737
q_matrix[1][1] = 0.445730

Updating V Matrix:
v_matrix[0][0] = 0.454868
v_matrix[0][1] = -0.449848
v_matrix[1][0] = 0.442578
v_matrix[1][1] = -0.443315

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
0.438062 -0.455204 
0.442130 -0.452624 

Q Matrix:
-0.451817 0.434797 
-0.446737 0.445730 

V Matrix:
0.454868 -0.449848 
0.442578 -0.443315 
Final K Matrix:
[-3.002701, 3.133881]
[17.520296, -18.241114]
[-5.696006, 5.787808]
[5.041067, -5.298937]
[21.352858, -22.192007]
[19.783549, -20.388423]
[-10.669235, 11.018803]
[-18.997828, 19.698434]
[23.055383, -23.816307]
[-15.042889, 15.409036]
Final Q Matrix:
[3.115633, -2.966462]
[-18.118461, 17.354019]
[5.696222, -5.786342]
[-5.281955, 4.942100]
[-22.028250, 21.190067]
[-20.174086, 19.807566]
[10.911686, -10.658525]
[19.536053, -18.899581]
[-23.586844, 23.026631]
[15.212039, -15.156151]
Final V Matrix:
[-3.151095, 3.105016]
[18.277953, -18.047115]
[-5.596493, 5.642994]
[5.381516, -5.272061]
[22.180787, -21.933006]
[20.131866, -20.049476]
[-10.913758, 10.849405]
[-19.622858, 19.441623]
[23.597208, -23.453427]
[-15.080207, 15.097529]
 

Self-Attention Matrix: 
18.277953 -18.047115  
-3.151095 3.105016  
18.277953 -18.047115  
-3.151095 3.105016  
-3.151095 3.105016  
-3.151095 3.105016  
18.277953 -18.047115  
18.277953 -18.047115  
-3.151095 3.105016  
18.277953 -18.047115  


Context Matrix (embedding_matrix + self_attention_matrix:
9.394474 -16.036816 
42.070058 -2.073042 
24.712356 -37.305398 
17.343979 -5.799702 
46.127672 2.575078 
16.912550 27.971944 
3.993859 -28.025878 
-18.746064 -24.332611 
28.536558 23.855054 
16.928094 -50.733318 




semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[511] = 0.004598
semi_final_layer_weights[512] = -0.004487


 Semi Final Layer Node Values: 
-0.000336 
-0.001795 
0.060560 
0.047807 
-0.002240 
0.207974 
0.111364 
-0.001960 
-0.002367 
-0.001530 
.
.
.
0.004086 
-0.000041 
0.004050 
0.004084 
-0.000040 
0.004063 
0.004085 
-0.000040 
0.003978 
-0.000042 
-0.000042 
0.004072 


final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Output Embedding: -0.138582, 0.000639 

 Expected Embedding: 19.465172, 25.668072 
 loss: 521.562135 


Updated weights for final layer:
final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[510] = -0.005402
semi_final_layer_weights[511] = 0.005513
Updating K Matrix:
k_matrix[0][0] = -0.561938
k_matrix[0][1] = 0.544796
k_matrix[1][0] = -0.557870
k_matrix[1][1] = 0.547376

Updating Q Matrix:
q_matrix[0][0] = 0.548183
q_matrix[0][1] = -0.565203
q_matrix[1][0] = 0.553263
q_matrix[1][1] = -0.554270

Updating V Matrix:
v_matrix[0][0] = -0.545132
v_matrix[0][1] = 0.550152
v_matrix[1][0] = -0.557422
v_matrix[1][1] = 0.556685

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
-0.561938 0.544796 
-0.557870 0.547376 

Q Matrix:
0.548183 -0.565203 
0.553263 -0.554270 

V Matrix:
-0.545132 0.550152 
-0.557422 0.556685 
Final K Matrix:
[-20.526096, 20.101811]
[-2.286441, 2.260878]
[-14.829448, 14.475499]
[-36.883286, 35.992874]
[-20.973423, 20.546549]
[-18.836590, 18.394428]
[5.383137, -5.214925]
[-13.335636, 13.017621]
[8.924200, -8.614747]
[-10.624439, 10.362472]
Final Q Matrix:
[20.303642, -20.433671]
[2.291760, -2.253389]
[14.602960, -14.812400]
[36.305988, -36.851457]
[20.755356, -20.872001]
[18.559207, -18.807143]
[-5.245816, 5.418611]
[13.132351, -13.320012]
[-8.654117, 9.015088]
[10.450546, -10.621015]
Final V Matrix:
[-20.414616, 20.420967]
[-2.328032, 2.309708]
[-14.630889, 14.677062]
[-36.364343, 36.488027]
[-20.876085, 20.876734]
[-18.602870, 18.655004]
[5.212180, -5.263783]
[-13.157775, 13.199052]
[8.564650, -8.677025]
[-10.461320, 10.501752]
 

Self-Attention Matrix: 
-2.328032 2.309708  
-2.328032 2.309708  
-2.328032 2.309708  
-2.328032 2.309708  
-2.328032 2.309708  
-2.328032 2.309708  
-20.414616 20.420967  
-2.328032 2.309708  
-20.414616 20.420967  
-2.328032 2.309708  


Context Matrix (embedding_matrix + self_attention_matrix:
3.481137 33.251896 
-4.983266 9.082829 
9.085067 17.395665 
27.592531 38.285481 
2.594524 34.946823 
11.038288 22.611136 
-30.603495 21.034695 
7.892813 15.918906 
-41.961119 26.127672 
7.124445 11.832963 




semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[511] = -0.005402
semi_final_layer_weights[512] = 0.005513


 Semi Final Layer Node Values: 
-0.002113 
0.027519 
0.152374 
-0.003656 
0.211743 
-0.001894 
-0.000476 
-0.001378 
-0.000826 
-0.001118 
.
.
.
0.005465 
-0.000054 
0.005505 
0.005468 
-0.000056 
0.005490 
0.005466 
-0.000056 
0.005583 
-0.000054 
-0.000054 
0.005480 


final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Output Embedding: 0.271466, 0.048402 

 Expected Embedding: 4.643339, -27.140186 
 loss: 379.166310 


Updated weights for final layer:
final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[510] = 0.004598
semi_final_layer_weights[511] = -0.004487
Updating K Matrix:
k_matrix[0][0] = 0.438062
k_matrix[0][1] = -0.455204
k_matrix[1][0] = 0.442130
k_matrix[1][1] = -0.452624

Updating Q Matrix:
q_matrix[0][0] = -0.451817
q_matrix[0][1] = 0.434797
q_matrix[1][0] = -0.446737
q_matrix[1][1] = 0.445730

Updating V Matrix:
v_matrix[0][0] = 0.454868
v_matrix[0][1] = -0.449848
v_matrix[1][0] = 0.442578
v_matrix[1][1] = -0.443315

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
0.438062 -0.455204 
0.442130 -0.452624 

Q Matrix:
-0.451817 0.434797 
-0.446737 0.445730 

V Matrix:
0.454868 -0.449848 
0.442578 -0.443315 
Final K Matrix:
[8.868763, -9.144590]
[-9.695325, 10.019495]
[-16.663847, 17.053094]
[15.884646, -16.235799]
[16.552897, -17.229523]
[-9.930727, 10.170312]
[10.686220, -11.012105]
[27.977111, -28.833842]
[20.004067, -20.473112]
[-4.337545, 4.516756]
Final Q Matrix:
[-9.050199, 8.874800]
[9.924518, -9.678996]
[16.828936, -16.805881]
[-16.014907, 16.040189]
[-17.112050, 16.400250]
[10.039489, -10.007658]
[-10.896025, 10.700045]
[-28.531223, 28.009715]
[-20.204667, 20.172774]
[4.486664, -4.295615]
Final V Matrix:
[9.036237, -8.995321]
[-9.933326, 9.869295]
[-16.665507, 16.698642]
[15.837966, -15.886514]
[17.258065, -17.043717]
[-9.950191, 9.963445]
[10.872324, -10.828538]
[28.472944, -28.355286]
[20.010366, -20.048629]
[-4.526953, 4.469160]
 

Self-Attention Matrix: 
-9.933326 9.869295  
9.036237 -8.995321  
9.036237 -8.995321  
-9.933326 9.869295  
-9.933326 9.869295  
9.036237 -8.995321  
-9.933326 9.869295  
-9.933326 9.869295  
-9.933326 9.869295  
9.036237 -8.995321  


Context Matrix (embedding_matrix + self_attention_matrix:
-0.246695 20.330951 
-4.909551 -17.106510 
9.966019 -47.606432 
-13.769261 49.597440 
32.139741 5.622316 
8.461182 -30.886635 
0.779422 23.424957 
18.638959 44.837877 
-10.786304 55.959138 
-2.271292 -7.602396 




semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[511] = 0.004598
semi_final_layer_weights[512] = -0.004487


 Semi Final Layer Node Values: 
0.092800 
0.105960 
0.172152 
0.166975 
-0.001747 
-0.001062 
-0.001121 
0.286670 
-0.002047 
-0.000478 
.
.
.
-0.000045 
0.004466 
-0.000044 
-0.000044 
0.004312 
-0.000044 
-0.000044 
0.004317 
-0.000043 
0.004536 
0.004545 
-0.000044 


final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Output Embedding: 0.262781, 0.280526 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1000.474531 


Updated weights for final layer:
final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[510] = -0.005402
semi_final_layer_weights[511] = 0.005513
Updating K Matrix:
k_matrix[0][0] = -0.561938
k_matrix[0][1] = 0.544796
k_matrix[1][0] = -0.557870
k_matrix[1][1] = 0.547376

Updating Q Matrix:
q_matrix[0][0] = 0.548183
q_matrix[0][1] = -0.565203
q_matrix[1][0] = 0.553263
q_matrix[1][1] = -0.554270

Updating V Matrix:
v_matrix[0][0] = -0.545132
v_matrix[0][1] = 0.550152
v_matrix[1][0] = -0.557422
v_matrix[1][1] = 0.556685

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
-0.561938 0.544796 
-0.557870 0.547376 

Q Matrix:
0.548183 -0.565203 
0.553263 -0.554270 

V Matrix:
-0.545132 0.550152 
-0.557422 0.556685 
Final K Matrix:
[26.802307, -26.206468]
[0.522863, -0.464376]
[18.427088, -17.892567]
[4.310159, -4.167758]
[6.710379, -6.511559]
[17.657485, -17.231250]
[15.520854, -15.214641]
[19.640839, -19.065216]
[-4.821473, 4.712134]
[16.622177, -16.369251]
Final Q Matrix:
[-26.453823, 26.725479]
[-0.451061, 0.570510]
[-18.014290, 18.505220]
[-4.189504, 4.346655]
[-6.554271, 6.743203]
[-17.381166, 17.642205]
[-15.372911, 15.435644]
[-19.192676, 19.730298]
[4.755798, -4.809911]
[-16.567790, 16.452252]
Final V Matrix:
[26.552648, -26.597619]
[0.401352, -0.443330]
[17.944644, -18.085078]
[4.154069, -4.202155]
[6.524315, -6.579112]
[17.409147, -17.468324]
[15.472924, -15.464912]
[19.111949, -19.266801]
[-4.771203, 4.781181]
[16.757555, -16.683184]
 

Self-Attention Matrix: 
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
0.401352 -0.443330  
26.552648 -26.597619  
0.401352 -0.443330  


Context Matrix (embedding_matrix + self_attention_matrix:
-13.548673 -34.435613 
-7.002653 6.077422 
-28.193105 -4.671520 
-8.931152 1.231128 
-10.646071 -1.343921 
-13.916645 -17.672555 
-1.767737 -26.080079 
-30.973413 -4.046625 
29.389834 -20.812841 
9.494549 -39.398656 




semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[511] = -0.005402
semi_final_layer_weights[512] = 0.005513


 Semi Final Layer Node Values: 
0.274243 
0.000403 
-0.001878 
0.036623 
-0.000714 
0.178179 
-0.001601 
0.200052 
0.042172 
0.173175 
.
.
.
-0.000057 
0.005720 
-0.000058 
-0.000057 
0.005883 
-0.000058 
-0.000057 
0.005878 
-0.000059 
0.005646 
0.005637 
-0.000058 


final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Output Embedding: 0.322195, 0.259582 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1044.304173 


Updated weights for final layer:
final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[510] = 0.004598
semi_final_layer_weights[511] = -0.004487
Updating K Matrix:
k_matrix[0][0] = 0.438062
k_matrix[0][1] = -0.455204
k_matrix[1][0] = 0.442130
k_matrix[1][1] = -0.452624

Updating Q Matrix:
q_matrix[0][0] = -0.451817
q_matrix[0][1] = 0.434797
q_matrix[1][0] = -0.446737
q_matrix[1][1] = 0.445730

Updating V Matrix:
v_matrix[0][0] = 0.454868
v_matrix[0][1] = -0.449848
v_matrix[1][0] = 0.442578
v_matrix[1][1] = -0.443315

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
0.438062 -0.455204 
0.442130 -0.452624 

Q Matrix:
-0.451817 0.434797 
-0.446737 0.445730 

V Matrix:
0.454868 -0.449848 
0.442578 -0.443315 
Final K Matrix:
[12.514903, -12.815631]
[3.282590, -3.314989]
[-4.844473, 5.034419]
[-16.803108, 17.194608]
[22.873212, -23.571816]
[18.098764, -18.742876]
[14.396792, -14.768445]
[-5.092815, 5.391369]
[-4.436649, 4.616723]
[-24.397099, 25.255652]
Final Q Matrix:
[-12.650330, 12.613070]
[-3.254770, 3.355423]
[4.997107, -4.807983]
[16.968215, -16.947342]
[-23.323739, 22.901724]
[-18.579714, 18.028807]
[-14.587611, 14.483682]
[5.388012, -4.954300]
[4.584772, -4.397033]
[25.032188, -24.312633]
Final V Matrix:
[12.536518, -12.554244]
[3.175490, -3.219774]
[-5.031248, 4.975406]
[-16.802356, 16.836621]
[23.274144, -23.179513]
[18.637593, -18.484835]
[14.484019, -14.482493]
[-5.529044, 5.386030]
[-4.622546, 4.566187]
[-25.099885, 24.902247]
 

Self-Attention Matrix: 
3.175490 -3.219774  
3.175490 -3.219774  
12.536518 -12.554244  
12.536518 -12.554244  
3.175490 -3.219774  
3.175490 -3.219774  
3.175490 -3.219774  
12.536518 -12.554244  
12.536518 -12.554244  
12.536518 -12.554244  


Context Matrix (embedding_matrix + self_attention_matrix:
3.722018 24.544632 
-3.573247 10.891345 
1.421268 -12.498387 
13.622324 -51.634921 
26.263615 25.638628 
34.986845 6.196937 
7.615589 24.943299 
-13.809101 2.030143 
1.449716 -11.604167 
-28.905556 -26.674279 




semi_final_layer_weights[0] = 0.004401
semi_final_layer_weights[1] = -0.004604
semi_final_layer_weights[2] = -0.004455
semi_final_layer_weights[3] = 0.004534
semi_final_layer_weights[4] = -0.004506
semi_final_layer_weights[5] = 0.004533
semi_final_layer_weights[6] = -0.004449
semi_final_layer_weights[7] = 0.004446
semi_final_layer_weights[8] = -0.004434
semi_final_layer_weights[9] = 0.004396
.
.
.
semi_final_layer_weights[511] = 0.004598
semi_final_layer_weights[512] = -0.004487


 Semi Final Layer Node Values: 
0.128814 
-0.000383 
0.053806 
-0.001769 
-0.002384 
0.191201 
-0.001493 
-0.000479 
0.049461 
-0.002487 
.
.
.
0.004642 
-0.000047 
0.004601 
0.004639 
-0.000045 
0.004616 
0.004641 
-0.000045 
0.004519 
-0.000047 
-0.000047 
0.004626 


final_layer_weights[0] = 1.097733
final_layer_weights[1] = 0.866172
final_layer_weights[2] = -0.377097
final_layer_weights[3] = 0.554780
final_layer_weights[4] = 1.002963
final_layer_weights[5] = 0.428133
final_layer_weights[6] = 0.188803
final_layer_weights[7] = 0.834257
final_layer_weights[8] = 1.054219
final_layer_weights[9] = 1.419603
.
.
.
final_layer_weights[1022] = 0.285357
final_layer_weights[1023] = -1.039501


Output Embedding: 0.071771, 0.277319 

 Expected Embedding: 1.038754, 36.179375 
 loss: 644.946332 


Updated weights for final layer:
final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[510] = -0.005402
semi_final_layer_weights[511] = 0.005513
Updating K Matrix:
k_matrix[0][0] = -0.561938
k_matrix[0][1] = 0.544796
k_matrix[1][0] = -0.557870
k_matrix[1][1] = 0.547376

Updating Q Matrix:
q_matrix[0][0] = 0.548183
q_matrix[0][1] = -0.565203
q_matrix[1][0] = 0.553263
q_matrix[1][1] = -0.554270

Updating V Matrix:
v_matrix[0][0] = -0.545132
v_matrix[0][1] = 0.550152
v_matrix[1][0] = -0.557422
v_matrix[1][1] = 0.556685

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
-0.561938 0.544796 
-0.557870 0.547376 

Q Matrix:
0.548183 -0.565203 
0.553263 -0.554270 

V Matrix:
-0.545132 0.550152 
-0.557422 0.556685 
Final K Matrix:
[-26.053816, 25.438211]
[-23.748442, 23.154574]
[-21.768652, 21.083151]
[21.191763, -20.800263]
[0.000000, 0.000000]
[-10.611025, 10.343840]
[7.462177, -7.244168]
[22.511293, -22.086112]
[-7.688953, 7.579637]
[26.682714, -26.082146]
Final Q Matrix:
[25.664565, -26.017314]
[23.348227, -23.749499]
[21.206026, -21.917634]
[-21.026656, 21.047529]
[0.000000, 0.000000]
[10.429655, -10.613426]
[-7.292860, 7.495444]
[-22.323000, 22.367835]
[7.674448, -7.602296]
[-26.325537, 26.613976]
Final V Matrix:
[-25.720516, 25.796160]
[-23.363158, 23.460818]
[-21.064195, 21.277498]
[21.192515, -21.158251]
[0.000000, 0.000000]
[-10.434299, 10.479548]
[7.262947, -7.321177]
[22.488935, -22.460697]
[-7.770691, 7.729553]
[26.415781, -26.467030]
 

Self-Attention Matrix: 
-23.363158 23.460818  
-23.363158 23.460818  
-23.363158 23.460818  
-25.720516 25.796160  
-24.541837 24.628489  
-23.363158 23.460818  
-25.720516 25.796160  
-25.720516 25.796160  
-23.363158 23.460818  
-25.720516 25.796160  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.262157 50.922866 
-0.970576 43.474820 
18.642083 20.170289 
-24.634710 -13.284517 
-24.541837 24.628489 
-13.078056 32.121331 
-37.536116 24.321723 
-25.982988 -14.291708 
-28.738211 42.657779 
-40.732612 -6.911932 




semi_final_layer_weights[0] = -0.005599
semi_final_layer_weights[1] = 0.005396
semi_final_layer_weights[2] = 0.005545
semi_final_layer_weights[3] = -0.005466
semi_final_layer_weights[4] = 0.005494
semi_final_layer_weights[5] = -0.005467
semi_final_layer_weights[6] = 0.005551
semi_final_layer_weights[7] = -0.005554
semi_final_layer_weights[8] = 0.005566
semi_final_layer_weights[9] = -0.005604
.
.
.
semi_final_layer_weights[511] = -0.005402
semi_final_layer_weights[512] = 0.005513


 Semi Final Layer Node Values: 
-0.002668 
0.234761 
0.220750 
0.212736 
0.005970 
-0.001096 
-0.000678 
0.229236 
0.083040 
0.272586 
.
.
.
0.005974 
-0.000060 
0.006018 
0.005977 
-0.000061 
0.006002 
0.005975 
-0.000061 
0.006103 
-0.000059 
-0.000059 
0.005991 


final_layer_weights[0] = 1.087733
final_layer_weights[1] = 0.876172
final_layer_weights[2] = -0.367097
final_layer_weights[3] = 0.544780
final_layer_weights[4] = 1.012963
final_layer_weights[5] = 0.418133
final_layer_weights[6] = 0.198803
final_layer_weights[7] = 0.824257
final_layer_weights[8] = 1.064219
final_layer_weights[9] = 1.409603
.
.
.
final_layer_weights[1022] = 0.275357
final_layer_weights[1023] = -1.029501


Output Embedding: 0.504443, 0.519976 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.463306 


Updated weights for final layer:
final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[510] = -0.004405
semi_final_layer_weights[511] = 0.004495
Updating K Matrix:
k_matrix[0][0] = -0.458186
k_matrix[0][1] = 0.444209
k_matrix[1][0] = -0.454868
k_matrix[1][1] = 0.446312

Updating Q Matrix:
q_matrix[0][0] = 0.446970
q_matrix[0][1] = -0.460848
q_matrix[1][0] = 0.451112
q_matrix[1][1] = -0.451933

Updating V Matrix:
v_matrix[0][0] = -0.444483
v_matrix[0][1] = 0.448576
v_matrix[1][0] = -0.454503
v_matrix[1][1] = 0.453902

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.458186 0.444209 
-0.454868 0.446312 

Q Matrix:
0.446970 -0.460848 
0.451112 -0.451933 

V Matrix:
-0.444483 0.448576 
-0.454503 0.453902 
Final K Matrix:
[-3.162546, 3.141782]
[-16.570439, 16.303127]
[7.582550, -7.261094]
[17.279063, -16.959847]
[14.150123, -13.890600]
[1.575616, -1.516013]
[14.308165, -13.944568]
[0.000000, 0.000000]
[-9.577698, 9.473231]
[-8.198308, 7.997430]
Final Q Matrix:
[3.190147, -3.101525]
[16.495175, -16.416954]
[-7.271890, 7.721150]
[-17.144440, 17.161459]
[-14.042498, 14.051835]
[-1.521040, 1.596873]
[-14.058997, 14.314882]
[0.000000, 0.000000]
[9.603604, -9.436509]
[8.065882, -8.194337]
Final V Matrix:
[-3.256397, 3.218253]
[-16.667631, 16.606798]
[7.131386, -7.278196]
[17.279676, -17.251738]
[14.155326, -14.130790]
[1.499770, -1.523968]
[14.061604, -14.125528]
[0.000000, 0.000000]
[-9.758419, 9.679373]
[-8.075592, 8.105680]
 

Self-Attention Matrix: 
-3.256397 3.218253  
-3.256397 3.218253  
-16.667631 16.606798  
-16.667631 16.606798  
-16.667631 16.606798  
-16.667631 16.606798  
-16.667631 16.606798  
-9.962014 9.912526  
-3.256397 3.218253  
-3.256397 3.218253  


Context Matrix (embedding_matrix + self_attention_matrix:
-10.484559 17.451789 
-11.541998 47.993365 
-50.043974 33.556778 
-15.581825 -22.473879 
-15.425971 -15.752088 
-22.260455 18.776515 
-34.296121 2.908232 
-9.962014 9.912526 
-17.384961 38.505831 
5.452703 12.469114 




semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[511] = -0.004405
semi_final_layer_weights[512] = 0.004495


 Semi Final Layer Node Values: 
-0.000364 
0.164784 
-0.000700 
0.174066 
-0.001441 
0.011073 
-0.001375 
-0.000043 
0.100389 
-0.000865 
.
.
.
0.004261 
-0.000042 
0.004292 
0.004263 
-0.000044 
0.004280 
0.004262 
-0.000044 
0.004353 
-0.000042 
-0.000042 
0.004273 


final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Output Embedding: 0.185211, 0.243626 

 Expected Embedding: -37.748848, 10.192697 
 loss: 768.988409 


Updated weights for final layer:
final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[510] = 0.005595
semi_final_layer_weights[511] = -0.005505
Updating K Matrix:
k_matrix[0][0] = 0.541814
k_matrix[0][1] = -0.555791
k_matrix[1][0] = 0.545132
k_matrix[1][1] = -0.553688

Updating Q Matrix:
q_matrix[0][0] = -0.553030
q_matrix[0][1] = 0.539152
q_matrix[1][0] = -0.548888
q_matrix[1][1] = 0.548067

Updating V Matrix:
v_matrix[0][0] = 0.555517
v_matrix[0][1] = -0.551424
v_matrix[1][0] = 0.545497
v_matrix[1][1] = -0.546098

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.541814 -0.555791 
0.545132 -0.553688 

Q Matrix:
-0.553030 0.539152 
-0.548888 0.548067 

V Matrix:
0.555517 -0.551424 
0.545497 -0.546098 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[511] = 0.005595
semi_final_layer_weights[512] = -0.005505


 Semi Final Layer Node Values: 
0.005435 
-0.000056 
-0.000055 
0.005543 
-0.000055 
0.005542 
-0.000055 
0.005472 
-0.000055 
0.005431 
.
.
.
-0.000055 
0.005530 
-0.000055 
-0.000055 
0.005403 
-0.000055 
-0.000055 
0.005407 
-0.000054 
0.005588 
0.005595 
-0.000055 


final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Output Embedding: -0.034807, 0.008483 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.602388 


Updated weights for final layer:
final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[510] = -0.004405
semi_final_layer_weights[511] = 0.004495
Updating K Matrix:
k_matrix[0][0] = -0.458186
k_matrix[0][1] = 0.444209
k_matrix[1][0] = -0.454868
k_matrix[1][1] = 0.446312

Updating Q Matrix:
q_matrix[0][0] = 0.446970
q_matrix[0][1] = -0.460848
q_matrix[1][0] = 0.451112
q_matrix[1][1] = -0.451933

Updating V Matrix:
v_matrix[0][0] = -0.444483
v_matrix[0][1] = 0.448576
v_matrix[1][0] = -0.454503
v_matrix[1][1] = 0.453902

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 13  total loss: 5049.836169 ******************************************************************* 

Epoch: 14
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.458186 0.444209 
-0.454868 0.446312 

Q Matrix:
0.446970 -0.460848 
0.451112 -0.451933 

V Matrix:
-0.444483 0.448576 
-0.454503 0.453902 
Final K Matrix:
[-10.975091, 10.615270]
[-6.764296, 6.496565]
[-16.912748, 16.558972]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.671719, -11.065106]
[6.513565, -6.867975]
[16.723655, -16.840976]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-10.584553, 10.704558]
[-6.409198, 6.523470]
[-16.810488, 16.819387]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-6.409198 6.523470  
-6.409198 6.523470  
-6.409198 6.523470  
-8.496876 8.614014  
-8.496876 8.614014  
-8.496876 8.614014  
-8.496876 8.614014  
-8.496876 8.614014  
-8.496876 8.614014  
-8.496876 8.614014  


Context Matrix (embedding_matrix + self_attention_matrix:
22.216614 1.816956 
19.812365 -5.018434 
0.241441 37.005961 
-8.496876 8.614014 
-8.496876 8.614014 
-8.496876 8.614014 
-8.496876 8.614014 
-8.496876 8.614014 
-8.496876 8.614014 
-8.496876 8.614014 




semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[511] = -0.004405
semi_final_layer_weights[512] = 0.004495


 Semi Final Layer Node Values: 
-0.001143 
0.060693 
0.172917 
-0.000050 
0.005004 
-0.000050 
0.005056 
-0.000051 
0.005070 
-0.000051 
.
.
.
0.005008 
-0.000050 
0.005044 
0.005010 
-0.000051 
0.005031 
0.005009 
-0.000051 
0.005116 
-0.000049 
-0.000049 
0.005022 


final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Output Embedding: 0.059956, 0.041500 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.434843 


Updated weights for final layer:
final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[510] = 0.005595
semi_final_layer_weights[511] = -0.005505
Updating K Matrix:
k_matrix[0][0] = 0.541814
k_matrix[0][1] = -0.555791
k_matrix[1][0] = 0.545132
k_matrix[1][1] = -0.553688

Updating Q Matrix:
q_matrix[0][0] = -0.553030
q_matrix[0][1] = 0.539152
q_matrix[1][0] = -0.548888
q_matrix[1][1] = 0.548067

Updating V Matrix:
v_matrix[0][0] = 0.555517
v_matrix[0][1] = -0.551424
v_matrix[1][0] = 0.545497
v_matrix[1][1] = -0.546098

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.541814 -0.555791 
0.545132 -0.553688 

Q Matrix:
-0.553030 0.539152 
-0.548888 0.548067 

V Matrix:
0.555517 -0.551424 
0.545497 -0.546098 
Final K Matrix:
[12.925602, -13.245095]
[7.825696, -7.954609]
[20.220382, -20.574158]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-13.174107, 12.876305]
[-7.887933, 7.861636]
[-20.409475, 20.292154]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[13.218530, -13.132824]
[7.845757, -7.849176]
[20.322642, -20.313743]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.845757 -7.849176  
7.845757 -7.849176  
7.845757 -7.849176  
10.532143 -10.491000  
10.532143 -10.491000  
10.532143 -10.491000  
10.532143 -10.491000  
10.532143 -10.491000  
10.532143 -10.491000  
10.532143 -10.491000  


Context Matrix (embedding_matrix + self_attention_matrix:
29.155480 -5.318251 
8.956744 5.402206 
14.496397 22.633315 
10.532143 -10.491000 
10.532143 -10.491000 
10.532143 -10.491000 
10.532143 -10.491000 
10.532143 -10.491000 
10.532143 -10.491000 
10.532143 -10.491000 




semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[511] = 0.005595
semi_final_layer_weights[512] = -0.005505


 Semi Final Layer Node Values: 
0.124122 
-0.000860 
-0.002089 
-0.000053 
0.005293 
-0.000053 
0.005249 
-0.000052 
0.005237 
-0.000052 
.
.
.
0.005290 
-0.000053 
0.005259 
0.005288 
-0.000052 
0.005271 
0.005289 
-0.000052 
0.005197 
-0.000054 
-0.000054 
0.005278 


final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Output Embedding: 0.141721, 0.134538 

 Expected Embedding: 12.470385, -17.532656 
 loss: 232.062830 


Updated weights for final layer:
final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[510] = -0.004405
semi_final_layer_weights[511] = 0.004495
Updating K Matrix:
k_matrix[0][0] = -0.458186
k_matrix[0][1] = 0.444209
k_matrix[1][0] = -0.454868
k_matrix[1][1] = 0.446312

Updating Q Matrix:
q_matrix[0][0] = 0.446970
q_matrix[0][1] = -0.460848
q_matrix[1][0] = 0.451112
q_matrix[1][1] = -0.451933

Updating V Matrix:
v_matrix[0][0] = -0.444483
v_matrix[0][1] = 0.448576
v_matrix[1][0] = -0.454503
v_matrix[1][1] = 0.453902

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.458186 0.444209 
-0.454868 0.446312 

Q Matrix:
0.446970 -0.460848 
0.451112 -0.451933 

V Matrix:
-0.444483 0.448576 
-0.454503 0.453902 
Final K Matrix:
[3.155861, -3.048901]
[-18.364347, 17.776615]
[5.811833, -5.736982]
[-5.340073, 5.129815]
[-22.337769, 21.653555]
[-20.504052, 20.010858]
[11.083790, -10.798764]
[19.822945, -19.251694]
[-23.957362, 23.336930]
[15.486406, -15.187861]
Final Q Matrix:
[-3.063780, 3.185409]
[17.876622, -18.499923]
[-5.811657, 5.738176]
[5.143661, -5.420768]
[21.787077, -22.470503]
[20.185621, -20.484469]
[-10.886104, 11.092523]
[-19.384094, 19.903052]
[23.524027, -23.980806]
[-15.348486, 15.394055]
Final V Matrix:
[3.034866, -3.072437]
[-17.746578, 17.934796]
[5.892973, -5.855058]
[-5.062483, 5.151729]
[-21.662703, 21.864736]
[-20.220046, 20.287224]
[10.884415, -10.936885]
[19.313316, -19.461089]
[-23.515576, 23.632811]
[15.455978, -15.441853]
 

Self-Attention Matrix: 
-17.746578 17.934796  
3.034866 -3.072437  
-17.746578 17.934796  
3.034866 -3.072437  
3.034866 -3.072437  
3.034866 -3.072437  
-17.746578 17.934796  
-17.746578 17.934796  
3.034866 -3.072437  
-17.746578 17.934796  


Context Matrix (embedding_matrix + self_attention_matrix:
-26.630057 19.945094 
48.256019 -8.250495 
-11.312176 -1.323488 
23.529939 -11.977155 
52.313633 -3.602375 
23.098510 21.794491 
-32.030672 7.956032 
-54.770595 11.649300 
34.722518 17.677601 
-19.096437 -14.751407 




semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[511] = -0.004405
semi_final_layer_weights[512] = 0.004495


 Semi Final Layer Node Values: 
0.025951 
0.171622 
-0.000616 
-0.000470 
0.213726 
-0.002046 
-0.001044 
0.190745 
0.242340 
0.159220 
.
.
.
0.004820 
-0.000048 
0.004855 
0.004823 
-0.000049 
0.004842 
0.004821 
-0.000049 
0.004925 
-0.000047 
-0.000047 
0.004834 


final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Output Embedding: 0.803501, -0.189711 

 Expected Embedding: 19.465172, 25.668072 
 loss: 508.441444 


Updated weights for final layer:
final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[510] = 0.005595
semi_final_layer_weights[511] = -0.005505
Updating K Matrix:
k_matrix[0][0] = 0.541814
k_matrix[0][1] = -0.555791
k_matrix[1][0] = 0.545132
k_matrix[1][1] = -0.553688

Updating Q Matrix:
q_matrix[0][0] = -0.553030
q_matrix[0][1] = 0.539152
q_matrix[1][0] = -0.548888
q_matrix[1][1] = 0.548067

Updating V Matrix:
v_matrix[0][0] = 0.555517
v_matrix[0][1] = -0.551424
v_matrix[1][0] = 0.545497
v_matrix[1][1] = -0.546098

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.541814 -0.555791 
0.545132 -0.553688 

Q Matrix:
-0.553030 0.539152 
-0.548888 0.548067 

V Matrix:
0.555517 -0.551424 
0.545497 -0.546098 
Final K Matrix:
[20.015057, -20.361005]
[2.253598, -2.274441]
[14.407613, -14.696212]
[35.822924, -36.548936]
[20.458636, -20.806695]
[18.309015, -18.669539]
[-5.185919, 5.323073]
[12.956605, -13.215904]
[-8.563300, 8.815617]
[10.312916, -10.526514]
Final Q Matrix:
[-20.196439, 20.090418]
[-2.249261, 2.280547]
[-14.592284, 14.421514]
[-36.293633, 35.848876]
[-20.636441, 20.541332]
[-18.535183, 18.333025]
[5.297886, -5.156995]
[-13.122357, 12.969344]
[8.783517, -8.489193]
[-10.454702, 10.315707]
Final V Matrix:
[20.105954, -20.100776]
[2.219686, -2.234627]
[14.569512, -14.531863]
[36.246053, -36.145205]
[20.538002, -20.537473]
[18.499582, -18.457074]
[-5.325311, 5.283236]
[13.101627, -13.067971]
[-8.856465, 8.764838]
[10.445918, -10.412951]
 

Self-Attention Matrix: 
2.219686 -2.234627  
2.219686 -2.234627  
2.219686 -2.234627  
2.219686 -2.234627  
2.219686 -2.234627  
2.219686 -2.234627  
20.105954 -20.100776  
2.219686 -2.234627  
20.105954 -20.100776  
2.219686 -2.234627  


Context Matrix (embedding_matrix + self_attention_matrix:
8.028855 28.707562 
-0.435549 4.538494 
13.632784 12.851330 
32.140249 33.741146 
7.142242 30.402489 
15.586006 18.066801 
9.917075 -19.487048 
12.440530 11.374571 
-1.440549 -14.394071 
11.672163 7.288629 




semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[511] = 0.005595
semi_final_layer_weights[512] = -0.005505


 Semi Final Layer Node Values: 
0.205101 
-0.000286 
-0.001506 
0.370732 
-0.002128 
0.192047 
0.057858 
0.135776 
0.091947 
0.108407 
.
.
.
0.005544 
-0.000056 
0.005511 
0.005542 
-0.000054 
0.005524 
0.005543 
-0.000054 
0.005447 
-0.000056 
-0.000056 
0.005531 


final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Output Embedding: 0.469485, 0.785368 

 Expected Embedding: 4.643339, -27.140186 
 loss: 398.628827 


Updated weights for final layer:
final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[510] = -0.004405
semi_final_layer_weights[511] = 0.004495
Updating K Matrix:
k_matrix[0][0] = -0.458186
k_matrix[0][1] = 0.444209
k_matrix[1][0] = -0.454868
k_matrix[1][1] = 0.446312

Updating Q Matrix:
q_matrix[0][0] = 0.446970
q_matrix[0][1] = -0.460848
q_matrix[1][0] = 0.451112
q_matrix[1][1] = -0.451933

Updating V Matrix:
v_matrix[0][0] = -0.444483
v_matrix[0][1] = 0.448576
v_matrix[1][0] = -0.454503
v_matrix[1][1] = 0.453902

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.458186 0.444209 
-0.454868 0.446312 

Q Matrix:
0.446970 -0.460848 
0.451112 -0.451933 

V Matrix:
-0.444483 0.448576 
-0.454503 0.453902 
Final K Matrix:
[-9.196951, 8.972050]
[10.079283, -9.814965]
[17.136960, -16.819581]
[-16.313506, 16.027187]
[-17.345457, 16.793758]
[10.221148, -10.025799]
[-11.074468, 10.808752]
[-28.997511, 28.298961]
[-20.573989, 20.191545]
[4.547350, -4.401227]
Final Q Matrix:
[9.049014, -9.192028]
[-9.892406, 10.092597]
[-17.002351, 17.021150]
[16.207295, -16.186681]
[16.889542, -17.469920]
[-10.132467, 10.158421]
[10.903401, -11.063196]
[28.545706, -28.970926]
[20.410426, -20.436431]
[-4.425763, 4.581538]
Final V Matrix:
[-9.060398, 9.093759]
[9.885225, -9.937433]
[17.135607, -17.108590]
[-16.351568, 16.311983]
[-16.770486, 16.945258]
[10.205277, -10.194471]
[-10.922725, 10.958427]
[-28.593225, 28.689159]
[-20.568854, 20.537655]
[4.392913, -4.440035]
 

Self-Attention Matrix: 
9.885225 -9.937433  
-9.060398 9.093759  
-9.060398 9.093759  
9.885225 -9.937433  
9.885225 -9.937433  
-9.060398 9.093759  
9.885225 -9.937433  
9.885225 -9.937433  
9.885225 -9.937433  
-9.060398 9.093759  


Context Matrix (embedding_matrix + self_attention_matrix:
19.571855 0.524223 
-23.006186 0.982570 
-8.130616 -29.517351 
6.049289 29.790712 
51.958291 -14.184412 
-9.635453 -12.797554 
20.597972 3.618229 
38.457509 25.031149 
9.032246 36.152410 
-20.367927 10.486684 




semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[511] = -0.004405
semi_final_layer_weights[512] = 0.004495


 Semi Final Layer Node Values: 
-0.000963 
-0.000925 
-0.001747 
-0.001642 
0.164731 
0.104464 
0.114134 
-0.002920 
0.209595 
0.040578 
.
.
.
-0.000045 
0.004512 
-0.000046 
-0.000045 
0.004640 
-0.000045 
-0.000045 
0.004636 
-0.000046 
0.004453 
0.004446 
-0.000045 


final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Output Embedding: 0.336681, 0.080664 

 Expected Embedding: 44.423141, -6.847703 
 loss: 995.809151 


Updated weights for final layer:
final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[510] = 0.005595
semi_final_layer_weights[511] = -0.005505
Updating K Matrix:
k_matrix[0][0] = 0.541814
k_matrix[0][1] = -0.555791
k_matrix[1][0] = 0.545132
k_matrix[1][1] = -0.553688

Updating Q Matrix:
q_matrix[0][0] = -0.553030
q_matrix[0][1] = 0.539152
q_matrix[1][0] = -0.548888
q_matrix[1][1] = 0.548067

Updating V Matrix:
v_matrix[0][0] = 0.555517
v_matrix[0][1] = -0.551424
v_matrix[1][0] = 0.545497
v_matrix[1][1] = -0.546098

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.541814 -0.555791 
0.545132 -0.553688 

Q Matrix:
-0.553030 0.539152 
-0.548888 0.548067 

V Matrix:
0.555517 -0.551424 
0.545497 -0.546098 
Final K Matrix:
[-26.088594, 26.574421]
[-0.456929, 0.504617]
[-17.797809, 18.233640]
[-4.143685, 4.259794]
[-6.476594, 6.638705]
[-17.149893, 17.497431]
[-15.150647, 15.400323]
[-18.963570, 19.432913]
[4.690693, -4.779845]
[-16.308956, 16.515183]
Final Q Matrix:
[26.372736, -26.151236]
[0.515474, -0.418079]
[18.134391, -17.734103]
[4.242063, -4.113927]
[6.603879, -6.449830]
[17.375194, -17.162352]
[15.271274, -15.220124]
[19.328987, -18.890628]
[-4.744243, 4.700121]
[16.353301, -16.447507]
Final V Matrix:
[-26.292157, 26.255489]
[-0.556005, 0.521778]
[-18.191178, 18.076673]
[-4.270956, 4.231748]
[-6.628304, 6.583625]
[-17.352379, 17.304128]
[-15.189727, 15.196260]
[-19.394809, 19.268548]
[4.731683, -4.723547]
[-16.198573, 16.259212]
 

Self-Attention Matrix: 
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-0.556005 0.521778  
-26.292157 26.255489  
-0.556005 0.521778  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.506030 -33.470505 
-7.960010 7.042529 
-29.150462 -3.706412 
-9.888509 2.196236 
-11.603428 -0.378814 
-14.874002 -16.707448 
-2.725094 -25.114972 
-31.930770 -3.081517 
-23.454971 32.040267 
8.537192 -38.433548 




semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[511] = 0.005595
semi_final_layer_weights[512] = -0.005505


 Semi Final Layer Node Values: 
-0.002662 
-0.000005 
0.185501 
-0.000371 
0.071668 
-0.001806 
0.157864 
-0.001970 
-0.000524 
-0.001678 
.
.
.
-0.000053 
0.005334 
-0.000053 
-0.000053 
0.005212 
-0.000053 
-0.000053 
0.005216 
-0.000052 
0.005390 
0.005397 
-0.000053 


final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Output Embedding: -0.025458, 0.045285 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1061.969435 


Updated weights for final layer:
final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[510] = -0.004405
semi_final_layer_weights[511] = 0.004495
Updating K Matrix:
k_matrix[0][0] = -0.458186
k_matrix[0][1] = 0.444209
k_matrix[1][0] = -0.454868
k_matrix[1][1] = 0.446312

Updating Q Matrix:
q_matrix[0][0] = 0.446970
q_matrix[0][1] = -0.460848
q_matrix[1][0] = 0.451112
q_matrix[1][1] = -0.451933

Updating V Matrix:
v_matrix[0][0] = -0.444483
v_matrix[0][1] = 0.448576
v_matrix[1][0] = -0.454503
v_matrix[1][1] = 0.453902

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.458186 0.444209 
-0.454868 0.446312 

Q Matrix:
0.446970 -0.460848 
0.451112 -0.451933 

V Matrix:
-0.444483 0.448576 
-0.454503 0.453902 
Final K Matrix:
[-12.879561, 12.634358]
[-3.326528, 3.300111]
[5.067439, -4.912564]
[17.279063, -16.959847]
[-23.705421, 23.135802]
[-18.858869, 18.333681]
[-14.844880, 14.541847]
[5.437207, -5.193776]
[4.647653, -4.500826]
[25.410918, -24.710882]
Final Q Matrix:
[12.769139, -12.799519]
[3.349212, -3.267142]
[-4.942987, 5.097192]
[-17.144440, 17.161459]
[23.338075, -23.682173]
[18.466718, -18.915909]
[14.689292, -14.774033]
[-5.196513, 5.550148]
[-4.526878, 4.679954]
[-24.893088, 25.479789]
Final V Matrix:
[-12.861938, 12.847484]
[-3.413854, 3.377746]
[4.915149, -4.960681]
[17.279676, -17.251738]
[-23.378513, 23.455672]
[-18.419525, 18.544079]
[-14.773758, 14.775002]
[5.081521, -5.198129]
[4.496079, -4.542032]
[24.837889, -24.999037]
 

Self-Attention Matrix: 
-3.413854 3.377746  
-3.413854 3.377746  
-12.861938 12.847484  
-12.861938 12.847484  
-3.413854 3.377746  
-3.413854 3.377746  
-3.413854 3.377746  
-12.861938 12.847484  
-12.861938 12.847484  
-12.861938 12.847484  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.867326 31.142152 
-10.162591 17.488865 
-23.977187 12.903342 
-11.776132 -26.233193 
19.674271 32.236148 
28.397502 12.794457 
1.026245 31.540819 
-39.207557 27.431871 
-23.948739 13.797561 
-54.304011 -1.272550 




semi_final_layer_weights[0] = -0.004565
semi_final_layer_weights[1] = 0.004400
semi_final_layer_weights[2] = 0.004521
semi_final_layer_weights[3] = -0.004457
semi_final_layer_weights[4] = 0.004480
semi_final_layer_weights[5] = -0.004458
semi_final_layer_weights[6] = 0.004526
semi_final_layer_weights[7] = -0.004528
semi_final_layer_weights[8] = 0.004538
semi_final_layer_weights[9] = -0.004569
.
.
.
semi_final_layer_weights[511] = -0.004405
semi_final_layer_weights[512] = 0.004495


 Semi Final Layer Node Values: 
-0.001336 
0.036635 
-0.000455 
0.173860 
0.237016 
-0.001881 
0.151932 
0.048797 
-0.000415 
0.258499 
.
.
.
0.004369 
-0.000044 
0.004401 
0.004372 
-0.000045 
0.004389 
0.004370 
-0.000045 
0.004464 
-0.000043 
-0.000043 
0.004382 


final_layer_weights[0] = 1.088767
final_layer_weights[1] = 0.875176
final_layer_weights[2] = -0.368121
final_layer_weights[3] = 0.545789
final_layer_weights[4] = 1.011949
final_layer_weights[5] = 0.419142
final_layer_weights[6] = 0.197778
final_layer_weights[7] = 0.825283
final_layer_weights[8] = 1.063191
final_layer_weights[9] = 1.410638
.
.
.
final_layer_weights[1022] = 0.276354
final_layer_weights[1023] = -1.030519


Output Embedding: 0.853131, 0.177630 

 Expected Embedding: 1.038754, 36.179375 
 loss: 648.080036 


Updated weights for final layer:
final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[510] = 0.005595
semi_final_layer_weights[511] = -0.005505
Updating K Matrix:
k_matrix[0][0] = 0.541814
k_matrix[0][1] = -0.555791
k_matrix[1][0] = 0.545132
k_matrix[1][1] = -0.553688

Updating Q Matrix:
q_matrix[0][0] = -0.553030
q_matrix[0][1] = 0.539152
q_matrix[1][0] = -0.548888
q_matrix[1][1] = 0.548067

Updating V Matrix:
v_matrix[0][0] = 0.555517
v_matrix[0][1] = -0.551424
v_matrix[1][0] = 0.545497
v_matrix[1][1] = -0.546098

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.541814 -0.555791 
0.545132 -0.553688 

Q Matrix:
-0.553030 0.539152 
-0.548888 0.548067 

V Matrix:
0.555517 -0.551424 
0.545497 -0.546098 
Final K Matrix:
[25.319629, -25.821573]
[23.042889, -23.527110]
[20.965273, -21.524208]
[-20.715808, 21.035025]
[0.000000, 0.000000]
[10.293736, -10.511590]
[-7.205625, 7.383382]
[-21.995376, 22.342055]
[7.552589, -7.641722]
[-25.963986, 26.453669]
Final Q Matrix:
[-25.637012, 25.349392]
[-23.369211, 23.042028]
[-21.424020, 20.843798]
[20.850431, -20.833412]
[0.000000, 0.000000]
[-10.441620, 10.291779]
[7.343680, -7.178500]
[22.148904, -22.112347]
[-7.564416, 7.623246]
[26.255216, -26.020032]
Final V Matrix:
[25.591391, -25.529714]
[23.357037, -23.277408]
[21.539664, -21.365744]
[-20.715195, 20.743133]
[0.000000, 0.000000]
[10.437833, -10.400939]
[-7.368070, 7.320592]
[-22.013606, 22.036631]
[7.485943, -7.519485]
[-26.181633, 26.139847]
 

Self-Attention Matrix: 
23.357037 -23.277408  
23.357037 -23.277408  
23.357037 -23.277408  
25.591391 -25.529714  
24.474214 -24.403561  
23.357037 -23.277408  
25.591391 -25.529714  
25.591391 -25.529714  
23.357037 -23.277408  
25.591391 -25.529714  


Context Matrix (embedding_matrix + self_attention_matrix:
42.458039 4.184639 
45.749619 -3.263406 
65.362278 -26.567937 
26.677197 -64.610391 
24.474214 -24.403561 
33.642140 -14.616895 
13.775791 -27.004151 
25.328919 -65.617581 
17.981984 -4.080448 
10.579295 -58.237805 




semi_final_layer_weights[0] = 0.005435
semi_final_layer_weights[1] = -0.005600
semi_final_layer_weights[2] = -0.005479
semi_final_layer_weights[3] = 0.005543
semi_final_layer_weights[4] = -0.005520
semi_final_layer_weights[5] = 0.005542
semi_final_layer_weights[6] = -0.005474
semi_final_layer_weights[7] = 0.005472
semi_final_layer_weights[8] = -0.005462
semi_final_layer_weights[9] = 0.005431
.
.
.
semi_final_layer_weights[511] = 0.005595
semi_final_layer_weights[512] = -0.005505


 Semi Final Layer Node Values: 
0.258942 
-0.002323 
-0.002071 
-0.002158 
0.005130 
0.099896 
0.077883 
-0.002259 
-0.000705 
-0.002643 
.
.
.
0.005127 
-0.000051 
0.005097 
0.005125 
-0.000050 
0.005108 
0.005127 
-0.000050 
0.005037 
-0.000052 
-0.000052 
0.005116 


final_layer_weights[0] = 1.098767
final_layer_weights[1] = 0.865176
final_layer_weights[2] = -0.378121
final_layer_weights[3] = 0.555789
final_layer_weights[4] = 1.001949
final_layer_weights[5] = 0.429142
final_layer_weights[6] = 0.187778
final_layer_weights[7] = 0.835283
final_layer_weights[8] = 1.053191
final_layer_weights[9] = 1.420638
.
.
.
final_layer_weights[1022] = 0.286354
final_layer_weights[1023] = -1.040519


Output Embedding: 0.767280, 0.506839 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.947650 


Updated weights for final layer:
final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[510] = 0.004535
semi_final_layer_weights[511] = -0.004462
Updating K Matrix:
k_matrix[0][0] = 0.439153
k_matrix[0][1] = -0.450482
k_matrix[1][0] = 0.441842
k_matrix[1][1] = -0.448777

Updating Q Matrix:
q_matrix[0][0] = -0.448244
q_matrix[0][1] = 0.436996
q_matrix[1][0] = -0.444887
q_matrix[1][1] = 0.444221

Updating V Matrix:
v_matrix[0][0] = 0.450260
v_matrix[0][1] = -0.446942
v_matrix[1][0] = 0.442138
v_matrix[1][1] = -0.442625

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
0.439153 -0.450482 
0.441842 -0.448777 

Q Matrix:
-0.448244 0.436996 
-0.444887 0.444221 

V Matrix:
0.450260 -0.446942 
0.442138 -0.442625 
Final K Matrix:
[3.114703, -3.131532]
[16.144876, -16.361539]
[-7.168118, 7.428666]
[-16.790650, 17.049382]
[-13.752236, 13.962586]
[-1.497435, 1.545745]
[-13.794212, 14.088916]
[0.000000, 0.000000]
[9.386929, -9.471602]
[7.912049, -8.074865]
Final Q Matrix:
[-3.092332, 3.164162]
[-16.205879, 16.269278]
[7.419916, -7.055780]
[16.899764, -16.885970]
[13.839469, -13.831901]
[1.541670, -1.480206]
[13.996168, -13.788768]
[0.000000, 0.000000]
[-9.365931, 9.501366]
[-8.019383, 7.915267]
Final V Matrix:
[3.038635, -3.069551]
[16.066099, -16.115405]
[-7.533797, 7.414804]
[-16.790152, 16.812797]
[-13.748020, 13.767906]
[-1.558910, 1.539297]
[-13.994055, 13.942243]
[0.000000, 0.000000]
[9.240450, -9.304519]
[8.011513, -7.987126]
 

Self-Attention Matrix: 
3.038635 -3.069551  
3.038635 -3.069551  
16.066099 -16.115405  
16.066099 -16.115405  
16.066099 -16.115405  
16.066099 -16.115405  
16.066099 -16.115405  
9.552367 -9.592478  
3.038635 -3.069551  
3.038635 -3.069551  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.189528 11.163986 
-5.246967 41.705561 
-17.310244 0.834574 
17.151905 -55.196082 
17.307759 -48.474292 
10.473275 -13.945689 
-1.562391 -29.813972 
9.552367 -9.592478 
-11.089930 32.218027 
11.747734 6.181310 




semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[511] = 0.004535
semi_final_layer_weights[512] = -0.004462


 Semi Final Layer Node Values: 
0.035130 
-0.001700 
0.068725 
-0.001754 
0.143927 
-0.000201 
0.143642 
-0.000046 
-0.000980 
0.083325 
.
.
.
0.004651 
-0.000047 
0.004624 
0.004649 
-0.000046 
0.004634 
0.004650 
-0.000046 
0.004569 
-0.000047 
-0.000047 
0.004641 


final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Output Embedding: 0.692201, 0.197686 

 Expected Embedding: -37.748848, 10.192697 
 loss: 788.807252 


Updated weights for final layer:
final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[510] = -0.005465
semi_final_layer_weights[511] = 0.005538
Updating K Matrix:
k_matrix[0][0] = -0.560847
k_matrix[0][1] = 0.549518
k_matrix[1][0] = -0.558158
k_matrix[1][1] = 0.551223

Updating Q Matrix:
q_matrix[0][0] = 0.551756
q_matrix[0][1] = -0.563004
q_matrix[1][0] = 0.555113
q_matrix[1][1] = -0.555779

Updating V Matrix:
v_matrix[0][0] = -0.549740
v_matrix[0][1] = 0.553058
v_matrix[1][0] = -0.557862
v_matrix[1][1] = 0.557375

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.560847 0.549518 
-0.558158 0.551223 

Q Matrix:
0.551756 -0.563004 
0.555113 -0.555779 

V Matrix:
-0.549740 0.553058 
-0.557862 0.557375 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[511] = -0.005465
semi_final_layer_weights[512] = 0.005538


 Semi Final Layer Node Values: 
-0.000056 
0.005461 
0.005559 
-0.000055 
0.005526 
-0.000055 
0.005563 
-0.000056 
0.005573 
-0.000056 
.
.
.
0.005528 
-0.000055 
0.005555 
0.005530 
-0.000056 
0.005545 
0.005529 
-0.000056 
0.005607 
-0.000055 
-0.000055 
0.005538 


final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Output Embedding: 0.076116, 0.045546 

 Expected Embedding: -10.885829, -17.296432 
 loss: 210.454227 


Updated weights for final layer:
final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[510] = 0.004535
semi_final_layer_weights[511] = -0.004462
Updating K Matrix:
k_matrix[0][0] = 0.439153
k_matrix[0][1] = -0.450482
k_matrix[1][0] = 0.441842
k_matrix[1][1] = -0.448777

Updating Q Matrix:
q_matrix[0][0] = -0.448244
q_matrix[0][1] = 0.436996
q_matrix[1][0] = -0.444887
q_matrix[1][1] = 0.444221

Updating V Matrix:
v_matrix[0][0] = 0.450260
v_matrix[0][1] = -0.446942
v_matrix[1][0] = 0.442138
v_matrix[1][1] = -0.442625

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 14  total loss: 5094.635694 ******************************************************************* 

Epoch: 15
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.439153 -0.450482 
0.441842 -0.448777 

Q Matrix:
-0.448244 0.436996 
-0.444887 0.444221 

V Matrix:
0.450260 -0.446942 
0.442138 -0.442625 
Final K Matrix:
[10.491584, -10.783227]
[6.415588, -6.632590]
[16.389095, -16.675838]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-10.737474, 10.418624]
[-6.618811, 6.331553]
[-16.542359, 16.447267]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[10.808124, -10.710858]
[6.703403, -6.610782]
[16.471979, -16.464766]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
6.703403 -6.610782  
6.703403 -6.610782  
6.703403 -6.610782  
8.755763 -8.660820  
8.755763 -8.660820  
8.755763 -8.660820  
8.755763 -8.660820  
8.755763 -8.660820  
8.755763 -8.660820  
8.755763 -8.660820  


Context Matrix (embedding_matrix + self_attention_matrix:
35.329215 -11.317297 
32.924966 -18.152687 
13.354042 23.871708 
8.755763 -8.660820 
8.755763 -8.660820 
8.755763 -8.660820 
8.755763 -8.660820 
8.755763 -8.660820 
8.755763 -8.660820 
8.755763 -8.660820 




semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[511] = 0.004535
semi_final_layer_weights[512] = -0.004462


 Semi Final Layer Node Values: 
0.101374 
-0.000625 
-0.001698 
-0.000041 
0.004050 
-0.000041 
0.004015 
-0.000040 
0.004007 
-0.000040 
.
.
.
0.004047 
-0.000041 
0.004023 
0.004046 
-0.000040 
0.004032 
0.004047 
-0.000040 
0.003976 
-0.000041 
-0.000041 
0.004038 


final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Output Embedding: 0.110496, 0.106294 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.949696 


Updated weights for final layer:
final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[510] = -0.005465
semi_final_layer_weights[511] = 0.005538
Updating K Matrix:
k_matrix[0][0] = -0.560847
k_matrix[0][1] = 0.549518
k_matrix[1][0] = -0.558158
k_matrix[1][1] = 0.551223

Updating Q Matrix:
q_matrix[0][0] = 0.551756
q_matrix[0][1] = -0.563004
q_matrix[1][0] = 0.555113
q_matrix[1][1] = -0.555779

Updating V Matrix:
v_matrix[0][0] = -0.549740
v_matrix[0][1] = 0.553058
v_matrix[1][0] = -0.557862
v_matrix[1][1] = 0.557375

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.560847 0.549518 
-0.558158 0.551223 

Q Matrix:
0.551756 -0.563004 
0.555113 -0.555779 

V Matrix:
-0.549740 0.553058 
-0.557862 0.557375 
Final K Matrix:
[-13.364144, 13.105187]
[-8.019458, 7.914971]
[-20.744035, 20.457292]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[13.162724, -13.404100]
[7.969014, -7.990329]
[20.590771, -20.685863]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-13.126719, 13.196186]
[-8.003199, 8.000428]
[-20.661151, 20.668364]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.003199 8.000428  
-8.003199 8.000428  
-8.003199 8.000428  
-10.564959 10.598307  
-10.564959 10.598307  
-10.564959 10.598307  
-10.564959 10.598307  
-10.564959 10.598307  
-10.564959 10.598307  
-10.564959 10.598307  


Context Matrix (embedding_matrix + self_attention_matrix:
13.306524 10.531353 
-6.892211 21.251810 
-1.352559 38.482918 
-10.564959 10.598307 
-10.564959 10.598307 
-10.564959 10.598307 
-10.564959 10.598307 
-10.564959 10.598307 
-10.564959 10.598307 
-10.564959 10.598307 




semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[511] = -0.005465
semi_final_layer_weights[512] = 0.005538


 Semi Final Layer Node Values: 
-0.001390 
0.083879 
0.211973 
-0.000057 
0.005710 
-0.000057 
0.005749 
-0.000058 
0.005759 
-0.000058 
.
.
.
0.005712 
-0.000057 
0.005740 
0.005714 
-0.000058 
0.005730 
0.005713 
-0.000058 
0.005794 
-0.000057 
-0.000056 
0.005723 


final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Output Embedding: 0.073918, 0.047612 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.369094 


Updated weights for final layer:
final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[510] = 0.004535
semi_final_layer_weights[511] = -0.004462
Updating K Matrix:
k_matrix[0][0] = 0.439153
k_matrix[0][1] = -0.450482
k_matrix[1][0] = 0.441842
k_matrix[1][1] = -0.448777

Updating Q Matrix:
q_matrix[0][0] = -0.448244
q_matrix[0][1] = 0.436996
q_matrix[1][0] = -0.444887
q_matrix[1][1] = 0.444221

Updating V Matrix:
v_matrix[0][0] = 0.450260
v_matrix[0][1] = -0.446942
v_matrix[1][0] = 0.442138
v_matrix[1][1] = -0.442625

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
0.439153 -0.450482 
0.441842 -0.448777 

Q Matrix:
-0.448244 0.436996 
-0.444887 0.444221 

V Matrix:
0.450260 -0.446942 
0.442138 -0.442625 
Final K Matrix:
[-3.012975, 3.099668]
[17.571136, -18.047506]
[-5.683430, 5.744099]
[5.066001, -5.236420]
[21.406785, -21.961357]
[19.798269, -20.198015]
[-10.681944, 10.912964]
[-19.036416, 19.499428]
[23.083976, -23.586851]
[-15.034933, 15.276910]
Final Q Matrix:
[3.087609, -2.989025]
[-17.966448, 17.461248]
[5.683572, -5.743130]
[-5.225198, 5.000596]
[-21.853134, 21.299201]
[-20.056365, 19.814142]
[10.842173, -10.674866]
[19.392115, -18.971487]
[-23.435204, 23.064974]
[15.146720, -15.109785]
Final V Matrix:
[-3.111044, 3.080592]
[18.071852, -17.919297]
[-5.617664, 5.648395]
[5.290995, -5.218658]
[21.953942, -21.790189]
[20.028463, -19.974013]
[-10.843542, 10.801014]
[-19.449482, 19.329709]
[23.442053, -23.347032]
[-15.059595, 15.071043]
 

Self-Attention Matrix: 
18.071852 -17.919297  
-3.111044 3.080592  
18.071852 -17.919297  
-3.111044 3.080592  
-3.111044 3.080592  
-3.111044 3.080592  
18.071852 -17.919297  
18.071852 -17.919297  
-3.111044 3.080592  
18.071852 -17.919297  


Context Matrix (embedding_matrix + self_attention_matrix:
9.188373 -15.908999 
42.110109 -2.097465 
24.506254 -37.177580 
17.384029 -5.824126 
46.167722 2.550654 
16.952600 27.947520 
3.787758 -27.898060 
-18.952165 -24.204793 
28.576608 23.830630 
16.721993 -50.605500 




semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[511] = 0.004535
semi_final_layer_weights[512] = -0.004462


 Semi Final Layer Node Values: 
-0.000340 
-0.001771 
0.060712 
0.047444 
-0.002225 
0.206181 
0.111405 
-0.001958 
-0.002364 
-0.001536 
.
.
.
0.004199 
-0.000042 
0.004174 
0.004197 
-0.000041 
0.004183 
0.004198 
-0.000041 
0.004125 
-0.000043 
-0.000043 
0.004189 


final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Output Embedding: -0.137015, 0.002294 

 Expected Embedding: 19.465172, 25.668072 
 loss: 521.488923 


Updated weights for final layer:
final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[510] = -0.005465
semi_final_layer_weights[511] = 0.005538
Updating K Matrix:
k_matrix[0][0] = -0.560847
k_matrix[0][1] = 0.549518
k_matrix[1][0] = -0.558158
k_matrix[1][1] = 0.551223

Updating Q Matrix:
q_matrix[0][0] = 0.551756
q_matrix[0][1] = -0.563004
q_matrix[1][0] = 0.555113
q_matrix[1][1] = -0.555779

Updating V Matrix:
v_matrix[0][0] = -0.549740
v_matrix[0][1] = 0.553058
v_matrix[1][0] = -0.557862
v_matrix[1][1] = 0.557375

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
-0.560847 0.549518 
-0.558158 0.551223 

Q Matrix:
0.551756 -0.563004 
0.555113 -0.555779 

V Matrix:
-0.549740 0.553058 
-0.557862 0.557375 
Final K Matrix:
[-20.528683, 20.248284]
[-2.291292, 2.274398]
[-14.821346, 14.587430]
[-36.861014, 36.272564]
[-20.977466, 20.695356]
[-18.827860, 18.535647]
[5.371842, -5.260675]
[-13.328409, 13.118241]
[8.899042, -8.694532]
[-10.616871, 10.443745]
Final Q Matrix:
[20.381669, -20.467601]
[2.294808, -2.269449]
[14.671666, -14.810079]
[36.479493, -36.839979]
[20.833351, -20.910439]
[18.644546, -18.808400]
[-5.281090, 5.395285]
[13.194063, -13.318084]
[-8.720551, 8.959107]
[10.501950, -10.614609]
Final V Matrix:
[-20.455009, 20.459206]
[-2.318778, 2.306669]
[-14.690123, 14.720638]
[-36.518058, 36.599797]
[-20.913138, 20.913567]
[-18.673401, 18.707855]
[5.258861, -5.292964]
[-13.210865, 13.238144]
[8.661424, -8.735690]
[-10.509070, 10.535791]
 

Self-Attention Matrix: 
-2.318778 2.306669  
-2.318778 2.306669  
-2.318778 2.306669  
-2.318778 2.306669  
-2.318778 2.306669  
-2.318778 2.306669  
-20.455009 20.459206  
-2.318778 2.306669  
-20.455009 20.459206  
-2.318778 2.306669  


Context Matrix (embedding_matrix + self_attention_matrix:
3.490390 33.248857 
-4.974013 9.079790 
9.094320 17.392625 
27.601784 38.282442 
2.603778 34.943784 
11.047541 22.608097 
-30.643888 21.072934 
7.902066 15.915866 
-42.001512 26.165911 
7.133698 11.829924 




semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[511] = -0.005465
semi_final_layer_weights[512] = 0.005538


 Semi Final Layer Node Values: 
-0.002111 
0.027883 
0.152804 
-0.003683 
0.212997 
-0.001909 
-0.000477 
-0.001381 
-0.000827 
-0.001118 
.
.
.
0.005506 
-0.000055 
0.005533 
0.005508 
-0.000056 
0.005523 
0.005507 
-0.000056 
0.005585 
-0.000054 
-0.000054 
0.005516 


final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Output Embedding: 0.272902, 0.048217 

 Expected Embedding: 4.643339, -27.140186 
 loss: 379.155011 


Updated weights for final layer:
final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[510] = 0.004535
semi_final_layer_weights[511] = -0.004462
Updating K Matrix:
k_matrix[0][0] = 0.439153
k_matrix[0][1] = -0.450482
k_matrix[1][0] = 0.441842
k_matrix[1][1] = -0.448777

Updating Q Matrix:
q_matrix[0][0] = -0.448244
q_matrix[0][1] = 0.436996
q_matrix[1][0] = -0.444887
q_matrix[1][1] = 0.444221

Updating V Matrix:
v_matrix[0][0] = 0.450260
v_matrix[0][1] = -0.446942
v_matrix[1][0] = 0.442138
v_matrix[1][1] = -0.442625

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
0.439153 -0.450482 
0.441842 -0.448777 

Q Matrix:
-0.448244 0.436996 
-0.444887 0.444221 

V Matrix:
0.450260 -0.446942 
0.442138 -0.442625 
Final K Matrix:
[8.876315, -9.058602]
[-9.708203, 9.922439]
[-16.651694, 16.908937]
[15.869000, -16.101068]
[16.600033, -17.047198]
[-9.925039, 10.083375]
[10.693999, -10.909369]
[27.998202, -28.564393]
[19.989841, -20.299820]
[-4.350286, 4.468722]
Final Q Matrix:
[-8.996222, 8.880305]
[9.859671, -9.697411]
[16.760797, -16.745561]
[-15.955086, 15.971794]
[-16.969563, 16.499153]
[9.996917, -9.975881]
[-10.832654, 10.703136]
[-28.364400, 28.019749]
[-20.122412, 20.101334]
[4.448835, -4.322575]
Final V Matrix:
[8.986995, -8.959954]
[-9.865492, 9.823175]
[-16.652791, 16.674689]
[15.838150, -15.870234]
[17.066060, -16.924403]
[-9.937903, 9.946662]
[10.816991, -10.788054]
[28.325885, -28.248128]
[19.994003, -20.019290]
[-4.475461, 4.437267]
 

Self-Attention Matrix: 
-9.865492 9.823175  
8.986995 -8.959954  
8.986995 -8.959954  
-9.865492 9.823175  
-9.865492 9.823175  
8.986995 -8.959954  
-9.865492 9.823175  
-9.865492 9.823175  
-9.865492 9.823175  
8.986995 -8.959954  


Context Matrix (embedding_matrix + self_attention_matrix:
-0.178861 20.284832 
-4.958794 -17.071143 
9.916776 -47.571065 
-13.701427 49.551321 
32.207575 5.576197 
8.411939 -30.851268 
0.847256 23.378837 
18.706793 44.791757 
-10.718470 55.913019 
-2.320535 -7.567029 




semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[511] = 0.004535
semi_final_layer_weights[512] = -0.004462


 Semi Final Layer Node Values: 
0.092977 
0.104532 
0.171658 
0.165560 
-0.001735 
-0.001053 
-0.001119 
0.286038 
-0.002045 
-0.000479 
.
.
.
-0.000044 
0.004448 
-0.000044 
-0.000044 
0.004346 
-0.000044 
-0.000044 
0.004349 
-0.000044 
0.004495 
0.004501 
-0.000044 


final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Output Embedding: 0.261194, 0.279625 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1000.538185 


Updated weights for final layer:
final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[510] = -0.005465
semi_final_layer_weights[511] = 0.005538
Updating K Matrix:
k_matrix[0][0] = -0.560847
k_matrix[0][1] = 0.549518
k_matrix[1][0] = -0.558158
k_matrix[1][1] = 0.551223

Updating Q Matrix:
q_matrix[0][0] = 0.551756
q_matrix[0][1] = -0.563004
q_matrix[1][0] = 0.555113
q_matrix[1][1] = -0.555779

Updating V Matrix:
v_matrix[0][0] = -0.549740
v_matrix[0][1] = 0.553058
v_matrix[1][0] = -0.557862
v_matrix[1][1] = 0.557375

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
-0.560847 0.549518 
-0.558158 0.551223 

Q Matrix:
0.551756 -0.563004 
0.555113 -0.555779 

V Matrix:
-0.549740 0.553058 
-0.557862 0.557375 
Final K Matrix:
[26.796890, -26.403116]
[0.512902, -0.474250]
[18.397105, -18.043854]
[4.299492, -4.205383]
[6.698583, -6.567188]
[17.646831, -17.365143]
[15.525883, -15.323514]
[19.607641, -19.227227]
[-4.820046, 4.747787]
[16.643337, -16.476185]
Final Q Matrix:
[-26.566586, 26.746117]
[-0.465450, 0.544391]
[-18.124297, 18.448741]
[-4.219754, 4.323611]
[-6.595416, 6.720276]
[-17.464219, 17.636733]
[-15.428111, 15.469570]
[-19.311462, 19.666762]
[4.776643, -4.812405]
[-16.607394, 16.531038]
Final V Matrix:
[26.631897, -26.661617]
[0.432599, -0.460341]
[18.078270, -18.171079]
[4.196336, -4.228115]
[6.575619, -6.611832]
[17.482711, -17.521819]
[15.494207, -15.488912]
[19.258112, -19.360449]
[-4.786824, 4.793418]
[16.732805, -16.683656]
 

Self-Attention Matrix: 
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
0.432599 -0.460341  
26.631897 -26.661617  
0.432599 -0.460341  


Context Matrix (embedding_matrix + self_attention_matrix:
-13.517426 -34.452624 
-6.971406 6.060410 
-28.161858 -4.688531 
-8.899905 1.214117 
-10.614824 -1.360932 
-13.885398 -17.689566 
-1.736490 -26.097090 
-30.942166 -4.063636 
29.469083 -20.876839 
9.525796 -39.415667 




semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[511] = -0.005465
semi_final_layer_weights[512] = 0.005538


 Semi Final Layer Node Values: 
0.273974 
0.000486 
-0.001882 
0.036820 
-0.000717 
0.179425 
-0.001604 
0.200380 
0.042312 
0.172923 
.
.
.
-0.000057 
0.005676 
-0.000057 
-0.000057 
0.005782 
-0.000057 
-0.000057 
0.005779 
-0.000058 
0.005628 
0.005622 
-0.000057 


final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Output Embedding: 0.322787, 0.257252 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1044.370280 


Updated weights for final layer:
final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[510] = 0.004535
semi_final_layer_weights[511] = -0.004462
Updating K Matrix:
k_matrix[0][0] = 0.439153
k_matrix[0][1] = -0.450482
k_matrix[1][0] = 0.441842
k_matrix[1][1] = -0.448777

Updating Q Matrix:
q_matrix[0][0] = -0.448244
q_matrix[0][1] = 0.436996
q_matrix[1][0] = -0.444887
q_matrix[1][1] = 0.444221

Updating V Matrix:
v_matrix[0][0] = 0.450260
v_matrix[0][1] = -0.446942
v_matrix[1][0] = 0.442138
v_matrix[1][1] = -0.442625

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
0.439153 -0.450482 
0.441842 -0.448777 

Q Matrix:
-0.448244 0.436996 
-0.444887 0.444221 

V Matrix:
0.450260 -0.446942 
0.442138 -0.442625 
Final K Matrix:
[12.507491, -12.706234]
[3.271155, -3.292567]
[-4.856618, 4.982148]
[-16.790650, 17.049382]
[22.890081, -23.351770]
[18.130760, -18.556438]
[14.393513, -14.639129]
[-5.125771, 5.323078]
[-4.449021, 4.568028]
[-24.438248, 25.005644]
Final Q Matrix:
[-12.596990, 12.572366]
[-3.252769, 3.319288]
[4.957490, -4.832503]
[16.899764, -16.885970]
[-23.187823, 22.908924]
[-18.448608, 18.084528]
[-14.519620, 14.450936]
[5.320859, -5.034230]
[4.546912, -4.422840]
[24.857962, -24.382427]
Final V Matrix:
[12.521775, -12.533490]
[3.200375, -3.229642]
[-4.980053, 4.943148]
[-16.790152, 16.812797]
[23.155047, -23.092508]
[18.486859, -18.385905]
[14.451159, -14.450150]
[-5.414063, 5.319549]
[-4.571876, 4.534630]
[-24.902701, 24.772087]
 

Self-Attention Matrix: 
3.200375 -3.229642  
3.200375 -3.229642  
12.521775 -12.533490  
12.521775 -12.533490  
3.200375 -3.229642  
3.200375 -3.229642  
3.200375 -3.229642  
12.521775 -12.533490  
12.521775 -12.533490  
12.521775 -12.533490  


Context Matrix (embedding_matrix + self_attention_matrix:
3.746903 24.534765 
-3.548362 10.881478 
1.406525 -12.477632 
13.607581 -51.614167 
26.288501 25.628761 
35.011731 6.187069 
7.640475 24.933431 
-13.823844 2.050897 
1.434973 -11.583413 
-28.920299 -26.653524 




semi_final_layer_weights[0] = 0.004405
semi_final_layer_weights[1] = -0.004539
semi_final_layer_weights[2] = -0.004441
semi_final_layer_weights[3] = 0.004493
semi_final_layer_weights[4] = -0.004474
semi_final_layer_weights[5] = 0.004492
semi_final_layer_weights[6] = -0.004437
semi_final_layer_weights[7] = 0.004435
semi_final_layer_weights[8] = -0.004427
semi_final_layer_weights[9] = 0.004402
.
.
.
semi_final_layer_weights[511] = 0.004535
semi_final_layer_weights[512] = -0.004462


 Semi Final Layer Node Values: 
0.128994 
-0.000378 
0.053606 
-0.001753 
-0.002368 
0.189555 
-0.001490 
-0.000478 
0.049353 
-0.002490 
.
.
.
0.004564 
-0.000046 
0.004537 
0.004562 
-0.000045 
0.004547 
0.004563 
-0.000045 
0.004483 
-0.000046 
-0.000046 
0.004553 


final_layer_weights[0] = 1.097737
final_layer_weights[1] = 0.866237
final_layer_weights[2] = -0.377083
final_layer_weights[3] = 0.554739
final_layer_weights[4] = 1.002995
final_layer_weights[5] = 0.428092
final_layer_weights[6] = 0.188815
final_layer_weights[7] = 0.834246
final_layer_weights[8] = 1.054226
final_layer_weights[9] = 1.419609
.
.
.
final_layer_weights[1022] = 0.285294
final_layer_weights[1023] = -1.039476


Output Embedding: 0.071325, 0.276963 

 Expected Embedding: 1.038754, 36.179375 
 loss: 644.959561 


Updated weights for final layer:
final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[510] = -0.005465
semi_final_layer_weights[511] = 0.005538
Updating K Matrix:
k_matrix[0][0] = -0.560847
k_matrix[0][1] = 0.549518
k_matrix[1][0] = -0.558158
k_matrix[1][1] = 0.551223

Updating Q Matrix:
q_matrix[0][0] = 0.551756
q_matrix[0][1] = -0.563004
q_matrix[1][0] = 0.555113
q_matrix[1][1] = -0.555779

Updating V Matrix:
v_matrix[0][0] = -0.549740
v_matrix[0][1] = 0.553058
v_matrix[1][0] = -0.557862
v_matrix[1][1] = 0.557375

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
-0.560847 0.549518 
-0.558158 0.551223 

Q Matrix:
0.551756 -0.563004 
0.555113 -0.555779 

V Matrix:
-0.549740 0.553058 
-0.557862 0.557375 
Final K Matrix:
[-26.040895, 25.634057]
[-23.729781, 23.337308]
[-21.721866, 21.268836]
[21.204222, -20.945489]
[0.000000, 0.000000]
[-10.602300, 10.425725]
[7.449709, -7.305632]
[22.522571, -22.241580]
[-7.700356, 7.628112]
[26.675767, -26.278868]
Final Q Matrix:
[25.783649, -26.016771]
[23.465289, -23.730479]
[21.350041, -21.820324]
[-21.095107, 21.108901]
[0.000000, 0.000000]
[10.482437, -10.603887]
[-7.337812, 7.471694]
[-22.398133, 22.427763]
[7.690770, -7.643087]
[-26.439719, 26.630341]
Final V Matrix:
[-25.820625, 25.870616]
[-23.475156, 23.539697]
[-21.256309, 21.397274]
[21.204719, -21.182074]
[0.000000, 0.000000]
[-10.485507, 10.515410]
[7.318043, -7.356526]
[22.507795, -22.489133]
[-7.754375, 7.727188]
[26.499359, -26.533228]
 

Self-Attention Matrix: 
-23.475156 23.539697  
-23.475156 23.539697  
-23.475156 23.539697  
-25.820625 25.870616  
-24.647891 24.705157  
-23.475156 23.539697  
-25.820625 25.870616  
-25.820625 25.870616  
-23.475156 23.539697  
-25.820625 25.870616  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.374155 51.001745 
-1.082575 43.553699 
18.530085 20.249168 
-24.734819 -13.210061 
-24.647891 24.705157 
-13.190054 32.200210 
-37.636226 24.396179 
-26.083098 -14.217251 
-28.850209 42.736658 
-40.832722 -6.837475 




semi_final_layer_weights[0] = -0.005595
semi_final_layer_weights[1] = 0.005461
semi_final_layer_weights[2] = 0.005559
semi_final_layer_weights[3] = -0.005507
semi_final_layer_weights[4] = 0.005526
semi_final_layer_weights[5] = -0.005508
semi_final_layer_weights[6] = 0.005563
semi_final_layer_weights[7] = -0.005565
semi_final_layer_weights[8] = 0.005573
semi_final_layer_weights[9] = -0.005598
.
.
.
semi_final_layer_weights[511] = -0.005465
semi_final_layer_weights[512] = 0.005538


 Semi Final Layer Node Values: 
-0.002665 
0.237397 
0.221139 
0.214476 
0.005842 
-0.001102 
-0.000681 
0.229845 
0.082963 
0.272458 
.
.
.
0.005845 
-0.000058 
0.005873 
0.005847 
-0.000059 
0.005862 
0.005845 
-0.000059 
0.005928 
-0.000058 
-0.000058 
0.005856 


final_layer_weights[0] = 1.087737
final_layer_weights[1] = 0.876237
final_layer_weights[2] = -0.367083
final_layer_weights[3] = 0.544739
final_layer_weights[4] = 1.012995
final_layer_weights[5] = 0.418092
final_layer_weights[6] = 0.198815
final_layer_weights[7] = 0.824246
final_layer_weights[8] = 1.064226
final_layer_weights[9] = 1.409609
.
.
.
final_layer_weights[1022] = 0.275294
final_layer_weights[1023] = -1.029476


Output Embedding: 0.505584, 0.519473 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.462700 


Updated weights for final layer:
final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[510] = -0.004456
semi_final_layer_weights[511] = 0.004516
Updating K Matrix:
k_matrix[0][0] = -0.457299
k_matrix[0][1] = 0.448062
k_matrix[1][0] = -0.455107
k_matrix[1][1] = 0.449452

Updating Q Matrix:
q_matrix[0][0] = 0.449887
q_matrix[0][1] = -0.459058
q_matrix[1][0] = 0.452624
q_matrix[1][1] = -0.453167

Updating V Matrix:
v_matrix[0][0] = -0.448243
v_matrix[0][1] = 0.450949
v_matrix[1][0] = -0.454866
v_matrix[1][1] = 0.454468

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.457299 0.448062 
-0.455107 0.449452 

Q Matrix:
0.449887 -0.459058 
0.452624 -0.453167 

V Matrix:
-0.448243 0.450949 
-0.454866 0.454468 
Final K Matrix:
[-3.172348, 3.158626]
[-16.588466, 16.411805]
[7.548923, -7.336480]
[17.289350, -17.078387]
[14.158944, -13.987431]
[1.570141, -1.530750]
[14.295808, -14.055515]
[0.000000, 0.000000]
[-9.598640, 9.529600]
[-8.192796, 8.060040]
Final Q Matrix:
[3.190589, -3.132021]
[16.538725, -16.487031]
[-7.343614, 7.640521]
[-17.200381, 17.211628]
[-14.087817, 14.093988]
[-1.534073, 1.584189]
[-14.131139, 14.300248]
[0.000000, 0.000000]
[9.615761, -9.505332]
[8.105279, -8.190172]
Final V Matrix:
[-3.234372, 3.209164]
[-16.652698, 16.612495]
[7.250758, -7.347782]
[17.289755, -17.271292]
[14.162383, -14.146168]
[1.520016, -1.536008]
[14.132862, -14.175108]
[0.000000, 0.000000]
[-9.718075, 9.665835]
[-8.111695, 8.131580]
 

Self-Attention Matrix: 
-3.234372 3.209164  
-3.234372 3.209164  
-16.652698 16.612495  
-16.652698 16.612495  
-16.652698 16.612495  
-16.652698 16.612495  
-16.652698 16.612495  
-9.943535 9.910830  
-3.234372 3.209164  
-3.234372 3.209164  


Context Matrix (embedding_matrix + self_attention_matrix:
-10.462535 17.442701 
-11.519974 47.984276 
-50.029041 33.562475 
-15.566892 -22.468182 
-15.411038 -15.746391 
-22.245523 18.782212 
-34.281189 2.913929 
-9.943535 9.910830 
-17.362937 38.496743 
5.474727 12.460025 




semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[511] = -0.004456
semi_final_layer_weights[512] = 0.004516


 Semi Final Layer Node Values: 
-0.000364 
0.166820 
-0.000701 
0.175283 
-0.001449 
0.011063 
-0.001378 
-0.000044 
0.100579 
-0.000864 
.
.
.
0.004360 
-0.000044 
0.004381 
0.004361 
-0.000044 
0.004373 
0.004361 
-0.000044 
0.004422 
-0.000043 
-0.000043 
0.004368 


final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Output Embedding: 0.187951, 0.244895 

 Expected Embedding: -37.748848, 10.192697 
 loss: 769.079750 


Updated weights for final layer:
final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[510] = 0.005544
semi_final_layer_weights[511] = -0.005484
Updating K Matrix:
k_matrix[0][0] = 0.542701
k_matrix[0][1] = -0.551938
k_matrix[1][0] = 0.544893
k_matrix[1][1] = -0.550548

Updating Q Matrix:
q_matrix[0][0] = -0.550113
q_matrix[0][1] = 0.540942
q_matrix[1][0] = -0.547376
q_matrix[1][1] = 0.546833

Updating V Matrix:
v_matrix[0][0] = 0.551757
v_matrix[0][1] = -0.549051
v_matrix[1][0] = 0.545134
v_matrix[1][1] = -0.545532

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.542701 -0.551938 
0.544893 -0.550548 

Q Matrix:
-0.550113 0.540942 
-0.547376 0.546833 

V Matrix:
0.551757 -0.549051 
0.545134 -0.545532 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[511] = 0.005544
semi_final_layer_weights[512] = -0.005484


 Semi Final Layer Node Values: 
0.005438 
-0.000055 
-0.000055 
0.005510 
-0.000055 
0.005509 
-0.000055 
0.005462 
-0.000055 
0.005435 
.
.
.
-0.000055 
0.005501 
-0.000055 
-0.000055 
0.005417 
-0.000055 
-0.000055 
0.005420 
-0.000054 
0.005540 
0.005544 
-0.000055 


final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Output Embedding: -0.034716, 0.008506 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.603768 


Updated weights for final layer:
final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[510] = -0.004456
semi_final_layer_weights[511] = 0.004516
Updating K Matrix:
k_matrix[0][0] = -0.457299
k_matrix[0][1] = 0.448062
k_matrix[1][0] = -0.455107
k_matrix[1][1] = 0.449452

Updating Q Matrix:
q_matrix[0][0] = 0.449887
q_matrix[0][1] = -0.459058
q_matrix[1][0] = 0.452624
q_matrix[1][1] = -0.453167

Updating V Matrix:
v_matrix[0][0] = -0.448243
v_matrix[0][1] = 0.450949
v_matrix[1][0] = -0.454866
v_matrix[1][1] = 0.454468

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 15  total loss: 5049.976969 ******************************************************************* 

Epoch: 16
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.457299 0.448062 
-0.455107 0.449452 

Q Matrix:
0.449887 -0.459058 
0.452624 -0.453167 

V Matrix:
-0.448243 0.450949 
-0.454866 0.454468 
Final K Matrix:
[-10.948595, 10.710798]
[-6.738300, 6.561363]
[-16.914126, 16.680323]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.748103, -11.008085]
[6.572597, -6.806820]
[16.789159, -16.866694]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-10.690497, 10.769806]
[-6.503624, 6.579144]
[-16.846545, 16.852426]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-6.503624 6.579144  
-6.503624 6.579144  
-6.503624 6.579144  
-8.597061 8.674475  
-8.597061 8.674475  
-8.597061 8.674475  
-8.597061 8.674475  
-8.597061 8.674475  
-8.597061 8.674475  
-8.597061 8.674475  


Context Matrix (embedding_matrix + self_attention_matrix:
22.122189 1.872630 
19.717939 -4.962760 
0.147016 37.061635 
-8.597061 8.674475 
-8.597061 8.674475 
-8.597061 8.674475 
-8.597061 8.674475 
-8.597061 8.674475 
-8.597061 8.674475 
-8.597061 8.674475 




semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[511] = -0.004456
semi_final_layer_weights[512] = 0.004516


 Semi Final Layer Node Values: 
-0.001140 
0.061249 
0.173192 
-0.000048 
0.004854 
-0.000048 
0.004887 
-0.000049 
0.004896 
-0.000049 
.
.
.
0.004856 
-0.000048 
0.004880 
0.004858 
-0.000049 
0.004871 
0.004857 
-0.000049 
0.004925 
-0.000048 
-0.000048 
0.004865 


final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Output Embedding: 0.057875, 0.040121 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.436427 


Updated weights for final layer:
final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[510] = 0.005544
semi_final_layer_weights[511] = -0.005484
Updating K Matrix:
k_matrix[0][0] = 0.542701
k_matrix[0][1] = -0.551938
k_matrix[1][0] = 0.544893
k_matrix[1][1] = -0.550548

Updating Q Matrix:
q_matrix[0][0] = -0.550113
q_matrix[0][1] = 0.540942
q_matrix[1][0] = -0.547376
q_matrix[1][1] = 0.546833

Updating V Matrix:
v_matrix[0][0] = 0.551757
v_matrix[0][1] = -0.549051
v_matrix[1][0] = 0.545134
v_matrix[1][1] = -0.545532

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.542701 -0.551938 
0.544893 -0.550548 

Q Matrix:
-0.550113 0.540942 
-0.547376 0.546833 

V Matrix:
0.551757 -0.549051 
0.545134 -0.545532 
Final K Matrix:
[12.943886, -13.155032]
[7.823519, -7.908715]
[20.219004, -20.452807]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-13.108118, 12.911307]
[-7.864650, 7.847271]
[-20.343971, 20.266436]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[13.137476, -13.080834]
[7.836777, -7.839036]
[20.286585, -20.280704]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.836777 -7.839036  
7.836777 -7.839036  
7.836777 -7.839036  
10.487126 -10.459935  
10.487126 -10.459935  
10.487126 -10.459935  
10.487126 -10.459935  
10.487126 -10.459935  
10.487126 -10.459935  
10.487126 -10.459935  


Context Matrix (embedding_matrix + self_attention_matrix:
29.146500 -5.308111 
8.947764 5.412346 
14.487417 22.643454 
10.487126 -10.459935 
10.487126 -10.459935 
10.487126 -10.459935 
10.487126 -10.459935 
10.487126 -10.459935 
10.487126 -10.459935 
10.487126 -10.459935 




semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[511] = 0.005544
semi_final_layer_weights[512] = -0.005484


 Semi Final Layer Node Values: 
0.124200 
-0.000852 
-0.002085 
-0.000054 
0.005345 
-0.000054 
0.005315 
-0.000053 
0.005308 
-0.000053 
.
.
.
0.005343 
-0.000054 
0.005322 
0.005342 
-0.000053 
0.005330 
0.005343 
-0.000053 
0.005281 
-0.000054 
-0.000054 
0.005335 


final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Output Embedding: 0.142707, 0.135095 

 Expected Embedding: 12.470385, -17.532656 
 loss: 232.060520 


Updated weights for final layer:
final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[510] = -0.004456
semi_final_layer_weights[511] = 0.004516
Updating K Matrix:
k_matrix[0][0] = -0.457299
k_matrix[0][1] = 0.448062
k_matrix[1][0] = -0.455107
k_matrix[1][1] = 0.449452

Updating Q Matrix:
q_matrix[0][0] = 0.449887
q_matrix[0][1] = -0.459058
q_matrix[1][0] = 0.452624
q_matrix[1][1] = -0.453167

Updating V Matrix:
v_matrix[0][0] = -0.448243
v_matrix[0][1] = 0.450949
v_matrix[1][0] = -0.454866
v_matrix[1][1] = 0.454468

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
-0.457299 0.448062 
-0.455107 0.449452 

Q Matrix:
0.449887 -0.459058 
0.452624 -0.453167 

V Matrix:
-0.448243 0.450949 
-0.454866 0.454468 
Final K Matrix:
[3.147508, -3.076820]
[-18.323030, 17.934610]
[5.822131, -5.772664]
[-5.319783, 5.180827]
[-22.293965, 21.841782]
[-20.492202, 20.166260]
[11.073510, -10.885143]
[19.791629, -19.414101]
[-23.934227, 23.524196]
[15.493008, -15.295706]
Final Q Matrix:
[-3.086653, 3.167035]
[18.000703, -18.412629]
[-5.822015, 5.773453]
[5.189978, -5.373113]
[21.930024, -22.381686]
[20.281757, -20.479260]
[-10.942864, 11.079282]
[-19.501601, 19.844569]
[23.647845, -23.949720]
[-15.401860, 15.431976]
Final V Matrix:
[3.067544, -3.092375]
[-17.914759, 18.039149]
[5.875755, -5.850698]
[-5.136329, 5.195310]
[-21.847828, 21.981347]
[-20.304508, 20.348905]
[10.941747, -10.976424]
[19.454825, -19.552486]
[-23.642260, 23.719737]
[15.472899, -15.463564]
 

Self-Attention Matrix: 
-17.914759 18.039149  
3.067544 -3.092375  
-17.914759 18.039149  
3.067544 -3.092375  
3.067544 -3.092375  
3.067544 -3.092375  
-17.914759 18.039149  
-17.914759 18.039149  
3.067544 -3.092375  
-17.914759 18.039149  


Context Matrix (embedding_matrix + self_attention_matrix:
-26.798238 20.049447 
48.288698 -8.270432 
-11.480357 -1.219135 
23.562618 -11.997092 
52.346311 -3.622312 
23.131189 21.774553 
-32.198853 8.060385 
-54.938777 11.753652 
34.755197 17.657663 
-19.264619 -14.647054 




semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[511] = -0.004456
semi_final_layer_weights[512] = 0.004516


 Semi Final Layer Node Values: 
0.026225 
0.173739 
-0.000621 
-0.000474 
0.215015 
-0.002062 
-0.001050 
0.191424 
0.242716 
0.159354 
.
.
.
0.004732 
-0.000047 
0.004755 
0.004733 
-0.000048 
0.004746 
0.004732 
-0.000048 
0.004799 
-0.000047 
-0.000047 
0.004741 


final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Output Embedding: 0.804223, -0.191362 

 Expected Embedding: 19.465172, 25.668072 
 loss: 508.470676 


Updated weights for final layer:
final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[510] = 0.005544
semi_final_layer_weights[511] = -0.005484
Updating K Matrix:
k_matrix[0][0] = 0.542701
k_matrix[0][1] = -0.551938
k_matrix[1][0] = 0.544893
k_matrix[1][1] = -0.550548

Updating Q Matrix:
q_matrix[0][0] = -0.550113
q_matrix[0][1] = 0.540942
q_matrix[1][0] = -0.547376
q_matrix[1][1] = 0.546833

Updating V Matrix:
v_matrix[0][0] = 0.551757
v_matrix[0][1] = -0.549051
v_matrix[1][0] = 0.545134
v_matrix[1][1] = -0.545532

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
0.542701 -0.551938 
0.544893 -0.550548 

Q Matrix:
-0.550113 0.540942 
-0.547376 0.546833 

V Matrix:
0.551757 -0.549051 
0.545134 -0.545532 
Final K Matrix:
[20.012823, -20.241453]
[2.249629, -2.263403]
[14.414130, -14.604858]
[35.840860, -36.320666]
[20.455212, -20.685236]
[18.316018, -18.554281]
[-5.195096, 5.285738]
[12.962417, -13.133782]
[-8.583759, 8.750510]
[10.319022, -10.460185]
Final Q Matrix:
[-20.132695, 20.062628]
[-2.246762, 2.267439]
[-14.536175, 14.423316]
[-36.151942, 35.858011]
[-20.572719, 20.509864]
[-18.465488, 18.331886]
[5.269093, -5.175981]
[-13.071959, 12.970836]
[8.729296, -8.534783]
[-10.412725, 10.320866]
Final V Matrix:
[20.072895, -20.069473]
[2.227217, -2.237091]
[14.521125, -14.496244]
[36.120497, -36.053849]
[20.507663, -20.507313]
[18.441960, -18.413867]
[-5.287218, 5.259411]
[13.058259, -13.036017]
[-8.777506, 8.716951]
[10.406920, -10.385133]
 

Self-Attention Matrix: 
2.227217 -2.237091  
2.227217 -2.237091  
2.227217 -2.237091  
2.227217 -2.237091  
2.227217 -2.237091  
2.227217 -2.237091  
20.072895 -20.069473  
2.227217 -2.237091  
20.072895 -20.069473  
2.227217 -2.237091  


Context Matrix (embedding_matrix + self_attention_matrix:
8.036386 28.705098 
-0.428018 4.536030 
13.640315 12.848866 
32.147780 33.738682 
7.149773 30.400025 
15.593536 18.064337 
9.884016 -19.455745 
12.448061 11.372107 
-1.473608 -14.362768 
11.679693 7.286165 




semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[511] = 0.005544
semi_final_layer_weights[512] = -0.005484


 Semi Final Layer Node Values: 
0.205246 
-0.000283 
-0.001503 
0.368518 
-0.002118 
0.190926 
0.057762 
0.135575 
0.091857 
0.108524 
.
.
.
0.005510 
-0.000055 
0.005489 
0.005509 
-0.000054 
0.005497 
0.005510 
-0.000054 
0.005446 
-0.000056 
-0.000056 
0.005502 


final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Output Embedding: 0.468590, 0.783062 

 Expected Embedding: 4.643339, -27.140186 
 loss: 398.568157 


Updated weights for final layer:
final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[510] = -0.004456
semi_final_layer_weights[511] = 0.004516
Updating K Matrix:
k_matrix[0][0] = -0.457299
k_matrix[0][1] = 0.448062
k_matrix[1][0] = -0.455107
k_matrix[1][1] = 0.449452

Updating Q Matrix:
q_matrix[0][0] = 0.449887
q_matrix[0][1] = -0.459058
q_matrix[1][0] = 0.452624
q_matrix[1][1] = -0.453167

Updating V Matrix:
v_matrix[0][0] = -0.448243
v_matrix[0][1] = 0.450949
v_matrix[1][0] = -0.454866
v_matrix[1][1] = 0.454468

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
-0.457299 0.448062 
-0.455107 0.449452 

Q Matrix:
0.449887 -0.459058 
0.452624 -0.453167 

V Matrix:
-0.448243 0.450949 
-0.454866 0.454468 
Final K Matrix:
[-9.190861, 9.042229]
[10.068857, -9.894175]
[17.146997, -16.937248]
[-16.326385, 16.137163]
[-17.307153, 16.942546]
[10.225862, -10.096759]
[-11.068208, 10.892601]
[-28.980530, 28.518872]
[-20.585742, 20.332993]
[4.536995, -4.440426]
Final Q Matrix:
[9.093093, -9.187608]
[-9.945354, 10.077656]
[-17.058037, 17.070460]
[16.256192, -16.242569]
[17.005848, -17.389408]
[-10.167254, 10.184407]
[10.955153, -11.060758]
[28.681941, -28.962960]
[20.477647, -20.494833]
[-4.456641, 4.559590]
Final V Matrix:
[-9.100616, 9.122664]
[9.940608, -9.975112]
[17.146102, -17.128247]
[-16.351539, 16.325378]
[-16.927166, 17.042670]
[10.215373, -10.208231]
[-10.967924, 10.991518]
[-28.713345, 28.776746]
[-20.582348, 20.561730]
[4.434931, -4.466073]
 

Self-Attention Matrix: 
9.940608 -9.975112  
-9.100616 9.122664  
-9.100616 9.122664  
9.940608 -9.975112  
9.940608 -9.975112  
-9.100616 9.122664  
9.940608 -9.975112  
9.940608 -9.975112  
9.940608 -9.975112  
-9.100616 9.122664  


Context Matrix (embedding_matrix + self_attention_matrix:
19.627239 0.486545 
-23.046405 1.011475 
-8.170835 -29.488446 
6.104673 29.753034 
52.013675 -14.222090 
-9.675672 -12.768649 
20.653356 3.580550 
38.512893 24.993470 
9.087630 36.114732 
-20.408146 10.515589 




semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[511] = -0.004456
semi_final_layer_weights[512] = 0.004516


 Semi Final Layer Node Values: 
-0.000963 
-0.000937 
-0.001752 
-0.001655 
0.165761 
0.105291 
0.114467 
-0.002927 
0.209950 
0.040590 
.
.
.
-0.000045 
0.004527 
-0.000046 
-0.000045 
0.004611 
-0.000045 
-0.000045 
0.004609 
-0.000046 
0.004488 
0.004484 
-0.000045 


final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Output Embedding: 0.337941, 0.080339 

 Expected Embedding: 44.423141, -6.847703 
 loss: 995.751318 


Updated weights for final layer:
final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[510] = 0.005544
semi_final_layer_weights[511] = -0.005484
Updating K Matrix:
k_matrix[0][0] = 0.542701
k_matrix[0][1] = -0.551938
k_matrix[1][0] = 0.544893
k_matrix[1][1] = -0.550548

Updating Q Matrix:
q_matrix[0][0] = -0.550113
q_matrix[0][1] = 0.540942
q_matrix[1][0] = -0.547376
q_matrix[1][1] = 0.546833

Updating V Matrix:
v_matrix[0][0] = 0.551757
v_matrix[0][1] = -0.549051
v_matrix[1][0] = 0.545134
v_matrix[1][1] = -0.545532

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
0.542701 -0.551938 
0.544893 -0.550548 

Q Matrix:
-0.550113 0.540942 
-0.547376 0.546833 

V Matrix:
0.551757 -0.549051 
0.545134 -0.545532 
Final K Matrix:
[-26.092848, 26.413921]
[-0.465047, 0.496563]
[-17.822145, 18.110177]
[-4.152356, 4.229091]
[-6.486171, 6.593307]
[-17.158473, 17.388153]
[-15.146452, 15.311458]
[-18.990519, 19.300698]
[4.691828, -4.750746]
[-16.291601, 16.427893]
Final Q Matrix:
[26.280631, -26.134247]
[0.503738, -0.439372]
[18.044585, -17.780043]
[4.217373, -4.132690]
[6.570291, -6.468483]
[17.307370, -17.166706]
[15.226173, -15.192369]
[19.232015, -18.942313]
[-4.727218, 4.698058]
[16.320908, -16.383167]
Final V Matrix:
[-26.227378, 26.203145]
[-0.530524, 0.507904]
[-18.082114, 18.006440]
[-4.236467, 4.210555]
[-6.586433, 6.556905]
[-17.292292, 17.260404]
[-15.172280, 15.176597]
[-19.275516, 19.192072]
[4.718917, -4.713540]
[-16.218651, 16.258727]
 

Self-Attention Matrix: 
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-0.530524 0.507904  
-26.227378 26.203145  
-0.530524 0.507904  


Context Matrix (embedding_matrix + self_attention_matrix:
-14.480550 -33.484378 
-7.934530 7.028656 
-29.124982 -3.720286 
-9.863029 2.182363 
-11.577948 -0.392687 
-14.848521 -16.721321 
-2.699613 -25.128845 
-31.905290 -3.095390 
-23.390193 31.987924 
8.562673 -38.447422 




semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[511] = 0.005544
semi_final_layer_weights[512] = -0.005484


 Semi Final Layer Node Values: 
-0.002663 
-0.000005 
0.185039 
-0.000368 
0.071269 
-0.001794 
0.157512 
-0.001966 
-0.000524 
-0.001679 
.
.
.
-0.000054 
0.005372 
-0.000053 
-0.000054 
0.005290 
-0.000054 
-0.000054 
0.005293 
-0.000053 
0.005410 
0.005414 
-0.000054 


final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Output Embedding: -0.026075, 0.045259 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1061.988247 


Updated weights for final layer:
final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[510] = -0.004456
semi_final_layer_weights[511] = 0.004516
Updating K Matrix:
k_matrix[0][0] = -0.457299
k_matrix[0][1] = 0.448062
k_matrix[1][0] = -0.455107
k_matrix[1][1] = 0.449452

Updating Q Matrix:
q_matrix[0][0] = 0.449887
q_matrix[0][1] = -0.459058
q_matrix[1][0] = 0.452624
q_matrix[1][1] = -0.453167

Updating V Matrix:
v_matrix[0][0] = -0.448243
v_matrix[0][1] = 0.450949
v_matrix[1][0] = -0.454866
v_matrix[1][1] = 0.454468

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
-0.457299 0.448062 
-0.455107 0.449452 

Q Matrix:
0.449887 -0.459058 
0.452624 -0.453167 

V Matrix:
-0.448243 0.450949 
-0.454866 0.454468 
Final K Matrix:
[-12.885701, 12.723651]
[-3.335876, 3.318418]
[5.057574, -4.955220]
[17.289350, -17.078387]
[-23.691842, 23.315393]
[-18.832920, 18.485834]
[-14.847664, 14.647396]
[5.410376, -5.249497]
[4.637599, -4.540565]
[25.377555, -24.914916]
Final Q Matrix:
[12.812726, -12.832803]
[3.350868, -3.296630]
[-4.975326, 5.077237]
[-17.200381, 17.211628]
[23.449071, -23.676478]
[18.573756, -18.870616]
[14.744840, -14.800843]
[-5.251307, 5.485016]
[-4.557782, 4.658947]
[-25.035332, 25.423071]
Final V Matrix:
[-12.874054, 12.864502]
[-3.393588, 3.369725]
[4.956929, -4.987020]
[17.289755, -17.271292]
[-23.475796, 23.526789]
[-18.542567, 18.624882]
[-14.800661, 14.801484]
[5.175310, -5.252375]
[4.537427, -4.567797]
[24.998853, -25.105352]
 

Self-Attention Matrix: 
-3.393588 3.369725  
-3.393588 3.369725  
-12.874054 12.864502  
-12.874054 12.864502  
-3.393588 3.369725  
-3.393588 3.369725  
-3.393588 3.369725  
-12.874054 12.864502  
-12.874054 12.864502  
-12.874054 12.864502  


Context Matrix (embedding_matrix + self_attention_matrix:
-2.847060 31.134132 
-10.142325 17.480845 
-23.989304 12.920360 
-11.788248 -26.216175 
19.694537 32.228128 
28.417767 12.786436 
1.046511 31.532798 
-39.219673 27.448889 
-23.960856 13.814579 
-54.316128 -1.255532 




semi_final_layer_weights[0] = -0.004562
semi_final_layer_weights[1] = 0.004453
semi_final_layer_weights[2] = 0.004533
semi_final_layer_weights[3] = -0.004490
semi_final_layer_weights[4] = 0.004505
semi_final_layer_weights[5] = -0.004491
semi_final_layer_weights[6] = 0.004536
semi_final_layer_weights[7] = -0.004538
semi_final_layer_weights[8] = 0.004544
semi_final_layer_weights[9] = -0.004565
.
.
.
semi_final_layer_weights[511] = -0.004456
semi_final_layer_weights[512] = 0.004516


 Semi Final Layer Node Values: 
-0.001336 
0.037130 
-0.000456 
0.175145 
0.238437 
-0.001895 
0.152323 
0.048875 
-0.000416 
0.258221 
.
.
.
0.004432 
-0.000044 
0.004453 
0.004434 
-0.000045 
0.004445 
0.004433 
-0.000045 
0.004495 
-0.000044 
-0.000044 
0.004440 


final_layer_weights[0] = 1.088770
final_layer_weights[1] = 0.875229
final_layer_weights[2] = -0.368109
final_layer_weights[3] = 0.545756
final_layer_weights[4] = 1.011974
final_layer_weights[5] = 0.419109
final_layer_weights[6] = 0.197788
final_layer_weights[7] = 0.825273
final_layer_weights[8] = 1.063197
final_layer_weights[9] = 1.410642
.
.
.
final_layer_weights[1022] = 0.276303
final_layer_weights[1023] = -1.030498


Output Embedding: 0.857234, 0.178218 

 Expected Embedding: 1.038754, 36.179375 
 loss: 648.058123 


Updated weights for final layer:
final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[510] = 0.005544
semi_final_layer_weights[511] = -0.005484
Updating K Matrix:
k_matrix[0][0] = 0.542701
k_matrix[0][1] = -0.551938
k_matrix[1][0] = 0.544893
k_matrix[1][1] = -0.550548

Updating Q Matrix:
q_matrix[0][0] = -0.550113
q_matrix[0][1] = 0.540942
q_matrix[1][0] = -0.547376
q_matrix[1][1] = 0.546833

Updating V Matrix:
v_matrix[0][0] = 0.551757
v_matrix[0][1] = -0.549051
v_matrix[1][0] = 0.545134
v_matrix[1][1] = -0.545532

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
0.542701 -0.551938 
0.544893 -0.550548 

Q Matrix:
-0.550113 0.540942 
-0.547376 0.546833 

V Matrix:
0.551757 -0.549051 
0.545134 -0.545532 
Final K Matrix:
[25.330007, -25.661731]
[23.057961, -23.377973]
[21.003290, -21.372678]
[-20.705521, 20.916485]
[0.000000, 0.000000]
[10.300786, -10.444761]
[-7.215746, 7.333222]
[-21.986044, 22.215157]
[7.543245, -7.602151]
[-25.969487, 26.293109]
Final Q Matrix:
[-25.539758, 25.349676]
[-23.273621, 23.057392]
[-21.306466, 20.923009]
[20.794491, -20.783243]
[0.000000, 0.000000]
[-10.398519, 10.299493]
[7.306984, -7.197819]
[22.087507, -22.063348]
[-7.551061, 7.589941]
[26.161955, -26.006527]
Final V Matrix:
[25.509609, -25.468847]
[23.265575, -23.212950]
[21.382892, -21.267952]
[-20.705116, 20.723580]
[0.000000, 0.000000]
[10.396017, -10.371634]
[-7.323103, 7.291725]
[-21.998092, 22.013308]
[7.499200, -7.521367]
[-26.113326, 26.085710]
 

Self-Attention Matrix: 
23.265575 -23.212950  
23.265575 -23.212950  
23.265575 -23.212950  
25.509609 -25.468847  
24.387592 -24.340899  
23.265575 -23.212950  
25.509609 -25.468847  
25.509609 -25.468847  
23.265575 -23.212950  
25.509609 -25.468847  


Context Matrix (embedding_matrix + self_attention_matrix:
42.366577 4.249098 
45.658157 -3.198948 
65.270816 -26.503479 
26.595414 -64.549524 
24.387592 -24.340899 
33.550678 -14.552437 
13.694008 -26.943284 
25.247136 -65.556715 
17.890522 -4.015990 
10.497512 -58.176939 




semi_final_layer_weights[0] = 0.005438
semi_final_layer_weights[1] = -0.005547
semi_final_layer_weights[2] = -0.005467
semi_final_layer_weights[3] = 0.005510
semi_final_layer_weights[4] = -0.005495
semi_final_layer_weights[5] = 0.005509
semi_final_layer_weights[6] = -0.005464
semi_final_layer_weights[7] = 0.005462
semi_final_layer_weights[8] = -0.005456
semi_final_layer_weights[9] = 0.005435
.
.
.
semi_final_layer_weights[511] = 0.005544
semi_final_layer_weights[512] = -0.005484


 Semi Final Layer Node Values: 
0.258944 
-0.002300 
-0.002065 
-0.002146 
0.005238 
0.099150 
0.077855 
-0.002256 
-0.000702 
-0.002646 
.
.
.
0.005236 
-0.000052 
0.005215 
0.005235 
-0.000052 
0.005223 
0.005235 
-0.000052 
0.005175 
-0.000053 
-0.000053 
0.005228 


final_layer_weights[0] = 1.098770
final_layer_weights[1] = 0.865229
final_layer_weights[2] = -0.378109
final_layer_weights[3] = 0.555756
final_layer_weights[4] = 1.001974
final_layer_weights[5] = 0.429109
final_layer_weights[6] = 0.187788
final_layer_weights[7] = 0.835273
final_layer_weights[8] = 1.053197
final_layer_weights[9] = 1.420642
.
.
.
final_layer_weights[1022] = 0.286303
final_layer_weights[1023] = -1.040498


Output Embedding: 0.768077, 0.507300 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.952093 


Updated weights for final layer:
final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[510] = 0.004493
semi_final_layer_weights[511] = -0.004445
Updating K Matrix:
k_matrix[0][0] = 0.439848
k_matrix[0][1] = -0.447334
k_matrix[1][0] = 0.441624
k_matrix[1][1] = -0.446207

Updating Q Matrix:
q_matrix[0][0] = -0.445855
q_matrix[0][1] = 0.438422
q_matrix[1][0] = -0.443636
q_matrix[1][1] = 0.443197

Updating V Matrix:
v_matrix[0][0] = 0.447187
v_matrix[0][1] = -0.444995
v_matrix[1][0] = 0.441820
v_matrix[1][1] = -0.442142

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
0.439848 -0.447334 
0.441624 -0.446207 

Q Matrix:
-0.445855 0.438422 
-0.443636 0.443197 

V Matrix:
0.447187 -0.444995 
0.441820 -0.442142 
Final K Matrix:
[3.106587, -3.117709]
[16.129380, -16.272560]
[-7.194979, 7.367160]
[-16.781392, 16.952373]
[-13.744333, 13.883341]
[-1.501791, 1.533716]
[-13.803470, 13.998224]
[0.000000, 0.000000]
[9.369441, -9.425396]
[7.916083, -8.023679]
Final Q Matrix:
[-3.091804, 3.139272]
[-16.169694, 16.211591]
[7.361378, -7.120741]
[16.853499, -16.844384]
[13.801980, -13.796979]
[1.531023, -1.490405]
[13.936932, -13.799873]
[0.000000, 0.000000]
[-9.355565, 9.445066]
[-7.987014, 7.918210]
Final V Matrix:
[3.056318, -3.076749]
[16.077321, -16.109905]
[-7.436636, 7.358000]
[-16.781063, 16.796028]
[-13.741546, 13.754688]
[-1.542416, 1.529455]
[-13.935536, 13.901296]
[0.000000, 0.000000]
[9.272641, -9.314981]
[7.981813, -7.965697]
 

Self-Attention Matrix: 
3.056318 -3.076749  
3.056318 -3.076749  
16.077321 -16.109905  
16.077321 -16.109905  
16.077321 -16.109905  
16.077321 -16.109905  
16.077321 -16.109905  
9.566820 -9.593327  
3.056318 -3.076749  
3.056318 -3.076749  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.171844 11.156788 
-5.229283 41.698363 
-17.299021 0.840075 
17.163127 -55.190582 
17.318981 -48.468792 
10.484497 -13.940188 
-1.551169 -29.808471 
9.566820 -9.593327 
-11.072247 32.210829 
11.765418 6.174112 




semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[511] = 0.004493
semi_final_layer_weights[512] = -0.004445


 Semi Final Layer Node Values: 
0.035194 
-0.001685 
0.068500 
-0.001743 
0.143172 
-0.000199 
0.143297 
-0.000045 
-0.000979 
0.083435 
.
.
.
0.004570 
-0.000046 
0.004552 
0.004568 
-0.000045 
0.004558 
0.004569 
-0.000045 
0.004516 
-0.000046 
-0.000046 
0.004563 


final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Output Embedding: 0.689214, 0.197227 

 Expected Embedding: -37.748848, 10.192697 
 loss: 788.697002 


Updated weights for final layer:
final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[510] = -0.005507
semi_final_layer_weights[511] = 0.005555
Updating K Matrix:
k_matrix[0][0] = -0.560152
k_matrix[0][1] = 0.552666
k_matrix[1][0] = -0.558376
k_matrix[1][1] = 0.553793

Updating Q Matrix:
q_matrix[0][0] = 0.554145
q_matrix[0][1] = -0.561578
q_matrix[1][0] = 0.556364
q_matrix[1][1] = -0.556803

Updating V Matrix:
v_matrix[0][0] = -0.552813
v_matrix[0][1] = 0.555005
v_matrix[1][0] = -0.558180
v_matrix[1][1] = 0.557858

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.560152 0.552666 
-0.558376 0.553793 

Q Matrix:
0.554145 -0.561578 
0.556364 -0.556803 

V Matrix:
-0.552813 0.555005 
-0.558180 0.557858 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[511] = -0.005507
semi_final_layer_weights[512] = 0.005555


 Semi Final Layer Node Values: 
-0.000056 
0.005504 
0.005569 
-0.000055 
0.005547 
-0.000055 
0.005572 
-0.000056 
0.005578 
-0.000056 
.
.
.
0.005548 
-0.000055 
0.005566 
0.005550 
-0.000056 
0.005559 
0.005549 
-0.000056 
0.005600 
-0.000055 
-0.000055 
0.005555 


final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Output Embedding: 0.076280, 0.045674 

 Expected Embedding: -10.885829, -17.296432 
 loss: 210.458255 


Updated weights for final layer:
final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[510] = 0.004493
semi_final_layer_weights[511] = -0.004445
Updating K Matrix:
k_matrix[0][0] = 0.439848
k_matrix[0][1] = -0.447334
k_matrix[1][0] = 0.441624
k_matrix[1][1] = -0.446207

Updating Q Matrix:
q_matrix[0][0] = -0.445855
q_matrix[0][1] = 0.438422
q_matrix[1][0] = -0.443636
q_matrix[1][1] = 0.443197

Updating V Matrix:
v_matrix[0][0] = 0.447187
v_matrix[0][1] = -0.444995
v_matrix[1][0] = 0.441820
v_matrix[1][1] = -0.442142

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 16  total loss: 5094.440818 ******************************************************************* 

Epoch: 17
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.439848 -0.447334 
0.441624 -0.446207 

Q Matrix:
-0.445855 0.438422 
-0.443636 0.443197 

V Matrix:
0.447187 -0.444995 
0.441820 -0.442142 
Final K Matrix:
[10.512483, -10.705213]
[6.436305, -6.579709]
[16.387080, -16.576572]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-10.674978, 10.464268]
[-6.570604, 6.380771]
[-16.488363, 16.425522]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[10.721666, -10.657389]
[6.626505, -6.565298]
[16.441853, -16.437086]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
6.626505 -6.565298  
6.626505 -6.565298  
6.626505 -6.565298  
8.674086 -8.611343  
8.674086 -8.611343  
8.674086 -8.611343  
8.674086 -8.611343  
8.674086 -8.611343  
8.674086 -8.611343  
8.674086 -8.611343  


Context Matrix (embedding_matrix + self_attention_matrix:
35.252318 -11.271812 
32.848069 -18.107202 
13.277145 23.917193 
8.674086 -8.611343 
8.674086 -8.611343 
8.674086 -8.611343 
8.674086 -8.611343 
8.674086 -8.611343 
8.674086 -8.611343 
8.674086 -8.611343 




semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[511] = 0.004493
semi_final_layer_weights[512] = -0.004445


 Semi Final Layer Node Values: 
0.101288 
-0.000618 
-0.001692 
-0.000042 
0.004174 
-0.000042 
0.004150 
-0.000041 
0.004144 
-0.000041 
.
.
.
0.004172 
-0.000042 
0.004156 
0.004171 
-0.000041 
0.004162 
0.004172 
-0.000041 
0.004124 
-0.000042 
-0.000042 
0.004166 


final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Output Embedding: 0.112257, 0.107247 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.944754 


Updated weights for final layer:
final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[510] = -0.005507
semi_final_layer_weights[511] = 0.005555
Updating K Matrix:
k_matrix[0][0] = -0.560152
k_matrix[0][1] = 0.552666
k_matrix[1][0] = -0.558376
k_matrix[1][1] = 0.553793

Updating Q Matrix:
q_matrix[0][0] = 0.554145
q_matrix[0][1] = -0.561578
q_matrix[1][0] = 0.556364
q_matrix[1][1] = -0.556803

Updating V Matrix:
v_matrix[0][0] = -0.552813
v_matrix[0][1] = 0.555005
v_matrix[1][0] = -0.558180
v_matrix[1][1] = 0.557858

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.560152 0.552666 
-0.558376 0.553793 

Q Matrix:
0.554145 -0.561578 
0.556364 -0.556803 

V Matrix:
-0.552813 0.555005 
-0.558180 0.557858 
Final K Matrix:
[-13.349899, 13.178769]
[-8.021570, 7.952521]
[-20.746050, 20.556558]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[13.216793, -13.376304]
[7.988234, -8.002320]
[20.644767, -20.707608]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-13.192999, 13.238905]
[-8.010825, 8.008994]
[-20.691277, 20.696044]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-8.010825 8.008994  
-8.010825 8.008994  
-8.010825 8.008994  
-10.601912 10.623950  
-10.601912 10.623950  
-10.601912 10.623950  
-10.601912 10.623950  
-10.601912 10.623950  
-10.601912 10.623950  
-10.601912 10.623950  


Context Matrix (embedding_matrix + self_attention_matrix:
13.298898 10.539919 
-6.899838 21.260376 
-1.360186 38.491485 
-10.601912 10.623950 
-10.601912 10.623950 
-10.601912 10.623950 
-10.601912 10.623950 
-10.601912 10.623950 
-10.601912 10.623950 
-10.601912 10.623950 




semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[511] = -0.005507
semi_final_layer_weights[512] = 0.005555


 Semi Final Layer Node Values: 
-0.001389 
0.084546 
0.212351 
-0.000057 
0.005669 
-0.000057 
0.005695 
-0.000057 
0.005701 
-0.000057 
.
.
.
0.005671 
-0.000057 
0.005689 
0.005672 
-0.000057 
0.005682 
0.005671 
-0.000057 
0.005724 
-0.000056 
-0.000056 
0.005678 


final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Output Embedding: 0.073424, 0.047154 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.367164 


Updated weights for final layer:
final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[510] = 0.004493
semi_final_layer_weights[511] = -0.004445
Updating K Matrix:
k_matrix[0][0] = 0.439848
k_matrix[0][1] = -0.447334
k_matrix[1][0] = 0.441624
k_matrix[1][1] = -0.446207

Updating Q Matrix:
q_matrix[0][0] = -0.445855
q_matrix[0][1] = 0.438422
q_matrix[1][0] = -0.443636
q_matrix[1][1] = 0.443197

Updating V Matrix:
v_matrix[0][0] = 0.447187
v_matrix[0][1] = -0.444995
v_matrix[1][0] = 0.441820
v_matrix[1][1] = -0.442142

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 3: Current sample's first 10 elements:  7.000000,  8.000000,  9.000000,  10.000000,  11.000000,  12.000000,  13.000000,  14.000000,  15.000000,  16.000000, 
y_actual token: 12 
max sentence length: 512 
Embedding Matrix:
 [-8.883479, 1.010298] 
 [44.741726, -6.055640] 
 [5.592932, -19.798586] 
 [19.497578, -8.975454] 
 [48.369469, -0.113791] 
 [19.465172, 25.668072] 
 [-14.425214, -8.988771] 
 [-36.673233, -5.349040] 
 [32.444454, 21.403683] 
 [-0.372329, -32.475407] 
Processing Sentence 3...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-8.883479, 2.010298] 
 [45.221153, -5.178057] 
 [6.434402, -19.258284] 
 [20.495073, -8.904717] 
 [49.278767, -0.529937] 
 [20.063644, 24.866928] 
 [-14.284094, -9.978764] 
 [-37.024017, -6.285496] 
 [31.687653, 20.750038] 
 [-1.349859, -32.686203] 
 Self attention block 
K Matrix:
0.439848 -0.447334 
0.441624 -0.446207 

Q Matrix:
-0.445855 0.438422 
-0.443636 0.443197 

V Matrix:
0.447187 -0.444995 
0.441820 -0.442142 
Final K Matrix:
[-3.019580, 3.076871]
[17.603659, -17.918465]
[-5.674772, 5.714864]
[5.082168, -5.194789]
[21.441114, -21.807599]
[19.806788, -20.070957]
[-10.689690, 10.842358]
[-19.060754, 19.366732]
[23.101461, -23.433782]
[-15.028757, 15.188666]
Final Q Matrix:
[3.068901, -3.003753]
[-17.864898, 17.531041]
[5.674866, -5.714224]
[-5.187372, 5.038946]
[-21.736080, 21.370018]
[-19.977349, 19.817277]
[10.795576, -10.685012]
[19.295815, -19.017846]
[-23.333568, 23.088904]
[15.102631, -15.078223]
Final V Matrix:
[-3.084389, 3.064264]
[17.934554, -17.833739]
[-5.631311, 5.651619]
[5.230854, -5.183051]
[21.802699, -21.694484]
[19.958910, -19.922927]
[-10.796481, 10.768376]
[-19.333726, 19.254574]
[23.338094, -23.275300]
[-15.045056, 15.052621]
 

Self-Attention Matrix: 
17.934554 -17.833739  
-3.084389 3.064264  
17.934554 -17.833739  
-3.084389 3.064264  
-3.084389 3.064264  
-3.084389 3.064264  
17.934554 -17.833739  
17.934554 -17.833739  
-3.084389 3.064264  
17.934554 -17.833739  


Context Matrix (embedding_matrix + self_attention_matrix:
9.051075 -15.823441 
42.136765 -2.113793 
24.368956 -37.092023 
17.410685 -5.840453 
46.194378 2.534327 
16.979256 27.931192 
3.650460 -27.812503 
-19.089463 -24.119235 
28.603264 23.814302 
16.584695 -50.519942 




semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[511] = 0.004493
semi_final_layer_weights[512] = -0.004445


 Semi Final Layer Node Values: 
-0.000343 
-0.001754 
0.060808 
0.047201 
-0.002215 
0.204983 
0.111424 
-0.001957 
-0.002362 
-0.001539 
.
.
.
0.004272 
-0.000043 
0.004255 
0.004271 
-0.000042 
0.004261 
0.004272 
-0.000042 
0.004222 
-0.000043 
-0.000043 
0.004265 


final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Output Embedding: -0.135971, 0.003384 

 Expected Embedding: 19.465172, 25.668072 
 loss: 521.440495 


Updated weights for final layer:
final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[510] = -0.005507
semi_final_layer_weights[511] = 0.005555
Updating K Matrix:
k_matrix[0][0] = -0.560152
k_matrix[0][1] = 0.552666
k_matrix[1][0] = -0.558376
k_matrix[1][1] = 0.553793

Updating Q Matrix:
q_matrix[0][0] = 0.554145
q_matrix[0][1] = -0.561578
q_matrix[1][0] = 0.556364
q_matrix[1][1] = -0.556803

Updating V Matrix:
v_matrix[0][0] = -0.552813
v_matrix[0][1] = 0.555005
v_matrix[1][0] = -0.558180
v_matrix[1][1] = 0.557858

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 4: Current sample's first 10 elements:  3.000000,  36.000000,  28.000000,  29.000000,  37.000000,  38.000000,  18.000000,  28.000000,  39.000000,  40.000000, 
y_actual token: 48 
max sentence length: 512 
Embedding Matrix:
 [5.809169, 29.942188] 
 [-3.134660, 5.895538] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [4.013258, 33.053261] 
 [12.767848, 21.102571] 
 [-10.329999, 1.603721] 
 [10.571628, 14.545654] 
 [-20.789701, 6.360349] 
 [10.430007, 9.734051] 
Processing Sentence 4...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [5.809169, 30.942188] 
 [-2.655235, 6.773121] 
 [11.413098, 15.085957] 
 [29.920563, 35.975773] 
 [4.922556, 32.637115] 
 [13.366320, 20.301428] 
 [-10.188879, 0.613728] 
 [10.220844, 13.609198] 
 [-21.546503, 5.706706] 
 [9.452477, 9.523255] 
 Self attention block 
K Matrix:
-0.560152 0.552666 
-0.558376 0.553793 

Q Matrix:
0.554145 -0.561578 
0.556364 -0.556803 

V Matrix:
-0.552813 0.555005 
-0.558180 0.557858 
Final K Matrix:
[-20.531383, 20.346083]
[-2.294609, 2.283445]
[-14.816704, 14.662123]
[-36.848069, 36.459195]
[-20.981150, 20.794720]
[-18.822998, 18.629891]
[5.364634, -5.291170]
[-13.324274, 13.185386]
[8.882840, -8.747692]
[-10.612381, 10.497971]
Final Q Matrix:
[20.434229, -20.491017]
[2.296933, -2.280174]
[14.717789, -14.809259]
[36.595943, -36.834168]
[20.885913, -20.936856]
[18.701856, -18.810137]
[-5.304661, 5.380127]
[13.235493, -13.317451]
[-8.764886, 8.922534]
[10.536436, -10.610886]
Final V Matrix:
[-20.482696, 20.485469]
[-2.312773, 2.304771]
[-14.729987, 14.750152]
[-36.621429, 36.675445]
[-20.938639, 20.938923]
[-18.720925, 18.743693]
[5.289971, -5.312508]
[-13.246596, 13.264623]
[8.725812, -8.774891]
[-10.541141, 10.558799]
 

Self-Attention Matrix: 
-2.312773 2.304771  
-2.312773 2.304771  
-2.312773 2.304771  
-2.312773 2.304771  
-2.312773 2.304771  
-2.312773 2.304771  
-20.482696 20.485469  
-2.312773 2.304771  
-20.482696 20.485469  
-2.312773 2.304771  


Context Matrix (embedding_matrix + self_attention_matrix:
3.496395 33.246959 
-4.968008 9.077892 
9.100325 17.390727 
27.607789 38.280544 
2.609782 34.941886 
11.053546 22.606199 
-30.671575 21.099197 
7.908071 15.913969 
-42.029199 26.192175 
7.139703 11.828026 




semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[511] = -0.005507
semi_final_layer_weights[512] = 0.005555


 Semi Final Layer Node Values: 
-0.002111 
0.028125 
0.153096 
-0.003702 
0.213836 
-0.001918 
-0.000478 
-0.001383 
-0.000828 
-0.001117 
.
.
.
0.005534 
-0.000055 
0.005551 
0.005535 
-0.000056 
0.005545 
0.005534 
-0.000056 
0.005586 
-0.000055 
-0.000055 
0.005541 


final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Output Embedding: 0.273869, 0.048099 

 Expected Embedding: 4.643339, -27.140186 
 loss: 379.147556 


Updated weights for final layer:
final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[510] = 0.004493
semi_final_layer_weights[511] = -0.004445
Updating K Matrix:
k_matrix[0][0] = 0.439848
k_matrix[0][1] = -0.447334
k_matrix[1][0] = 0.441624
k_matrix[1][1] = -0.446207

Updating Q Matrix:
q_matrix[0][0] = -0.445855
q_matrix[0][1] = 0.438422
q_matrix[1][0] = -0.443636
q_matrix[1][1] = 0.443197

Updating V Matrix:
v_matrix[0][0] = 0.447187
v_matrix[0][1] = -0.444995
v_matrix[1][0] = 0.441820
v_matrix[1][1] = -0.442142

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 5: Current sample's first 10 elements:  49.000000,  13.000000,  22.000000,  50.000000,  46.000000,  51.000000,  28.000000,  29.000000,  19.000000,  18.000000, 
y_actual token: 52 
max sentence length: 512 
Embedding Matrix:
 [9.686630, 9.461657] 
 [-14.425214, -8.988771] 
 [0.088311, -39.151413] 
 [-4.833431, 39.657410] 
 [41.163769, -3.830831] 
 [-1.173527, -21.090170] 
 [10.571628, 14.545654] 
 [28.923067, 35.905037] 
 [-0.096176, 46.743488] 
 [-10.329999, 1.603721] 
Processing Sentence 5...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [9.686630, 10.461657] 
 [-13.945788, -8.111189] 
 [0.929782, -38.611111] 
 [-3.835936, 39.728146] 
 [42.073067, -4.246978] 
 [-0.575055, -21.891314] 
 [10.712748, 13.555662] 
 [28.572285, 34.968582] 
 [-0.852979, 46.089844] 
 [-11.307529, 1.392925] 
 Self attention block 
K Matrix:
0.439848 -0.447334 
0.441624 -0.446207 

Q Matrix:
-0.445855 0.438422 
-0.443636 0.443197 

V Matrix:
0.447187 -0.444995 
0.441820 -0.442142 
Final K Matrix:
[8.880764, -9.001227]
[-9.716121, 9.857697]
[-16.642647, 16.812644]
[15.857692, -16.011052]
[16.630168, -16.925674]
[-9.920675, 10.025310]
[10.698488, -10.840813]
[28.010430, -28.384594]
[19.979219, -20.184067]
[-4.358440, 4.436707]
Final Q Matrix:
[-8.960003, 8.883401]
[9.816217, -9.708989]
[16.714747, -16.704678]
[-15.914581, 15.925623]
[-16.874369, 16.563502]
[9.968175, -9.954273]
[-10.790116, 10.704526]
[-28.252430, 28.024670]
[-20.066828, 20.052899]
[4.423565, -4.340127]
Final V Matrix:
[8.953906, -8.936036]
[-9.820063, 9.792099]
[-16.643371, 16.657843]
[15.837305, -15.858507]
[16.938139, -16.844526]
[-9.929176, 9.934964]
[10.779766, -10.760643]
[28.226977, -28.175592]
[19.981970, -19.998681]
[-4.441161, 4.415920]
 

Self-Attention Matrix: 
-9.820063 9.792099  
8.953906 -8.936036  
8.953906 -8.936036  
-9.820063 9.792099  
-9.820063 9.792099  
8.953906 -8.936036  
-9.820063 9.792099  
-9.820063 9.792099  
-9.820063 9.792099  
8.953906 -8.936036  


Context Matrix (embedding_matrix + self_attention_matrix:
-0.133433 20.253756 
-4.991883 -17.047225 
9.883687 -47.547147 
-13.655999 49.520245 
32.253003 5.545121 
8.378850 -30.827350 
0.892684 23.347761 
18.752221 44.760681 
-10.673042 55.881943 
-2.353624 -7.543111 




semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[511] = 0.004493
semi_final_layer_weights[512] = -0.004445


 Semi Final Layer Node Values: 
0.093089 
0.103582 
0.171320 
0.164614 
-0.001728 
-0.001047 
-0.001118 
0.285603 
-0.002043 
-0.000480 
.
.
.
-0.000044 
0.004436 
-0.000044 
-0.000044 
0.004368 
-0.000044 
-0.000044 
0.004371 
-0.000044 
0.004467 
0.004471 
-0.000044 


final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Output Embedding: 0.260128, 0.279009 

 Expected Embedding: 44.423141, -6.847703 
 loss: 1000.580890 


Updated weights for final layer:
final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[510] = -0.005507
semi_final_layer_weights[511] = 0.005555
Updating K Matrix:
k_matrix[0][0] = -0.560152
k_matrix[0][1] = 0.552666
k_matrix[1][0] = -0.558376
k_matrix[1][1] = 0.553793

Updating Q Matrix:
q_matrix[0][0] = 0.554145
q_matrix[0][1] = -0.561578
q_matrix[1][0] = 0.556364
q_matrix[1][1] = -0.556803

Updating V Matrix:
v_matrix[0][0] = -0.552813
v_matrix[0][1] = 0.555005
v_matrix[1][0] = -0.558180
v_matrix[1][1] = 0.557858

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 6: Current sample's first 10 elements:  53.000000,  54.000000,  27.000000,  18.000000,  55.000000,  56.000000,  57.000000,  58.000000,  59.000000,  60.000000, 
y_actual token: 29 
max sentence length: 512 
Embedding Matrix:
 [-13.950026, -34.992283] 
 [-7.883431, 5.643169] 
 [-29.435928, -4.768492] 
 [-10.329999, 1.603721] 
 [-11.956720, -0.484444] 
 [-14.916470, -16.428082] 
 [-2.310209, -24.646757] 
 [-31.023983, -2.666838] 
 [3.593988, 6.438422] 
 [10.070727, -38.744530] 
Processing Sentence 6...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-13.950026, -33.992283] 
 [-7.404006, 6.520751] 
 [-28.594458, -4.228190] 
 [-9.332504, 1.674458] 
 [-11.047423, -0.900591] 
 [-14.317997, -17.229225] 
 [-2.169089, -25.636749] 
 [-31.374765, -3.603295] 
 [2.837185, 5.784778] 
 [9.093197, -38.955326] 
 Self attention block 
K Matrix:
-0.560152 0.552666 
-0.558376 0.553793 

Q Matrix:
0.554145 -0.561578 
0.556364 -0.556803 

V Matrix:
-0.552813 0.555005 
-0.558180 0.557858 
Final K Matrix:
[26.794601, -26.534378]
[0.506343, -0.480800]
[18.378172, -18.144728]
[4.292648, -4.230457]
[6.691109, -6.604278]
[17.640639, -17.454488]
[15.529955, -15.396222]
[19.586642, -19.335248]
[-4.819335, 4.771583]
[16.658127, -16.547666]
Final Q Matrix:
[-26.642407, 26.761048]
[-0.474985, 0.527152]
[-18.197889, 18.412295]
[-4.239954, 4.308587]
[-6.622932, 6.705445]
[-17.519961, 17.633966]
[-15.465344, 15.492741]
[-19.390914, 19.625712]
[4.790652, -4.814285]
[-16.634375, 16.583915]
Final V Matrix:
[26.685567, -26.705207]
[0.453275, -0.471608]
[18.167472, -18.228804]
[4.224478, -4.245479]
[6.609849, -6.633780]
[17.532182, -17.558026]
[15.509023, -15.505523]
[19.355658, -19.423287]
[-4.797380, 4.801738]
[16.717252, -16.684771]
 

Self-Attention Matrix: 
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
0.453275 -0.471608  
26.685567 -26.705207  
0.453275 -0.471608  


Context Matrix (embedding_matrix + self_attention_matrix:
-13.496750 -34.463891 
-6.950730 6.049143 
-28.141182 -4.699798 
-8.879229 1.202850 
-10.594148 -1.372200 
-13.864722 -17.700833 
-1.715814 -26.108357 
-30.921490 -4.074903 
29.522753 -20.920429 
9.546472 -39.426934 




semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[511] = -0.005507
semi_final_layer_weights[512] = 0.005555


 Semi Final Layer Node Values: 
0.273810 
0.000542 
-0.001885 
0.036951 
-0.000719 
0.180256 
-0.001606 
0.200605 
0.042407 
0.172765 
.
.
.
-0.000057 
0.005647 
-0.000057 
-0.000057 
0.005716 
-0.000057 
-0.000057 
0.005714 
-0.000057 
0.005615 
0.005611 
-0.000057 


final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Output Embedding: 0.323202, 0.255730 

 Expected Embedding: 28.923067, 35.905037 
 loss: 1044.412686 


Updated weights for final layer:
final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[510] = 0.004493
semi_final_layer_weights[511] = -0.004445
Updating K Matrix:
k_matrix[0][0] = 0.439848
k_matrix[0][1] = -0.447334
k_matrix[1][0] = 0.441624
k_matrix[1][1] = -0.446207

Updating Q Matrix:
q_matrix[0][0] = -0.445855
q_matrix[0][1] = 0.438422
q_matrix[1][0] = -0.443636
q_matrix[1][1] = 0.443197

Updating V Matrix:
v_matrix[0][0] = 0.447187
v_matrix[0][1] = -0.444995
v_matrix[1][0] = 0.441820
v_matrix[1][1] = -0.442142

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 7: Current sample's first 10 elements:  66.000000,  67.000000,  55.000000,  22.000000,  32.000000,  68.000000,  69.000000,  70.000000,  18.000000,  71.000000, 
y_actual token: 78 
max sentence length: 512 
Embedding Matrix:
 [0.546528, 26.764406] 
 [-7.228162, 13.233537] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [22.178827, 29.274549] 
 [31.212883, 10.217854] 
 [4.298979, 29.153065] 
 [-25.994837, 15.520844] 
 [-10.329999, 1.603721] 
 [-40.464542, -13.909239] 
Processing Sentence 7...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.546528, 27.764406] 
 [-6.748737, 14.111119] 
 [-11.115250, 0.055858] 
 [1.085806, -39.080677] 
 [23.088125, 28.858402] 
 [31.811356, 9.416711] 
 [4.440099, 28.163073] 
 [-26.345619, 14.584387] 
 [-11.086802, 0.950077] 
 [-41.442074, -14.120034] 
 Self attention block 
K Matrix:
0.439848 -0.447334 
0.441624 -0.446207 

Q Matrix:
-0.445855 0.438422 
-0.443636 0.443197 

V Matrix:
0.447187 -0.444995 
0.441820 -0.442142 
Final K Matrix:
[12.501828, -12.633166]
[3.263399, -3.277549]
[-4.864348, 4.947303]
[-16.781392, 16.952373]
[22.899831, -23.204935]
[18.150798, -18.432103]
[14.390467, -14.552781]
[-5.147236, 5.277625]
[-4.456926, 4.535570]
[-24.463948, 24.838908]
Final Q Matrix:
[-12.560974, 12.544701]
[-3.251249, 3.295207]
[4.931008, -4.848411]
[16.853499, -16.844384]
[-23.096592, 22.912284]
[-18.360845, 18.120246]
[-14.473804, 14.428415]
[5.276159, -5.086742]
[4.521616, -4.439624]
[24.741313, -24.427059]
Final V Matrix:
[12.511268, -12.519010]
[3.216625, -3.235965]
[-4.945919, 4.921530]
[-16.781063, 16.796028]
[23.074932, -23.033604]
[18.386123, -18.319408]
[14.428562, -14.427896]
[-5.337752, 5.275293]
[-4.538113, 4.513499]
[-24.770879, 24.684564]
 

Self-Attention Matrix: 
3.216625 -3.235965  
3.216625 -3.235965  
12.511268 -12.519010  
12.511268 -12.519010  
3.216625 -3.235965  
3.216625 -3.235965  
3.216625 -3.235965  
12.511268 -12.519010  
12.511268 -12.519010  
12.511268 -12.519010  


Context Matrix (embedding_matrix + self_attention_matrix:
3.763153 24.528441 
-3.532112 10.875154 
1.396019 -12.463152 
13.597074 -51.599687 
26.304750 25.622437 
35.027980 6.180746 
7.656724 24.927108 
-13.834351 2.065377 
1.424467 -11.568933 
-28.930806 -26.639044 




semi_final_layer_weights[0] = 0.004408
semi_final_layer_weights[1] = -0.004496
semi_final_layer_weights[2] = -0.004431
semi_final_layer_weights[3] = 0.004465
semi_final_layer_weights[4] = -0.004453
semi_final_layer_weights[5] = 0.004465
semi_final_layer_weights[6] = -0.004428
semi_final_layer_weights[7] = 0.004427
semi_final_layer_weights[8] = -0.004422
semi_final_layer_weights[9] = 0.004405
.
.
.
semi_final_layer_weights[511] = 0.004493
semi_final_layer_weights[512] = -0.004445


 Semi Final Layer Node Values: 
0.129104 
-0.000375 
0.053470 
-0.001742 
-0.002357 
0.188455 
-0.001487 
-0.000477 
0.049279 
-0.002492 
.
.
.
0.004512 
-0.000045 
0.004494 
0.004511 
-0.000044 
0.004501 
0.004511 
-0.000045 
0.004459 
-0.000046 
-0.000046 
0.004505 


final_layer_weights[0] = 1.097740
final_layer_weights[1] = 0.866280
final_layer_weights[2] = -0.377073
final_layer_weights[3] = 0.554711
final_layer_weights[4] = 1.003016
final_layer_weights[5] = 0.428065
final_layer_weights[6] = 0.188824
final_layer_weights[7] = 0.834238
final_layer_weights[8] = 1.054231
final_layer_weights[9] = 1.419612
.
.
.
final_layer_weights[1022] = 0.285252
final_layer_weights[1023] = -1.039459


Output Embedding: 0.071027, 0.276710 

 Expected Embedding: 1.038754, 36.179375 
 loss: 644.968928 


Updated weights for final layer:
final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[510] = -0.005507
semi_final_layer_weights[511] = 0.005555
Updating K Matrix:
k_matrix[0][0] = -0.560152
k_matrix[0][1] = 0.552666
k_matrix[1][0] = -0.558376
k_matrix[1][1] = 0.553793

Updating Q Matrix:
q_matrix[0][0] = 0.554145
q_matrix[0][1] = -0.561578
q_matrix[1][0] = 0.556364
q_matrix[1][1] = -0.556803

Updating V Matrix:
v_matrix[0][0] = -0.552813
v_matrix[0][1] = 0.555005
v_matrix[1][0] = -0.558180
v_matrix[1][1] = 0.557858

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 8: Current sample's first 10 elements:  79.000000,  73.000000,  46.000000,  22.000000,  0.000000,  49.000000,  55.000000,  22.000000,  81.000000,  82.000000, 
y_actual token: 91 
max sentence length: 512 
Embedding Matrix:
 [19.101002, 26.462048] 
 [21.913157, 19.136419] 
 [41.163769, -3.830831] 
 [0.088311, -39.151413] 
 [0.000000, 0.000000] 
 [9.686630, 9.461657] 
 [-11.956720, -0.484444] 
 [0.088311, -39.151413] 
 [-4.618250, 19.850605] 
 [-14.034566, -32.497295] 
Processing Sentence 8...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [19.101002, 27.462048] 
 [22.392582, 20.014002] 
 [42.005241, -3.290529] 
 [1.085806, -39.080677] 
 [0.000000, 0.000000] 
 [10.285103, 8.660513] 
 [-11.815600, -1.474437] 
 [-0.262472, -40.087868] 
 [-5.375053, 19.196960] 
 [-15.012096, -32.708092] 
 Self attention block 
K Matrix:
-0.560152 0.552666 
-0.558376 0.553793 

Q Matrix:
0.554145 -0.561578 
0.556364 -0.556803 

V Matrix:
-0.552813 0.555005 
-0.558180 0.557858 
Final K Matrix:
[-26.033609, 25.764753]
[-23.718589, 23.459226]
[-21.691986, 21.392604]
[21.213479, -21.042498]
[0.000000, 0.000000]
[-10.597044, 10.480355]
[7.441827, -7.346614]
[22.531112, -22.345421]
[-7.708265, 7.660523]
[26.672462, -26.410174]
Final Q Matrix:
[25.863610, -26.017667]
[23.543801, -23.719050]
[21.446268, -21.757051]
[-21.141372, 21.150488]
[0.000000, 0.000000]
[10.517833, -10.598093]
[-7.367880, 7.456355]
[-22.448878, 22.468459]
[7.701931, -7.670420]
[-26.516471, 26.642442]
Final V Matrix:
[-25.888045, 25.921082]
[-23.550322, 23.592974]
[-21.384326, 21.477482]
[21.213808, -21.198844]
[0.000000, 0.000000]
[-10.519862, 10.539623]
[7.354816, -7.380247]
[22.521347, -22.509015]
[-7.743963, 7.725997]
[26.555884, -26.578266]
 

Self-Attention Matrix: 
-23.550322 23.592974  
-23.550322 23.592974  
-23.550322 23.592974  
-25.888045 25.921082  
-24.719184 24.757028  
-23.550322 23.592974  
-25.888045 25.921082  
-25.888045 25.921082  
-23.550322 23.592974  
-25.888045 25.921082  


Context Matrix (embedding_matrix + self_attention_matrix:
-4.449320 51.055021 
-1.157740 43.606975 
18.454919 20.302445 
-24.802240 -13.159595 
-24.719184 24.757028 
-13.265219 32.253486 
-37.703646 24.446645 
-26.150518 -14.166786 
-28.925375 42.789934 
-40.900142 -6.787010 




semi_final_layer_weights[0] = -0.005592
semi_final_layer_weights[1] = 0.005504
semi_final_layer_weights[2] = 0.005569
semi_final_layer_weights[3] = -0.005535
semi_final_layer_weights[4] = 0.005547
semi_final_layer_weights[5] = -0.005535
semi_final_layer_weights[6] = 0.005572
semi_final_layer_weights[7] = -0.005573
semi_final_layer_weights[8] = 0.005578
semi_final_layer_weights[9] = -0.005595
.
.
.
semi_final_layer_weights[511] = -0.005507
semi_final_layer_weights[512] = 0.005555


 Semi Final Layer Node Values: 
-0.002662 
0.239148 
0.221406 
0.215637 
0.005757 
-0.001106 
-0.000683 
0.230259 
0.082917 
0.272387 
.
.
.
0.005758 
-0.000058 
0.005777 
0.005760 
-0.000058 
0.005770 
0.005759 
-0.000058 
0.005812 
-0.000057 
-0.000057 
0.005765 


final_layer_weights[0] = 1.087740
final_layer_weights[1] = 0.876280
final_layer_weights[2] = -0.367073
final_layer_weights[3] = 0.544711
final_layer_weights[4] = 1.013016
final_layer_weights[5] = 0.418065
final_layer_weights[6] = 0.198824
final_layer_weights[7] = 0.824238
final_layer_weights[8] = 1.064231
final_layer_weights[9] = 1.409612
.
.
.
final_layer_weights[1022] = 0.275252
final_layer_weights[1023] = -1.029459


Output Embedding: 0.506361, 0.519164 

 Expected Embedding: -1.493413, -5.218942 
 loss: 18.462479 


Updated weights for final layer:
final_layer_weights[0] = 1.088772
final_layer_weights[1] = 0.875264
final_layer_weights[2] = -0.368101
final_layer_weights[3] = 0.545733
final_layer_weights[4] = 1.011992
final_layer_weights[5] = 0.419087
final_layer_weights[6] = 0.197795
final_layer_weights[7] = 0.825267
final_layer_weights[8] = 1.063201
final_layer_weights[9] = 1.410645
.
.
.
final_layer_weights[1022] = 0.276269
final_layer_weights[1023] = -1.030484


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004560
semi_final_layer_weights[1] = 0.004488
semi_final_layer_weights[2] = 0.004541
semi_final_layer_weights[3] = -0.004513
semi_final_layer_weights[4] = 0.004523
semi_final_layer_weights[5] = -0.004513
semi_final_layer_weights[6] = 0.004543
semi_final_layer_weights[7] = -0.004544
semi_final_layer_weights[8] = 0.004548
semi_final_layer_weights[9] = -0.004562
.
.
.
semi_final_layer_weights[510] = -0.004490
semi_final_layer_weights[511] = 0.004530
Updating K Matrix:
k_matrix[0][0] = -0.456734
k_matrix[0][1] = 0.450630
k_matrix[1][0] = -0.455286
k_matrix[1][1] = 0.451549

Updating Q Matrix:
q_matrix[0][0] = 0.451836
q_matrix[0][1] = -0.457897
q_matrix[1][0] = 0.453645
q_matrix[1][1] = -0.454004

Updating V Matrix:
v_matrix[0][0] = -0.450750
v_matrix[0][1] = 0.452538
v_matrix[1][0] = -0.455126
v_matrix[1][1] = 0.454864

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 9: Current sample's first 10 elements:  67.000000,  85.000000,  86.000000,  22.000000,  88.000000,  92.000000,  93.000000,  0.000000,  95.000000,  49.000000, 
y_actual token: 105 
max sentence length: 512 
Embedding Matrix:
 [-7.228162, 13.233537] 
 [-8.765027, 43.897530] 
 [-34.217815, 16.409678] 
 [0.088311, -39.151413] 
 [0.332362, -31.942741] 
 [-6.191297, 2.970860] 
 [-17.769611, -12.708574] 
 [0.000000, 0.000000] 
 [-13.371762, 35.941223] 
 [9.686630, 9.461657] 
Processing Sentence 9...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [-7.228162, 14.233537] 
 [-8.285602, 44.775112] 
 [-33.376343, 16.949980] 
 [1.085806, -39.080677] 
 [1.241660, -32.358887] 
 [-5.592824, 2.169717] 
 [-17.628490, -13.698566] 
 [0.000000, 0.000000] 
 [-14.128565, 35.287579] 
 [8.709100, 9.250861] 
 Self attention block 
K Matrix:
-0.456734 0.450630 
-0.455286 0.451549 

Q Matrix:
0.451836 -0.457897 
0.453645 -0.454004 

V Matrix:
-0.450750 0.452538 
-0.455126 0.454864 
Final K Matrix:
[-3.178974, 3.169906]
[-16.601145, 16.484400]
[7.527041, -7.386649]
[17.296945, -17.157531]
[14.165427, -14.052083]
[1.566594, -1.540563]
[14.288298, -14.129501]
[0.000000, 0.000000]
[-9.612926, 9.567301]
[-8.189529, 8.101798]
Final Q Matrix:
[3.191029, -3.152324]
[16.568275, -16.534113]
[-7.391364, 7.587573]
[-17.238150, 17.245583]
[-14.118423, 14.122501]
[-1.542759, 1.575878]
[-14.179477, 14.291232]
[0.000000, 0.000000]
[9.624240, -9.551263]
[8.131694, -8.187795]
Final V Matrix:
[-3.219963, 3.203304]
[-16.643593, 16.617025]
[7.330001, -7.394118]
[17.297213, -17.285012]
[14.167699, -14.156984]
[1.533470, -1.544038]
[14.180615, -14.208534]
[0.000000, 0.000000]
[-9.691853, 9.657331]
[-8.135934, 8.149075]
 

Self-Attention Matrix: 
-3.219963 3.203304  
-3.219963 3.203304  
-16.643593 16.617025  
-16.643593 16.617025  
-16.643593 16.617025  
-16.643593 16.617025  
-16.643593 16.617025  
-9.931778 9.910164  
-3.219963 3.203304  
-3.219963 3.203304  


Context Matrix (embedding_matrix + self_attention_matrix:
-10.448125 17.436840 
-11.505564 47.978416 
-50.019936 33.567005 
-15.557787 -22.463652 
-15.401933 -15.741862 
-22.236418 18.786742 
-34.272084 2.918459 
-9.931778 9.910164 
-17.348527 38.490882 
5.489137 12.454165 




semi_final_layer_weights[0] = -0.004560
semi_final_layer_weights[1] = 0.004488
semi_final_layer_weights[2] = 0.004541
semi_final_layer_weights[3] = -0.004513
semi_final_layer_weights[4] = 0.004523
semi_final_layer_weights[5] = -0.004513
semi_final_layer_weights[6] = 0.004543
semi_final_layer_weights[7] = -0.004544
semi_final_layer_weights[8] = 0.004548
semi_final_layer_weights[9] = -0.004562
.
.
.
semi_final_layer_weights[511] = -0.004490
semi_final_layer_weights[512] = 0.004530


 Semi Final Layer Node Values: 
-0.000364 
0.168174 
-0.000702 
0.176094 
-0.001454 
0.011056 
-0.001379 
-0.000044 
0.100710 
-0.000864 
.
.
.
0.004426 
-0.000044 
0.004440 
0.004427 
-0.000045 
0.004435 
0.004427 
-0.000045 
0.004468 
-0.000044 
-0.000044 
0.004432 


final_layer_weights[0] = 1.088772
final_layer_weights[1] = 0.875264
final_layer_weights[2] = -0.368101
final_layer_weights[3] = 0.545733
final_layer_weights[4] = 1.011992
final_layer_weights[5] = 0.419087
final_layer_weights[6] = 0.197795
final_layer_weights[7] = 0.825267
final_layer_weights[8] = 1.063201
final_layer_weights[9] = 1.410645
.
.
.
final_layer_weights[1022] = 0.276269
final_layer_weights[1023] = -1.030484


Output Embedding: 0.189781, 0.245752 

 Expected Embedding: -37.748848, 10.192697 
 loss: 769.140652 


Updated weights for final layer:
final_layer_weights[0] = 1.098772
final_layer_weights[1] = 0.865264
final_layer_weights[2] = -0.378101
final_layer_weights[3] = 0.555733
final_layer_weights[4] = 1.001992
final_layer_weights[5] = 0.429087
final_layer_weights[6] = 0.187795
final_layer_weights[7] = 0.835267
final_layer_weights[8] = 1.053201
final_layer_weights[9] = 1.420645
.
.
.
final_layer_weights[1022] = 0.286269
final_layer_weights[1023] = -1.040484


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005440
semi_final_layer_weights[1] = -0.005512
semi_final_layer_weights[2] = -0.005459
semi_final_layer_weights[3] = 0.005487
semi_final_layer_weights[4] = -0.005477
semi_final_layer_weights[5] = 0.005487
semi_final_layer_weights[6] = -0.005457
semi_final_layer_weights[7] = 0.005456
semi_final_layer_weights[8] = -0.005452
semi_final_layer_weights[9] = 0.005438
.
.
.
semi_final_layer_weights[510] = 0.005510
semi_final_layer_weights[511] = -0.005470
Updating K Matrix:
k_matrix[0][0] = 0.543266
k_matrix[0][1] = -0.549370
k_matrix[1][0] = 0.544714
k_matrix[1][1] = -0.548451

Updating Q Matrix:
q_matrix[0][0] = -0.548164
q_matrix[0][1] = 0.542103
q_matrix[1][0] = -0.546355
q_matrix[1][1] = 0.545996

Updating V Matrix:
v_matrix[0][0] = 0.549250
v_matrix[0][1] = -0.547462
v_matrix[1][0] = 0.544874
v_matrix[1][1] = -0.545136

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 10: Current sample's first 10 elements:  106.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 106 
max sentence length: 512 
Embedding Matrix:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 10...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.543266 -0.549370 
0.544714 -0.548451 

Q Matrix:
-0.548164 0.542103 
-0.546355 0.545996 

V Matrix:
0.549250 -0.547462 
0.544874 -0.545136 
Final K Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  
0.000000 0.000000  


Context Matrix (embedding_matrix + self_attention_matrix:
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 
0.000000 0.000000 




semi_final_layer_weights[0] = 0.005440
semi_final_layer_weights[1] = -0.005512
semi_final_layer_weights[2] = -0.005459
semi_final_layer_weights[3] = 0.005487
semi_final_layer_weights[4] = -0.005477
semi_final_layer_weights[5] = 0.005487
semi_final_layer_weights[6] = -0.005457
semi_final_layer_weights[7] = 0.005456
semi_final_layer_weights[8] = -0.005452
semi_final_layer_weights[9] = 0.005438
.
.
.
semi_final_layer_weights[511] = 0.005510
semi_final_layer_weights[512] = -0.005470


 Semi Final Layer Node Values: 
0.005440 
-0.000055 
-0.000055 
0.005487 
-0.000055 
0.005487 
-0.000055 
0.005456 
-0.000055 
0.005438 
.
.
.
-0.000055 
0.005482 
-0.000055 
-0.000055 
0.005426 
-0.000055 
-0.000055 
0.005428 
-0.000054 
0.005507 
0.005510 
-0.000055 


final_layer_weights[0] = 1.098772
final_layer_weights[1] = 0.865264
final_layer_weights[2] = -0.378101
final_layer_weights[3] = 0.555733
final_layer_weights[4] = 1.001992
final_layer_weights[5] = 0.429087
final_layer_weights[6] = 0.187795
final_layer_weights[7] = 0.835267
final_layer_weights[8] = 1.053201
final_layer_weights[9] = 1.420645
.
.
.
final_layer_weights[1022] = 0.286269
final_layer_weights[1023] = -1.040484


Output Embedding: -0.034655, 0.008520 

 Expected Embedding: -10.885829, -17.296432 
 loss: 208.604688 


Updated weights for final layer:
final_layer_weights[0] = 1.088772
final_layer_weights[1] = 0.875264
final_layer_weights[2] = -0.368101
final_layer_weights[3] = 0.545733
final_layer_weights[4] = 1.011992
final_layer_weights[5] = 0.419087
final_layer_weights[6] = 0.197795
final_layer_weights[7] = 0.825267
final_layer_weights[8] = 1.063201
final_layer_weights[9] = 1.410645
.
.
.
final_layer_weights[1022] = 0.276269
final_layer_weights[1023] = -1.030484


Updated weights for semi-final layer:
semi_final_layer_weights[0] = -0.004560
semi_final_layer_weights[1] = 0.004488
semi_final_layer_weights[2] = 0.004541
semi_final_layer_weights[3] = -0.004513
semi_final_layer_weights[4] = 0.004523
semi_final_layer_weights[5] = -0.004513
semi_final_layer_weights[6] = 0.004543
semi_final_layer_weights[7] = -0.004544
semi_final_layer_weights[8] = 0.004548
semi_final_layer_weights[9] = -0.004562
.
.
.
semi_final_layer_weights[510] = -0.004490
semi_final_layer_weights[511] = 0.004530
Updating K Matrix:
k_matrix[0][0] = -0.456734
k_matrix[0][1] = 0.450630
k_matrix[1][0] = -0.455286
k_matrix[1][1] = 0.451549

Updating Q Matrix:
q_matrix[0][0] = 0.451836
q_matrix[0][1] = -0.457897
q_matrix[1][0] = 0.453645
q_matrix[1][1] = -0.454004

Updating V Matrix:
v_matrix[0][0] = -0.450750
v_matrix[0][1] = 0.452538
v_matrix[1][0] = -0.455126
v_matrix[1][1] = 0.454864

UPDATED WEIGHTS FOR THE LAST LAYER 


********************************************** Epoch 17  total loss: 5050.070292 ******************************************************************* 

Epoch: 18
Sample 1: Current sample's first 10 elements:  1.000000,  2.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [28.625813, -5.706514] 
 [25.742138, -12.419487] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 1...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [28.625813, -4.706514] 
 [26.221563, -11.541904] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
-0.456734 0.450630 
-0.455286 0.451549 

Q Matrix:
0.451836 -0.457897 
0.453645 -0.454004 

V Matrix:
-0.450750 0.452538 
-0.455126 0.454864 
Final K Matrix:
[-10.931584, 10.774437]
[-6.721426, 6.604498]
[-16.915815, 16.761308]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[10.799090, -10.970898]
[6.611923, -6.766707]
[16.833231, -16.884470]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[-10.761022, 10.813432]
[-6.566342, 6.616249]
[-16.871155, 16.875041]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
-6.566342 6.616249  
-6.566342 6.616249  
-6.566342 6.616249  
-8.663682 8.714841  
-8.663682 8.714841  
-8.663682 8.714841  
-8.663682 8.714841  
-8.663682 8.714841  
-8.663682 8.714841  
-8.663682 8.714841  


Context Matrix (embedding_matrix + self_attention_matrix:
22.059471 1.909735 
19.655221 -4.925656 
0.084298 37.098739 
-8.663682 8.714841 
-8.663682 8.714841 
-8.663682 8.714841 
-8.663682 8.714841 
-8.663682 8.714841 
-8.663682 8.714841 
-8.663682 8.714841 




semi_final_layer_weights[0] = -0.004560
semi_final_layer_weights[1] = 0.004488
semi_final_layer_weights[2] = 0.004541
semi_final_layer_weights[3] = -0.004513
semi_final_layer_weights[4] = 0.004523
semi_final_layer_weights[5] = -0.004513
semi_final_layer_weights[6] = 0.004543
semi_final_layer_weights[7] = -0.004544
semi_final_layer_weights[8] = 0.004548
semi_final_layer_weights[9] = -0.004562
.
.
.
semi_final_layer_weights[511] = -0.004490
semi_final_layer_weights[512] = 0.004530


 Semi Final Layer Node Values: 
-0.001139 
0.061617 
0.173381 
-0.000047 
0.004754 
-0.000047 
0.004775 
-0.000048 
0.004781 
-0.000048 
.
.
.
0.004755 
-0.000047 
0.004770 
0.004757 
-0.000048 
0.004765 
0.004756 
-0.000048 
0.004800 
-0.000047 
-0.000047 
0.004761 


final_layer_weights[0] = 1.088772
final_layer_weights[1] = 0.875264
final_layer_weights[2] = -0.368101
final_layer_weights[3] = 0.545733
final_layer_weights[4] = 1.011992
final_layer_weights[5] = 0.419087
final_layer_weights[6] = 0.197795
final_layer_weights[7] = 0.825267
final_layer_weights[8] = 1.063201
final_layer_weights[9] = 1.410645
.
.
.
final_layer_weights[1022] = 0.276269
final_layer_weights[1023] = -1.030484


Output Embedding: 0.056499, 0.039206 

 Expected Embedding: 12.470385, -17.532656 
 loss: 231.437448 


Updated weights for final layer:
final_layer_weights[0] = 1.098772
final_layer_weights[1] = 0.865264
final_layer_weights[2] = -0.378101
final_layer_weights[3] = 0.555733
final_layer_weights[4] = 1.001992
final_layer_weights[5] = 0.429087
final_layer_weights[6] = 0.187795
final_layer_weights[7] = 0.835267
final_layer_weights[8] = 1.053201
final_layer_weights[9] = 1.420645
.
.
.
final_layer_weights[1022] = 0.286269
final_layer_weights[1023] = -1.040484


Updated weights for semi-final layer:
semi_final_layer_weights[0] = 0.005440
semi_final_layer_weights[1] = -0.005512
semi_final_layer_weights[2] = -0.005459
semi_final_layer_weights[3] = 0.005487
semi_final_layer_weights[4] = -0.005477
semi_final_layer_weights[5] = 0.005487
semi_final_layer_weights[6] = -0.005457
semi_final_layer_weights[7] = 0.005456
semi_final_layer_weights[8] = -0.005452
semi_final_layer_weights[9] = 0.005438
.
.
.
semi_final_layer_weights[510] = 0.005510
semi_final_layer_weights[511] = -0.005470
Updating K Matrix:
k_matrix[0][0] = 0.543266
k_matrix[0][1] = -0.549370
k_matrix[1][0] = 0.544714
k_matrix[1][1] = -0.548451

Updating Q Matrix:
q_matrix[0][0] = -0.548164
q_matrix[0][1] = 0.542103
q_matrix[1][0] = -0.546355
q_matrix[1][1] = 0.545996

Updating V Matrix:
v_matrix[0][0] = 0.549250
v_matrix[0][1] = -0.547462
v_matrix[1][0] = 0.544874
v_matrix[1][1] = -0.545136

UPDATED WEIGHTS FOR THE LAST LAYER 


Sample 2: Current sample's first 10 elements:  5.000000,  6.000000,  3.000000,  4.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000,  0.000000, 
y_actual token: 4 
max sentence length: 512 
Embedding Matrix:
 [21.309723, 1.530925] 
 [0.631562, 12.373799] 
 [5.809169, 29.942188] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
Processing Sentence 2...
Adding positional encoding values to the matrix 
Matrix Post Positional Encoding:
 [21.309723, 2.530925] 
 [1.110987, 13.251382] 
 [6.650640, 30.482491] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 [0.000000, 0.000000] 
 Self attention block 
K Matrix:
0.543266 -0.549370 
0.544714 -0.548451 

Q Matrix:
-0.548164 0.542103 
-0.546355 0.545996 

V Matrix:
0.549250 -0.547462 
0.544874 -0.545136 
Final K Matrix:
[12.955471, -13.095006]
[7.821779, -7.878081]
[20.217315, -20.371822]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final Q Matrix:
[-13.064003, 12.933941]
[-7.848961, 7.837476]
[-20.299899, 20.248660]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
Final V Matrix:
[13.083404, -13.045973]
[7.830541, -7.832034]
[20.261975, -20.258089]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
[0.000000, 0.000000]
 

Self-Attention Matrix: 
7.830541 -7.832034  
7.830541 -7.832034  
7.830541 -7.832034  
10.456972 -10.439003  
10.456972 -10.439003  
10.456972 -10.439003  
10.456972 -10.439003  
10.456972 -10.439003  
10.456972 -10.439003  
10.456972 -10.439003  


Context Matrix (embedding_matrix + self_attention_matrix:
29.140264 -5.301109 